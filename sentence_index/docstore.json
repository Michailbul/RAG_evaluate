{"docstore/metadata": {"6dd94362-1e7c-4578-b709-dfa87d3a3075": {"doc_hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2"}, "547f99a0-9005-45f5-8d9c-1416d3ffef92": {"doc_hash": "2bd6e2ffd37d18eb7fd6a04f153818236ad58cc69de988206e1a0a722b98f4ec", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a704551f-348d-4902-9fd4-fe0d4dbaffa2": {"doc_hash": "73c7ae513af7285b3507f2c97c0bb37d193b4adb5f337ed4b69582a94cd7f7de", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "df2673b4-45d9-4d98-af32-7e6f50c4b1fe": {"doc_hash": "9e5270cbf15251726380b33a0941bfe2e9196df8be76ef21fb5a620c61f3a2db", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "330fbd79-b681-4383-8558-f1b285544663": {"doc_hash": "10476fec678e9549d9f44c5f94493134488b40c2f8a04c21b67c841198dec83f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "176923e6-80f8-42ae-87e0-e95abd4fc6ba": {"doc_hash": "d2681bac4b10efc0f1ea518754629b55a612d4241cb583db5b299e46659ca4cb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1347e842-ed25-47ae-ab26-989559fa8cd6": {"doc_hash": "cdc54048968980a5f8350007ad10da708c33148873b85ee34c15a687d41030cb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5ac97b78-3252-432b-a143-1a49729894c5": {"doc_hash": "352d73a07f0bf5ad435babe3601dcb7527d6e122f3bae34ea1970ac41e847174", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "81c5b805-9a95-46c8-9620-f1232187a2b7": {"doc_hash": "be62fff35d7b60bed1d260bd72ebe5fa410104773431037d32593a640e41ecd7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6d22f1c7-5ff2-4376-ae27-1be353771545": {"doc_hash": "41ea14f30df58948278104c7d233912ea7759ed02f601bed6415a162980a1df8", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "60c8a8df-9867-4032-ab78-d0cfe0720527": {"doc_hash": "e076167e6f756430b78cb519fd68b5cf9f5b0ef4727fa5c277dece04232ea6e9", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2670f080-137c-4464-9f0a-43ee1441290f": {"doc_hash": "c1602c94b4ba798f2907da8837e0c86f8a9768835cf31a1d6739cd3e5bd596f0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "57555c7a-62b8-4624-89f2-b79f64a40300": {"doc_hash": "029fbe68b3e8841bf85ca509271bf297952bd2b6f1d65afbed5a9f58f8e73ae7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "bc6519b1-a119-4d24-bcb8-eaa3ef83c1ec": {"doc_hash": "fd7c7586f57f7d19fedba6d3e10e3c2d52537ecab00657170420fe7488a110a1", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "81f28ccc-9706-488d-afbb-33eef33899c5": {"doc_hash": "db20817fa108d82185c088adc874c6f8b7ada8fae8b56fb87de7fbe24df0a184", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d5b71df2-552f-4e14-b735-350f72c9bc35": {"doc_hash": "95f5f64ed550e94f3bdc5d11c5576d435eda63ac5ac4cfad253d4440e6fb50f7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1639d32f-7f09-46a7-9836-66830e316fbc": {"doc_hash": "e18494a9bd6792c5a39245586b917dc0c998cae20447982f96cc0bb5fcb1b6dc", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "388685a8-36c3-465a-bfae-c8f9009edd45": {"doc_hash": "56c86d9b2092ee2d02aec6076369abc222b581bbcb98b6721206586c01479587", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2c8c0ba7-54f9-463c-82ee-5ff4b4c6375d": {"doc_hash": "c5c2874c7c480935820cd5b1d5b3b057daf7f24ad4c68402393e5d3577c92ec5", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "3f8372cf-fe56-4ca9-91b8-744d80142b12": {"doc_hash": "6055470d0fd5a4b44887bb8b7d8dbb74f0fe19e5cd467240bfbba9cad73e158d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "7a47188c-fded-44cf-a828-6595064fe1c2": {"doc_hash": "91b4fd6d7d4671f723d37297f08ee25afe11ecaa2813cff4524b26876f7db835", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "529c262f-c6d1-4328-9782-e160f69a4651": {"doc_hash": "86ca4b0746477ad186c8fa1b228888c48a026e45640d0ac70ccf7d4cf76a631d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "cd87d5d7-e94b-47c0-8873-0d28e7eca961": {"doc_hash": "561de8570895d75120e76fcd458864175088b62f4cf880750352b13b956e7a2d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "72db6013-3836-44fe-8c7d-201fc624d34f": {"doc_hash": "9bdad05cdaac28a9cc663a37fb3c9961d42386b3742b1cc9ad307d9b35215241", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a072151a-ab27-46fa-b8a3-cc20e8345dba": {"doc_hash": "0886a300a92a5937d8ffbad7e04c4a7c5ffe37443d9be7bc4f61b9c5e1978f42", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "401f55b0-539d-4506-a21f-3e0b83d1d304": {"doc_hash": "a0fd730278b5966d6ba32374daffeeaf1260e6bb9277e6c46df52e790b7ed4c2", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2a0fe853-350c-458b-b4b0-d344fa75973a": {"doc_hash": "853fbde1c09da3884d8c77870b04b715f5403e8821adfc3fd8002d9823e22213", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "db6ebf2f-1bd8-485f-b807-d7cfe1013727": {"doc_hash": "bc19a3f7b3016dd64c1a6fef00f460b679a5ab2bfd4a23868cd30c6d945b7d7e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d5077afc-82e5-4125-9ca2-d6202cc52226": {"doc_hash": "9c893ec86051ec490b3b4b66c7fc5f01d0d1b3abf6a92319765dc90cb4fc8815", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b9bcefab-a990-49dd-b7ad-253014eee8e7": {"doc_hash": "5ab2d64faeae16ceb86f6a448cc3cc47c85b89997877e2426a99bceb002b4227", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ef6b795e-15d2-4366-b0cb-05f979ee0072": {"doc_hash": "9b53c811b452890e515d4b53f011fc6de6a92867ebe1936566afb46fa000b21d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "742f0c59-f766-4b35-9088-c1b222883235": {"doc_hash": "f2a6a6c39b4e183c030bcadb1c14a6bfe277c67260839a1554656ccb0613fca7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "12ef0ec8-b8b8-46c5-b5ce-57d24e1db48e": {"doc_hash": "8ac98ba19ae2af1ed7f3a4656d830bf0ff9917d1591aca40e9fc6c5d2d3c9d55", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5586a07d-3dfb-415b-ae4a-a5c7d6cd688b": {"doc_hash": "1bf2c73faacb4d660d52e29d8e9eafc2aa19001130df5c11080f75e6ad05a409", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "bd04c7ec-e7dd-4b77-b28d-f900980fb8af": {"doc_hash": "bb9dc89b58d4d59e7b3011d78d0b4a75dda150638c3b526d6e327c9ab70d4d29", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "3e3bcdc6-9cf7-4470-9422-9aeb5e7592e4": {"doc_hash": "afb0506ba364e3a50857f6a12cc235b5b6a379361a2bcb339c110a2dcecd4ea1", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f65f0633-9e26-48bb-b1ea-dd02dbe3e01b": {"doc_hash": "4afac4b14c0bed447f156fb9fca2852cec80ddc0f5598b482816c5105c50c6c1", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5e41bf38-5f72-4c16-9ab9-a88497033515": {"doc_hash": "a281e561e33bb7035a0c96f231e15e7adaec93be1ed109def732b5929cc086c1", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "aab105bc-aa0c-4c68-b413-4328dd3c6e36": {"doc_hash": "4683a01a19512858473971abf4b78d9070ee9552079acc0ef50be85b4e44c40b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "20d1ca9b-f018-4854-baed-4b4cff004803": {"doc_hash": "8d8431195a48678287b5a66a9730759697064502673c0fd506398fb5f476971a", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "988e70b2-8d3d-46cd-a228-f1ca7e9e6d56": {"doc_hash": "c7ebf573366e4f5452fc9ef2ef2803f92df38e01edf05fc7ea824578f2d31d66", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "75278cee-8655-491b-ac50-2f87977ef4eb": {"doc_hash": "413864b3e770c0bd6b1b52daa5d39dc572766a3394a9e0cd12485822de465701", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0ff0f7b0-9beb-482c-889d-6a554935fd76": {"doc_hash": "bf5c90f205069d1b36cccdaa66d5f18a309864c7ce12e1e2df3ec92c4e27eab8", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "31b435c3-8b65-44fd-8028-a1def5d48cc0": {"doc_hash": "a8fa987af543b423a77f57d0f9eee99cd2db10423d567f1550ebe1a09c4ed26b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4b926bfe-6217-4111-860f-87600a16e9da": {"doc_hash": "440b90879e9b7e247995ec282707e6d53b8e6661ef15369b7619ac1c0c7edcff", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "617fc7c9-0a88-4fff-a94c-0c9dd3bd9166": {"doc_hash": "86c06ab4cb17589764eb349697cb47f6f69e14c53292ec5a022448fe235e7af7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8e7f7552-472f-43e5-b71f-83b5cd97eeac": {"doc_hash": "77d0780990c925d375e4e4c67ee78986977076c3b9804b4e82768d3581cdb821", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d469d2ee-1416-4b6a-951a-e62b49418299": {"doc_hash": "3ed262ad40cb00d5f8aed7d92f0413bc151d1d5ecf58d1f456c0353906a1b2a7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9e3fc467-9f60-4880-af4b-96e583e27ce2": {"doc_hash": "991ad04d8f585ed6e7fe14d3bc75bf7773c02c0e17c6e4aa4dd32bebfb4d7a23", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f4befc74-4ea9-4ff6-a214-346c05bc8eba": {"doc_hash": "e05c259f521b9028324959ea2160efc992f76d833e9e8f41a02973d25b1b06b3", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0fd81ab0-44f5-4db2-a45d-a36e0eb02a4d": {"doc_hash": "2f806e20a429e42b3a99429a48969ed46c2cb1ac25083d1ca5197f0cf39fc422", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "97888f81-147c-45b9-af5b-fe7d6755510a": {"doc_hash": "0d8d4407aefee091ca04861753e3265635f0cb6b29cf3a56bc2ac92e2b044b44", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "42609e00-26a7-47c3-adff-77188ce87923": {"doc_hash": "7263eca56535895bbfef083176b873eb1bd5200637d7f2bf191a5d05a4589781", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e836b2ca-c5e4-4382-8146-482db695aa28": {"doc_hash": "3f5f0be4067925d5f25e60654436c563645ab9bf55db05bf956fef3bcef812ef", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4f20d9cb-b092-4c18-b149-cf207e22a228": {"doc_hash": "c75fa05e32d8286e21247d7b75953a751267d1b625cf4b818ec18ecc7ecf998f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "371ce4fb-cf20-4b1e-9172-0139f172c6a1": {"doc_hash": "44f5cfc8e194beda7d33ce37b4206e2cb83809f1e77cb744758a1f67246b8c42", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0b806241-bebb-4935-8d40-483024eb9fe7": {"doc_hash": "cf98198232ba8a95c46916bbf874e414b6ec5e1a1969ed7be769c0c6f8dde26e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "646bb0fe-60fb-4a56-9af1-1914f61869ef": {"doc_hash": "ae57a771eaefe4cc2eab3ce898899d34b933fe5e63a57f3962123f852c514854", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c7d6e4d0-65d0-4f09-9e5c-a67ac14950d8": {"doc_hash": "82a42acdb79594ca21ad11de248be9909f11ca51060c7572fe52c837af42ed36", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b72c7c39-8dc5-42ca-aae6-829ceb4de9ff": {"doc_hash": "e45c0bdf19643bb80b326ec7ea36db11b9633673f925f3f52e9c85c9660d9530", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "3cc0f7c4-ffaf-4a44-9f7f-16eabeb3e703": {"doc_hash": "f886b7d3aaa91159f377d72fbdcb29023262e3a2ebd193c5e358b960fac1d471", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c2f61c67-c169-4b85-8a2f-48bf1045ac2b": {"doc_hash": "cfe92b1f6158631f00ab0efb0cbb7dc2e7e023c2709b56280b6463d3a9c56911", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f4ad0b61-16f7-465a-8d09-88311b46b674": {"doc_hash": "b36621ba4b120aad003c49d17006df2f7717f3ee75772ffac134489dd257a313", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ae82fa1d-2588-4b6b-b73a-56a15eba0c31": {"doc_hash": "ebd2cddce60111c96ac1d7d704f4654f80179406b9f4bf67f0c150b400b28973", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1f16492d-cbf5-4165-b865-8edc18ff3a8e": {"doc_hash": "eaa75858c61e171f8a5b62179cd38029e4fc9168c3a22ee48db61b134aeaf3e9", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e0156189-1e63-4a4f-862f-39eacee0613f": {"doc_hash": "ea5d44e24b24dc4a323a09aaf85fd53265135d6e7ad44947e2de943d395bebd4", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1462ffbd-ae4d-48b0-8c24-88cf5574a082": {"doc_hash": "aaf8579707140e6a50f105542a31cb5da05fbb0bafc1b335bd757771eee81c14", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "24977c1d-216f-44a3-8be5-df0545bf8434": {"doc_hash": "5eeb0a1e40b17d2cc4ed6766785cde0530f2d78e6b4cba1b8aa407e47045a57e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fc8e5678-fcc2-4b89-8cc9-4402dd31031c": {"doc_hash": "2211bc727b80b58de92bc235afbb96d67936e9781a647d7182a67ebc813c89cb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "14998b1c-e5b4-41c8-857d-6593b7052de8": {"doc_hash": "8a4838c0e0189b2a49a43628039df3d4b89750c628603182873c34503f4321d5", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "dee275f5-7df6-4765-8362-3783e4192b12": {"doc_hash": "e00960ff6ac3d552b5a0ab913ac9d57e6929f15f0ce9e3bdbf5709a9d9526228", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "43d84b72-3f06-412e-a3cc-9058f6f7be8e": {"doc_hash": "87578a995d8f1783d4869b88221fa70bcbc3d9f6b7f705ef99f70226588757ea", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b75c5011-6320-4288-8ea2-784d530380a0": {"doc_hash": "375ea0151f9f1e75dba8c8d64a83ab555623ee763162a22df4691ff57f88b488", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "dd12e284-8f3f-4408-9836-e9a1303df12a": {"doc_hash": "56b5ec4bd1bb8e5a58d18e34bc228046ca173546f9f8b206543fd181b24dbce3", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4a739e16-d63b-4cca-bda8-b313c74c28d1": {"doc_hash": "1c5b69e50de3a87e8e76456b558811eab0d5788b323b1b4fbd2d64d166c3ab7f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "953ac142-30df-44c8-b09e-df0b245a59f3": {"doc_hash": "ec56ab70cae3202e7b2a78ad313f710974c9dc9e9c7012baa577e5cab933295f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "803d2b1e-80ed-4ffd-9e82-34df0eb70e6f": {"doc_hash": "09ec164c3067b42348f9a460ec3fa80effe0320e481ae8cb81ce510f3dfb3211", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "127063f6-0ba9-4bbe-bcab-ceb1b726e972": {"doc_hash": "4e2521ad905c053f9692fba6ec6056fb141b2e9b647e2b5da7e5d69953936ba8", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "138113a3-3d83-4093-a42f-e432b51ef0b1": {"doc_hash": "53f0be8549966ef4b043d1d83b081370591eb381d205fe311cf3c05fcaa9ad28", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b0a0b59a-76c9-4c90-bb02-4b728c1d0e88": {"doc_hash": "e2416094db40208cb97c3263aa3b65d955f90c3a11e2fc790c51dae371cc210b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "82dff16a-6bd9-4564-bbe4-87a576b113ac": {"doc_hash": "5f0497b3c6b47653cd49a2154aa4d5a9634a0ab79e1c52a69f4adb1048b62d47", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6bc86263-faba-4174-8d17-1da1890632f2": {"doc_hash": "3f24e0ea053a68fdc468bf3a71c6164d12068180bfdbf1ec1d6a3518a16b0f57", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6002afb9-52ac-4a0e-9750-80a7db473c11": {"doc_hash": "c59f0916e8bb70a96389555ef494c51abe98b1c5c47cbeacaefff2b5771ae662", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9e1adf3f-6e87-4ffd-9658-ad23b60aa15f": {"doc_hash": "8334eb4688303ef99e89cdde6ed2628ded36974136c2e887fd9aee261f4902fd", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "cd7210ec-f709-40c8-b0df-83a6cabba803": {"doc_hash": "f5da8211f8975f3a73a7d27cce4afc6fb81d1207d9404b36c042caee79c9eb1f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "819c1497-cf2a-4690-8f16-f3dbc76048df": {"doc_hash": "be5a2f708abc28ca38e922b603268217309abe61e8608d590b65826e247f0cac", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "91c48cf5-9875-43a2-a5b4-079ab11b97f2": {"doc_hash": "5fdc9a8dbd45c69e8afdee38f45d3547b469675f488176f4295a94fa68b77f22", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e661afe9-8e7d-45d4-af86-55591675fa82": {"doc_hash": "1a4a945893d61b15b1f9f64e9d506e315e95d2306d252d1b2d6af0b8e8ec663b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b0baca0f-baf3-47db-82d9-5fe2f8f13496": {"doc_hash": "f9e802ab8f46adbfab7a40c48304b38bdd9fc2814b14ceecca7b1aabde63e55d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5f5d2cbc-03df-475a-a591-c3710e4f9767": {"doc_hash": "46c74bd75f5dee5c312e0c60c43f6f29338c6f956f128f715bec3b97285d3347", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "81522c83-276b-4b1e-b8f6-12a56ce1ba0a": {"doc_hash": "f807166a6b7ac1c62bb7b5623e76cdfc838b7b5a2c8da9904a9b7bc4225fae8c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "dfcd7fb4-1f72-48cf-9375-1203dfd55d58": {"doc_hash": "3d998bfd640c4c955a1e5e960174eaca5fcbcbabea24b4e450666aff3f2c662c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "99fcad15-274a-42e0-b907-3c1340ec734b": {"doc_hash": "8e513e33408182b8ec5028e390bfbfd5b7ff9bede54b6a08af06815bb9ade480", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "7d345fca-28dc-4568-9795-fc5cf0bde6a1": {"doc_hash": "8034ca0d31823837f5230607a5edbc26044193a1247827d15d98cefefc733ac0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "72e54b2a-ee06-46b8-974b-9dc82506bb1a": {"doc_hash": "928d51cfc4c27ac49465611128179d329b0fd5c2d9b7604a03b8614c8932ca69", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "11e4b990-d314-4029-9516-735137186c65": {"doc_hash": "2bee0b0f3591fb7f4b91841ebf61087d37e3e585d26242bf3d4c9b8f827e90c9", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "3cf55fd4-8253-4390-9abf-9a6ec6d9d0bf": {"doc_hash": "aab33eb7a28d20e9dcdbf0c9ca98d185c40bb33aff702aed98b0431adb1f23c5", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d8e3aba4-9507-4439-a0da-0b3d5a13fa2b": {"doc_hash": "527fa58e7652147105bf60fdb832dd231cb68804a19f5922539b024060209e4e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fa3256e0-1567-4535-9b9d-c2461811531a": {"doc_hash": "2c09c5a56b799de654b870d9aec0237c28a9ea64f6581f6c6312590605083d30", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fff33e16-b299-48ab-85c2-f3f5b9e38e8d": {"doc_hash": "15da2224480dd2e1130df55ffb287215937c2257062ad5f26aa997572ac1b246", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "06ddb055-781e-48a3-909d-a1aba2d94922": {"doc_hash": "73002fede5425e951863b1e2d2464c796389c839e16a06ec80c757b769bc06c6", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "89eeb99d-27a9-4e88-8e74-10bf15ac7db2": {"doc_hash": "40c32eb123d839ba4f00e1f1fc18d8a96a5b6e1876eb4a4b0f4de86a6615de47", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "dfebabe9-75b2-42ef-93aa-9ae89ed66882": {"doc_hash": "5da2e07f172a9413494e0c0ee1ee17abb5635a4acac4504dda028cc0ac302dab", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "951976aa-bb00-49b7-9f45-6dec9a94e1ea": {"doc_hash": "1acc75102c37540eca154fe32c031ca0f2980d13b9a22c42d0df0af7001c7fd4", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9e015ef8-5f98-4aeb-b029-47c8ab64faaf": {"doc_hash": "b89ab9d82fb00b40d81eb777f1a8bd863baef3db6912540ae4294a130e9fce9b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a0fea3ad-36af-4392-abd5-4177de72c488": {"doc_hash": "a80edad22917be8d91abb9b90cc198c7c12fd3cfac9cd8a84040691db915de6e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fefea734-8549-428e-973c-9f462f99f80c": {"doc_hash": "c8d67e22c7061e1f6ceb83dd9c8dd225d2a4dd842703948ee56fff7e6a3bf37a", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6d5c4044-ce5f-453b-b0fc-1ad027eb0d55": {"doc_hash": "75e552ac4473babebeb85ae7eee4cb14cff81ddab3d4324c03e4c1f8e36c0393", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "294eef68-ffad-47bc-b64a-f2121d3e7212": {"doc_hash": "f853fc9450309b7afae29ad6604a40a185e416b3a8f6928e06095c50495961c4", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "298f9310-40cb-4375-b81b-8b2c99cf0347": {"doc_hash": "1e506aebe727a8eb856ce517f37e1b5aa1efbdad145781cd2c3ba5417dade6ba", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c198fa0d-7522-4620-88bd-07bdfa643302": {"doc_hash": "3d984e420a07089b49d3329945912992047b050585431c00ee1b393a705fd41f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f43c1320-da0e-4144-889f-e31614c757ac": {"doc_hash": "7af619c66d2690e9581c30861c6e87292b2ac2c001d32f337520cd8394c2873b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "aca4d951-e9aa-4877-b8ca-d110beb2e64d": {"doc_hash": "6777192805bfc5e90fd88f033c6486a5b8b487abb7b56bb0b5bcde589cc7111b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e912e6a0-f5ee-480e-8f78-6d2abb5bc632": {"doc_hash": "5178edb18df39a030b2dc0ba034520d5589da7a47cbc9d659b5c772361b4dd27", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "58ae5b0a-fce6-4459-a97d-f4b642a7984a": {"doc_hash": "1a5aa8f1da28bb3652e6273f655b8c05bfdd60c375bb4527469f18adef8f25de", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8649d834-25c2-40ad-9ed1-013a18467f87": {"doc_hash": "501eb2ec24287b432bca82ceb3f166420900b92f1b336cd38a5bbe061dc7767b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4466e77c-3386-43d1-b5c3-d0c5fffdeeea": {"doc_hash": "af2eafcbb42b80bade25b2fb53434c7ccfb75062b00304eb68c863e1357f6eae", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9d72e90f-e3ee-48fc-b0d4-6d77f9c97e46": {"doc_hash": "7c99b3afdb5d8ae41d3a9e6726914f4da5caf45f953528292e9f466cbede4e1a", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "70f02ad8-f45c-4f06-ad3e-2d63801cd0b5": {"doc_hash": "f6b73650495e240fb0e5b6abc1738a244a55537a0fa4376d2624162b5d1f56f0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6adbb3d9-511d-44e4-8b78-befe1f1d9628": {"doc_hash": "5f5eda55657091e06690aa622bc32e499dc38c66ab48851b8181aa4fad1ad3b7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "431ca2e5-480f-466c-976c-0d7f059cd6d8": {"doc_hash": "1722117ae6b1ad27383a067fb5853627dfa7054e44341e5761dc68088c44ace1", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "61721f0b-9f03-44be-88b9-d5893725ca43": {"doc_hash": "9de61725d709d73c88ac8ecf560943b683e8276f059c727544f100637b3475aa", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "cb56fc15-cca7-4cdc-883f-9b1ecb39ccf9": {"doc_hash": "1f806649bbf5c8ac94586830ef39a6c989ff7d5c27d92352a044cf6effdc50a7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e61c2eaf-977b-44c0-acd1-2b208c4dd282": {"doc_hash": "6e5d6c48b602453a9087a5fd4c6e1a7ff1d6a64b69488840488b3bd6da05c019", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fc6a850c-6129-4b6c-9ad0-3df8341f3346": {"doc_hash": "b686ceaef6436c20d6efc04d36a977c8862d02463a83bb59ac5e415c971fafec", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b62ba0f1-a715-400b-b942-d62cddf74b84": {"doc_hash": "5604a8c59af78d146f7920f45bebeb8633424992a3a699dda722c1b598d273bb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b34a463f-8734-4f1a-8aa2-0219615c2042": {"doc_hash": "eebe9feb16a8e26288678883951e38453bee933f4b456e471cb72999e9d3654a", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "64cdf3cd-d1e7-41f5-b9ed-3b18129c79ec": {"doc_hash": "e98d98da4cd5fcfddc10137132f280eb18d767c28c4ffa216f5537b934b55b10", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1908889c-6ace-4511-8282-3a56c4141879": {"doc_hash": "b264932e3ec58c07a2b674530b99052f8b8e413f7840c956e224377ad228b2cb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d64a585e-7c6d-4fef-a3d9-ed7bcf0689dc": {"doc_hash": "eb78584fc2c6ed4304823a2100762d3d1a0c4293de73e8b257c450afc8f912d6", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fbebd9be-4a75-4d33-8a71-890c2f2e9dcf": {"doc_hash": "659283ef6bed5e16d079be47e3323479a4bab7434ba9ced37ee2476cb7220107", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1807f7ac-1e86-4f32-87bc-83ae684d4327": {"doc_hash": "26b51fb28c2e7f5b35800e12f003d293001ce45c24f2238caac292e1488f1175", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "95a43bf2-b620-4f64-84a0-1d03a03139e2": {"doc_hash": "3164fd1ef5f076ce529280dd10ceebe38644d40cc70ad348dabea89655905af1", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f96a65d0-28bf-431b-9366-6861791d3e41": {"doc_hash": "433ba40933912c4c50580296a9bc6314a21ea5747c525baaf2a2015edd359d79", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4e24b4de-6e12-4d63-97aa-753bdc952f6f": {"doc_hash": "6d9b6b15725bd8f9878b0e0cf19064e5fc28905983e8ef197a9b019cd556f6d6", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0c108fc0-6b3d-4d7e-b3cf-61067f12742c": {"doc_hash": "29ffa423bdd2037352f5ae37249668ab7a366724e381bd354d6c7a560576a226", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f4518347-48f0-4546-9cc4-0154db6652de": {"doc_hash": "eec15f5ee02cd5e8b6d31106829be4ef223f50a347e5082455a80c652712e940", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "824e5304-1f2d-4538-b1e4-676573024aca": {"doc_hash": "028198e6b3affd945a45d5fb97c877b3ddf1cb8a88c6e68d684c956dac8ad416", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "997ea78f-7c5b-4259-8087-d5b2a35e4c44": {"doc_hash": "89eabf88b68fbc9ee0c82fca2940295d43d930cd93374d638802fe0cc9fb5ab5", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d7901f16-872b-48c0-8410-870a7b801725": {"doc_hash": "1d04a4cd648a24f77f930aa9f085ace0e8ad4b470c325b7f1929be6651aa17cc", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "05c7232d-627c-498b-9be8-81832a1cf585": {"doc_hash": "75cb00d10da7bb8156803602791103c99a8a307acc5538ad11392cae134f3913", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "222e4b78-4982-407d-aeaf-dca16f5360a7": {"doc_hash": "6fe24f054836983a37173e2a130962e189b748b22ecb4998527ebeab3c0d7a7d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9c2bbbf5-d038-4643-8927-d0339a7140d5": {"doc_hash": "38fcd2884e2630a6261956dd7a22ead2999a934fb47482f7aa3fec5445ba2186", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b99ab72d-ee41-41d6-9042-2c6401f9ce9e": {"doc_hash": "c9249e9a78450f50bc229e36eb6b54599a15c90c570cb3748c79da310bbd54a4", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "337d05fe-4019-452b-b08e-e8af51508ade": {"doc_hash": "d351bb615bc9435516874160511b4b0192b9d41fcbbc43baf0ac14e5efbf79a6", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c9dd879c-a983-4a07-8516-7454d5257988": {"doc_hash": "5969758806a6097976adeb3abf95c479e9efd06e7cfdf8898957d4fe3698e834", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e67ebded-242a-4922-9eb7-e4f587ce8aca": {"doc_hash": "1259230ced005f939ec13e92f537057aa1fdec954fa53112c21b1f10c5e7ed10", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9779bebf-26a6-4c5d-93c8-aeddeb1874a0": {"doc_hash": "2dc2aa890a13931ac27b5b8229da0bfbbc279b8fd50a18d1efe7e93458f14a1e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "87bfd103-7e85-451e-9561-b60d37567d57": {"doc_hash": "3945cd448a8465a6104af400f94dd3dd6f4223fdaf348104a26a6aa080bedd66", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "acc04eca-e503-49ee-879f-291846f10c3d": {"doc_hash": "c0bb45ff10e16ab2ae03703241e524c4b94b2713ab8cc5cba18813ae38951d61", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2d5e0bb8-e856-483f-b62c-da69dfae1a48": {"doc_hash": "d27cfada0c50bd558760835d10c515cfeb9f68cef93bb42a81a041eb728b7fcf", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a15859d4-293d-4076-8667-6eea8653d569": {"doc_hash": "242694acf98350e9739c587652b37e2be744d809d8ca3f5cd15ec7ff0d530993", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ea7ce394-8ce7-4d90-adf4-04be8005c1b2": {"doc_hash": "40ea45b7cbbe474478d48703a8b337ad907952af5b957a5dac48cb2ca9dd6a76", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1ceacb7f-fd65-48cf-b669-21f3d95e36f3": {"doc_hash": "2b5dd58708d30bba804110d94e9ab6e5b24aaeee2d6a8461a3ef1721f7c67eac", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "7590f3b9-f176-4232-bb89-2644ce7e1a79": {"doc_hash": "af1070cc60694a5918b38c3f4822b762533c64aabe8261381d3be6f3f3d21719", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "414b568b-a768-4621-83bf-bae1d1ae7a59": {"doc_hash": "939e6911cb30fe62036670ae71d3db398b64662d854493d7648faf341418d85d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b9033fbc-42bd-4157-8a82-e9b2682431fc": {"doc_hash": "2f0d62debe4653de18165741ca3c11477d1f58468acb1fa72d3c50d39b53e802", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "04ecb3b0-044d-442e-9407-83b12b77b1bd": {"doc_hash": "8a96a803b57cbc953bf99a14eced30e436e0c97f553cb83e2435af517500fb51", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0088b0ca-7379-4364-b4d6-e199e54268c6": {"doc_hash": "8f3f22f6421a9fbbaf8e27461914575daf3e7b438dfe4ab6161607c2c66e1cde", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4820c030-ecb5-4577-99d9-f063bb7ec589": {"doc_hash": "1555b78ba58075964ac122fa13929b81e8e8d700a1638b1f1f96a9eda071a796", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8cde89eb-7d8c-424f-a57c-78e503197265": {"doc_hash": "21a740a1d9a87413ddb4540dc16d703f773487f645da0ef22eb2c2fa4188f527", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fbfe8567-48e1-4545-bfd5-9bc68cb1f3cf": {"doc_hash": "1154fa65df9b7ca3fc7c635ca70c68159ccd05ff0f94ba976d29d461db7aebd9", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fe215daa-5d7f-4529-9ff9-cf57f2de54dc": {"doc_hash": "fea70d504044d560f3568012a0be30f0b11e69b7b7d664d3c92a4014fff9822e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "20e6f4f8-621e-4f45-a9bd-4920b2f32628": {"doc_hash": "2a3a8d7ff3eea50e50d94507162071172786b731a51a483578b65457d3a675b8", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "97649038-9eb7-430a-a618-b292110fc983": {"doc_hash": "175c7140ab0c0fa5788a43a9cb47bd27db94e3e915d49124720dbad1bb7c6a17", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e30baacb-fd04-42ec-ad6f-783fb65214b9": {"doc_hash": "1cf78e2fd95406c1222fa08b21ab29734ac31b11729bcb537095e83464c1a43b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "aff23419-1adc-4fbe-82d0-2f8a7b60b137": {"doc_hash": "0713caab9f8c66d673b76160e7a466d6d42abaa3ffe01f21e3b52b3b2103e84c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "db090263-2d71-420a-82f0-90ab34352e11": {"doc_hash": "d72bcd67987f6d2883700a3aded0511297e58b18ffae71089f2c79ff1ec90cc9", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "628243f3-2dc8-458c-a2c5-f977392fb0d4": {"doc_hash": "148a460b752fabc401155446c62e5272da9b8199bdb6e09a2a88d4e480b2ab13", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f00a85e5-a2a2-4b10-90e2-75fc8579c00e": {"doc_hash": "0f9b55d88ab4ff0bdb9f683b213f00eefd6d61b112dcef2f2dea6b9748041975", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c880acad-48aa-4c59-acd6-5b168a6fe355": {"doc_hash": "22284dabeafe61bf4a8d19946a2ad0d53159e10b56a001febc65df5df7016713", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "21734035-2713-41b0-8028-819e718595c1": {"doc_hash": "4e11500fa2c9c8d4d58c01360f8358652653aa2a1697832a04b788105a07d97c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9ab17891-63ce-440c-ab57-d516372bed56": {"doc_hash": "a52b261571b20fb29421a38668a8ca505debed3f49d5ba177349d856d01a94bb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "88f0dfc5-8089-48ee-8136-195f86136e48": {"doc_hash": "429a5ea67de8b48cabcd8193bb6ebb00847ad70a5c8d77317ea908a07ac3e532", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c699add8-ae04-4aef-ad01-3c84a296ed71": {"doc_hash": "6ea9c82f6340d359bd686a956ee9651b7a78e9f5a7ca7d1d4f1a60d1c1f46c32", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "11fe265f-2e0a-40af-aea6-727a0e11b372": {"doc_hash": "2ff2effc4bbb556e064d8adb13b521510d85be23dcc25f83cca8d48b5ea18a1b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6739b1f4-a215-4bc8-b7b6-a283836831f5": {"doc_hash": "9a1321bada0a54486a99643fed01489d8426ed12e028a1b0d1af2feeca53bd31", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ed8fc279-0b74-4cf7-95d6-370b8c3d4947": {"doc_hash": "d8fb0922ffada62b212b55d4a2c51ede35045b33e57d5160ea4937eb3d4735e4", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "74c63ce1-903d-4ca4-8946-93693d9b20f0": {"doc_hash": "a25088991313303a76dd289ebfe8836d925f97eccf50662a348e78ee8f01dd30", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1ae66b9d-f22b-44c4-97b8-36a786893308": {"doc_hash": "9cc783c04448ba01601cd9451ed02ca7327474ec5904c6053e2c35f2db1c01ba", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2aa4fdc6-66ad-42a4-945d-03e69b64ee2b": {"doc_hash": "d054117d9f614cae5b829cb8b4dd0323cb091e7ccfd0977ba3891cb5cb06d407", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "47b5be23-9e06-4af9-9f41-35549a10b10d": {"doc_hash": "42657660262243a3217245413a0a43ddf1dba6f0017199843da8af1e74b00353", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8707c936-00f5-4c06-899c-b4f0bf6e37e7": {"doc_hash": "b14e65a8b1cc620bcb765ffae993cbd8b1803fe854d9223b9af2bad0a474480c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "cc1562e9-94b7-46b4-a5af-bed32a601635": {"doc_hash": "1a53338002d70d571757a06262d83bc34adcf6a4643602b281a6597730a64904", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "106181b1-d011-493a-8f44-3c522e0f6001": {"doc_hash": "18061973689fb0f4f9ca1b287e1a356661700b09f7ebb2ecc9e96773df2b6544", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0db5f796-3afd-4ea1-af06-858425a9b453": {"doc_hash": "80bd4cd10f91c47584db78c64842d1ac833d5400f13a81d1d9ddd2671e6b7320", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4c2bb0a0-a437-4fa0-8798-e6c70b96a812": {"doc_hash": "17f30b3aaf68bb9982dbe77088a99b96b6b1b106ad51cd770c4d1aec7b2589a9", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ac675ba0-5541-4aae-8e78-16be6378afa3": {"doc_hash": "6cb19a2703a75fcc238a6b7ad6816da4a15be42a83863da1e99953175c3976eb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0e50f682-48b8-4b6c-9d69-43188569c5fa": {"doc_hash": "c353d6e800a99843dc937cbceced32c00e7a37123e4d77f72de5ab600da80cc4", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a357e2bd-d2ff-41cc-8ee6-b94767dea07b": {"doc_hash": "1929fe15174ea06e41f8196c051fb7da61fc14c2ecfa5209f94a6cd0bba9370e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a36a69a1-4a79-48ed-abad-5aa841ce7775": {"doc_hash": "2d30a3cd345076b80511beaed63113f231d346fc3aff92a685082783b08458a7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "27a7693a-7691-4b80-acd0-a2a75c10b84f": {"doc_hash": "1ddcb365daa8d89a50f7f9034d8ee2078531282258ceac96606ef72e26156d2e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "628baf0b-57d6-4fbe-938a-75efa6958749": {"doc_hash": "daf5954921b8647e9901b4aeebc66fee0a72517432e1699b70ddef2fc6939a2a", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fdec1b10-2761-4c16-be59-a0d20bcc6032": {"doc_hash": "bcb32d9c406d696792b2cd336dcbc7f330dd04e935c7818a5b07388049f856f0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f592164c-a8ab-435c-a478-3faad276ca8f": {"doc_hash": "5109222dea30110e6926e046557e2d646b2ff452b405ed5d4eaa594e3955668f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9e1f96bf-f22b-4bfd-b6d9-f0aaaecbdbf0": {"doc_hash": "7ca95fefb8c57b4a75e8ddbbd58dbdaf597db4248457228ed5faadccfe7a5c25", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b982fdad-9671-43e9-be32-d523ba375968": {"doc_hash": "de5457cd02b9b3a270876d03f639a65416a7fc41ec06d3c93aa49bb2ad48cc5c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "094fe451-2ad4-4459-b64b-9d8aaff77ab4": {"doc_hash": "73881924c17dc1fd5b8ed0bb3793ddcf3629ae3b4c36f3f2f5b376ad191b3a62", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c9958109-b101-4252-a517-7a95b1ac6eca": {"doc_hash": "c88036a4940d0052c3cbf4a13207efa2a2ccec46c65f1113dbd0a6d0d37e0392", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4374f124-646e-4603-a50e-3cbd2f6ab560": {"doc_hash": "386dee69cfee323203216e14f96180678d8bf20a8a5c51fb7c5955c9cd62ee01", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8b2d69eb-edac-4d43-89a9-15e94830542a": {"doc_hash": "a49d9f0fa29b02d3a76b3546b0187e6f59fb7478d12725cc5cdae3b0bd57cfba", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2c34196f-f916-444b-9f42-869e6b7985db": {"doc_hash": "26cc0d01d4c85287761e6e35a210711bb243e20a74c3dea67d55367402b6ba38", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "3d699d0e-c727-4b35-b367-d393b51776a2": {"doc_hash": "88515b37ff41a6545ca841fd6990bcee54f5a170ed13bf8fb3aa9f822d04881c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "99666033-5b1d-4c46-94e7-1d052aaef210": {"doc_hash": "2dc1528b0d4e8cb6514cc6a69ad365fc6d82531c45399a943d2d3bd167b473ec", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b4be2129-2f4c-4e37-a259-a89b638ed331": {"doc_hash": "9adc5f7d3219de48fdea6613b7b6b5439c861bf9216c8813d59ef850e81c6791", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1187b050-891d-4705-8bed-3260d00376b1": {"doc_hash": "f819f1fb80adcb335696daf48c7214d7dbd7eee0cd0207b8f6fe084f32b79b44", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8e0122d6-a04e-4b6b-8f4f-e1ce16cc2411": {"doc_hash": "8942fecbf4cdf7dec17f1a282d16b2fb210905ba97bc4b1b868af4a0b829398e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9142af83-a119-4d22-9625-6ee2f3d695cd": {"doc_hash": "790cbe8f0219244ad888ec307de042e0263bb825e4c44336ec753d24b4212a07", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a2cc09fc-7357-4138-b0b2-04412c26f38c": {"doc_hash": "1b1d7334bdb92474f5e28bbf44b843735d9c777ebe78eae033ce85ee03f76f8b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1ce084b9-df7f-4e67-a48b-793abbf668eb": {"doc_hash": "f8c35c15831de534759a92c69e5b6321e4529c85d5a0691f9d76978ac296f1cd", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9b87cc37-621d-4ee3-b38f-9bc769c24bf8": {"doc_hash": "500fcb3760bddc2ac65b7d9c323374cf406c4f66c0470819bebac09fa3a24f22", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "06389bd4-0980-4b2e-987c-13042428129a": {"doc_hash": "7678a32c6b168bdf08422d6979ce5f9c76f589daee12538dfc35f424d311b156", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d75bbcc9-36c5-49b5-ac24-3de68abaef96": {"doc_hash": "c8184b259292f2eb05e771a992ef4893a00d3ec87ed0906847957310140e0b12", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "7a568791-ba68-48e3-93c0-4f5c90f7a846": {"doc_hash": "34246b70ce5ac308e508cca9316db9778fa108478b9dce652d11f81792371810", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1653500f-7229-4cb9-acab-b0f854653929": {"doc_hash": "c32dac2cf47f132c3ca8b16c3fec53b7e75342bd45ae508094393151fa535474", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "138559ae-0287-4566-8af9-d5733a46bd06": {"doc_hash": "fa1105d3674e215e66085684894930bc496dd31fd87622e4ad473ef6ba0e1ac5", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a7f9eab7-bfd8-49dd-9b20-62a2c0d0dc22": {"doc_hash": "c41df95b26a53c2755a8b46f400ebbacf9aecdf5ce66139657d55ef50c1defc3", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "88b948a0-b65a-406d-be91-70d816eb9b88": {"doc_hash": "668409e0e58b3eb24585c70967346712004f5b95435689e6d6b47bd70fb96d95", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b8dc0d2b-9396-4835-b36d-b895e0832c17": {"doc_hash": "c113663968cf30ee590852ff13da54070a155cead333dd0c09707c9fcb683546", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "12950db0-2646-4518-b126-e58a28f3eb2d": {"doc_hash": "bb501e31904cca7e125fd4bbd6567617e4ca17ea9579019270d8ccdc20b61937", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "85e6f023-ff97-4f52-b68a-0bb9c2fc10ed": {"doc_hash": "e6ea55780ef994c4773f9b675e3e892d2ba41bfa7f3631dd524d9e34ca0134bb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "bf5ef6e3-8946-4ae1-a3ab-128e8775483a": {"doc_hash": "8d0e1cef4c4cb311d3cb5acab566414f31d8f363265385e49dd3eb8c070a6813", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ddd1adc4-3bfd-4c11-8488-718c5c6633ae": {"doc_hash": "07a5953b20fda92d60f8ae16cef04952b5fa2be3418100cef52fa8c3cad5a160", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "40ffc149-c26f-44bc-bb7f-87e1859a7ef2": {"doc_hash": "959a5e7d95be29644e9929faa78e9927e69f43c8513faca125bb8f1adb6835be", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "75db9b41-7ab7-410d-86bc-e431fad9045a": {"doc_hash": "af486214f49ef98f0635eed6779b7e22e50d0fc64a77f243d481899e1af3f989", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c829b6a0-3957-4082-83fc-f95cdc2baff5": {"doc_hash": "884dde18458ba5843271f838f4b4bca0961c2b3251190d3788be0e8927b6b571", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f27722b8-21b8-4701-9e21-644db5d20f8e": {"doc_hash": "241b4d777037d3be914668056efc78c6c09cb6af1f5cfc57d079dc4eada8cdc3", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f62e14e1-3795-41bc-aa94-4778af734943": {"doc_hash": "9aeeb6a185a8e2692c48a8a546676c2c4e87c332c566b10a0598d4244a634057", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f21046b5-32ba-41af-bf41-3e7ba186b0cb": {"doc_hash": "f935ef720d9466287e8b3d34f15ff6f8cd355bee1b310d08ae223ffe0149324f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ea76a5da-62c4-4c27-a892-b96027a527aa": {"doc_hash": "d23259cfd15dddd1e92baa4cd4b299ca91f8a27faa39d8eb310b10b1ba27ac74", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "42a1e880-abd9-405f-bcfc-6269c4e96cf7": {"doc_hash": "1bb7bc36a3fa32a99a02549f17f7592ee074a69338105c0911b878b4c9a81a73", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "efae40fa-d98a-473d-b147-f989c42c928b": {"doc_hash": "0799184f63bd4c494eb564892bdcf04fe5b22cc925e3ee39039943e96b5dcd4f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "accee453-e9f1-47ae-8737-375f66f7e5b1": {"doc_hash": "c99dacf28578c4ab0b71ec460a5998cccb7ef5986bb5efc4e53287d65096c73e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d59a0348-2bab-4583-a064-1d5a1edad1cb": {"doc_hash": "4e62b617272a63840b3cf1d4205fc2402c30d942b32e5b5f0b2e1b37313543d0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "33e37030-09b0-4d8c-a256-6b5372789fad": {"doc_hash": "d2c93b3dfebf0845bbf318abba186b59d865d4c56fc8f3916af66ecfb9340009", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6d741be6-e4a0-4691-9f66-a3e2ec830329": {"doc_hash": "4a810e2a59548495eedbd811da1f3602364e9e30f6c9da4e9e11181c07a237e0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9a68da14-ddad-4ca3-bb15-3213f5d87e25": {"doc_hash": "5111134a27da4f30675dc73a4c4490e2e96fa58ce3a25fc1bfbbd2c0562716ff", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "bde180a5-a2a6-4265-8e5d-6464768e6d23": {"doc_hash": "25e641999ee603b96480f04eac2368bed75b1b5f917876d670e06417bfdb4867", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c0ff4ede-5af4-4928-a5ed-86a0856ce3b8": {"doc_hash": "d34f6ee0a2f5551ad356d5c0ac618724db8b9eac536428bcc36c5909a19a23af", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "75784e55-82ce-4f13-b205-aad6ba353694": {"doc_hash": "711ec24c0a4efa5ea06e43b9028c7e36f4e518d3024306da9f4799b40ab4632e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e6613249-ce2a-44cc-aaf7-2f3b091dec62": {"doc_hash": "738f9867ffc61d4854f34520255b654060c59ba04c4096f64316bb51b2544b3f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ec312de2-659f-4b0a-ae74-40b2bb52510f": {"doc_hash": "e95a34bbc96912f151d452f41ebb13ac9385b2785ec0c153268524bb3d37ce72", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8b3a24ce-7745-4efc-884f-29b7b4d06514": {"doc_hash": "3696f33b0c631bd13e39d7017b929375896aefe2d7bd628e97484e553ed51f1a", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "532dd7cb-6964-41f4-8a71-d609a5dc6462": {"doc_hash": "656de0576b9357c75ce86701f5d0f8c2037122fd0cbac438190937227a66f273", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "3bec0056-d0e6-4968-b8a4-888e28d931c3": {"doc_hash": "e24b25b9a26f4d2f8888b652de338d12bc3a487cf031c0e5e7fe19f086e64a65", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2c99b0a0-06a3-4dd8-aa03-e38afa073a24": {"doc_hash": "ec2d8fc6644ddc2e3a9816cc2139bb8ceced332044653d32baf8709ab295a2bc", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d03636a0-375f-4510-a332-2074242815e2": {"doc_hash": "1cdd022c3d3b60cfb83d4b846b7aa0ba16465970ef2dd73db78285dc269418f5", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e263d69b-6fc9-4c4d-802c-8eca6cee3de3": {"doc_hash": "e558e06ac0647f1665ed617faf09ea5554a38daf84997c0f863245cccd7e41c8", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "23039490-0bbc-45c0-972a-d8bba6dbe2ff": {"doc_hash": "f16582729259a1ba2603308e64a24eee2e40db4d86c79a136af585f07433ee22", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "420b0c1e-f5db-4009-b628-70fc56dee777": {"doc_hash": "364a098479f8104a2bc878ccbbd3067a02f0fa0d689065410aa4d5d82795d99e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d41be0a5-45cb-45f3-932e-92cb40ce8f7b": {"doc_hash": "ae86f3f62a6efc70cc3049c5efb36fc65113f63f4b68cba5f2fd9066fd182de5", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "00409930-21fb-4c06-9f09-0d202f60a222": {"doc_hash": "3465d067d14eb93c78c4f26f387d66b58fd86ec6b6fe2b5b4be7016cef953c53", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "30a68976-0a54-4426-bf39-a2931902602e": {"doc_hash": "8c019088420dbe4800ecffe5f02cfcba51ab02b1b62ed2c149ee1ebd5978c2fa", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5f288f02-f821-4a60-9448-1b89db8228d5": {"doc_hash": "42d91dd76cb8113016cd2e998d2a559aac9752ef365540e562eedb5a08cc49c5", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fc14d6ec-6f8d-489c-952a-e6469e55cde0": {"doc_hash": "69bbb08896058af6e1c29d4b6d29ecf63147b00ce3b9f616eefb846fefe649d2", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "062c1682-bd6e-4eb1-a2c1-7c0e71563322": {"doc_hash": "74d7e0adbae54a93a0775e8d688407eafe832d86772d09dbe3b2a72dc1a65ee5", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5699fd5b-61dc-458f-8e38-178607fa0099": {"doc_hash": "5683496c83362fb09a0737eb20a800d4a7ecd298b367aa7833da96e3c125aac0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e920d973-62a5-4562-808f-3fac72752ad8": {"doc_hash": "61e10d09961e809d7b570e881419aaecd6d1c064b6c4116c3427da1b849ac303", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "85e3746d-3039-415d-9bae-05db41ef764d": {"doc_hash": "4520c7855439e71f389c9f47824b47c79129a93186326b0191e3196c2dd1656d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8c04fabe-a4f2-4383-88db-b725b774eb26": {"doc_hash": "5780de3a3883f8d1d38cf923a27779c0aac4b0bf3dae7c8e68b435047ba61087", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0ff1413b-5f9a-4e1a-a019-025d7a49d225": {"doc_hash": "d23e75707b2f78c520f83e73540808a66618bd01b6f7a7f72b1c68d18425fd69", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c7c72136-3670-4b55-8ce3-f6b3f0e789a3": {"doc_hash": "30d060e1997327db6a03d5935fe2ab3a60a27da6210bd9f218998c86221e4be0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6163482f-820a-418b-9661-b0cb8dc43cb9": {"doc_hash": "25bf4661c0386faf432a20fe2d404f232654ee64c6678ed6c8c03c2b7290f255", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f1686d47-c496-4758-b9a4-871a97c9cb88": {"doc_hash": "967ea8231804556dd22f56acd033932752102f6ac3c3b8c821a4210465322aa7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5462c98a-04ba-4403-a0f8-1ebec84e51e6": {"doc_hash": "d9c9328dd3e488ac55420f20f3086fa3d0781ca588477c7b78e7a414b335cab8", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "847e0e08-a132-4209-8307-ced74cb732de": {"doc_hash": "0136e6e7e7fd5cc117de71054594cf386d597d2084cc52eb7711cb14bff950d3", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "72b7c5fa-0ed3-486c-8668-af259444fe20": {"doc_hash": "e6f130841ada63c6a77542c9f0cfeebe9f01dc92711c3a503fa69c882897ed13", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c7a37038-0020-4f69-974d-7197b38efe7b": {"doc_hash": "af6fd0c5bfff75a5c60002f4a4206640955d7363f376853d111190fbd973ea1c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "bf64f5dc-7e41-47f6-928b-1686c0d8fc08": {"doc_hash": "94940a57f50f40c2e2d63972503708e3c25e75247dfec360a3cd71d413d36de6", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "191e9baf-9ac0-461c-89df-7610bd035032": {"doc_hash": "b6fb2096560a088046123e6fbd2eeff7615570d9b19a571afb0b66c9b21d947d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c2e8882d-16d3-4aa3-8930-23ddb198e069": {"doc_hash": "52a183d5ada86eddcb8ef47c0d015ee12d4a15057a4d0036dd306d5b4eee420b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "459fb65c-c455-404e-b8e9-2f9e02703923": {"doc_hash": "d038ce9a9b7ccb71ca2c0ea0cc14d4aa43a4c46410d9d9e9403f083b0abef047", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "bbf4ad60-a8d6-4e9d-a057-97aca6a5200a": {"doc_hash": "fd7096c80e4c0c3ed662a50dfaba466b48b6c8ef78add369cd1ae0621b4777ac", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "177c2f40-d84c-4c93-bb4c-ae6b137c9183": {"doc_hash": "82a221b29e7343d729828a43268d659a1e97eb82e873139111db5065dd559644", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "34502be7-bfc7-46c1-9604-50e6dd824f6c": {"doc_hash": "fbbf090e44aa73dbcce179159f2da282108935318ab29bf857a50e85d10ad2b7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c9c6b1bb-c4a8-44fa-acd8-fbaffa2f66e2": {"doc_hash": "b8f297dc9993cb4ebf9732dedbfd2ad94df02515b1fb739705570fa40a3360d7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d6cc1e19-6600-47e5-8784-657a63fb56ba": {"doc_hash": "d4d96d983bf0b5bfc517c1e8f4deb1bcce736ad5bfa5523eb06be22e2033907c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "83db6689-2c97-42cd-a930-8cd411607d48": {"doc_hash": "c84aca7a5acf6f66a15dc743286ac5d842408d329fcb79d96bf86edbb6936a83", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8bf7bfaa-a348-4cfb-a5ce-25cd1640b7c3": {"doc_hash": "1cdaa9a76863e8382ebd28ba19bf66f35b552a87190db94d959002e917bfb6d0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9429b9e7-db88-4d6f-b65e-bb0a58b9c4d9": {"doc_hash": "901ca1fcda0b8e85fb0fd5e24f936a0e499fb04beb56f4026bec649f4de59e79", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "53ec3d4f-92a0-4bf1-99b5-857819ffb954": {"doc_hash": "e64a1dea14ca6932572514b67381fddb4eb5e4bf3a9967f81ce3ce53ac7059f1", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ec984e97-9f20-4218-8b92-dd7613a4373c": {"doc_hash": "d06c69f248e8d308c54b9b23e59c5c7a818c700d8cdcc5ae9ad30de34ee0b5be", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "65ada733-c0b5-438b-aed8-eb4b63c86d84": {"doc_hash": "47b220423ebd28ce0ce17f730fee742321071232051e34d644743dafd9fd6da3", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2f22c501-dad9-4b0f-80d2-80abbad2a24a": {"doc_hash": "2021d10cbc480bb1a236d352217c046536713a58e2e4a0558bb23255bef94100", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6873a094-14cf-4617-a09e-61dac8f36718": {"doc_hash": "e00f416f3c4a18fe57d636887c601f789fc3155a1ea999a7425f828464da42dc", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "92425318-9c80-489a-9f8a-3a0b8cee8060": {"doc_hash": "5c0628374696490f6fbe8e27040858bd037bfb6b232b143b15afc26cad02a289", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a529a13f-9be0-4d92-8f91-f98d197b6c10": {"doc_hash": "acb89a112521fb8b3320fddb0b67cf4a06c786bc903ad9edb8fbd30e6f43aa2b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2e35420c-2831-49a2-a160-be38c0744ab4": {"doc_hash": "b5a680eb88887e91f267f2a5eb68fa7c904a5348d2285e1f438e542cda7989be", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e28aded2-d5bc-41ed-a6a7-b06b9394eec2": {"doc_hash": "e1cee393bdc21c87dc773b32c7751c9620743a634ac42fc291032b3ca55a7a66", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0db4fa9b-7ea0-4d85-a118-e477bb74a518": {"doc_hash": "ea7f20bebe8fc168883dbdfd9b2b95161e99907ba2c43826047b80bb31420d28", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2060ad82-81e9-4cca-a8d8-b7586a14ea3b": {"doc_hash": "46d11c61c57ab2b71694869a6711f4c23679e6a8e30184babdc428c8a2d20906", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c5a04e3c-efdd-4f5a-bd6f-68c0925dc560": {"doc_hash": "dfd63b9c4433a4ab99a6233c4509c13604885a9c6696e2773c9047dc0554af97", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0a64d6e8-0861-4c23-86c3-49333b3019af": {"doc_hash": "eb95d6384254fc7c125660b3be408d1db5867bb6ab34f54e2f52bdf6cc7eb85e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4d27e5f2-34eb-4be9-90be-8a0f5b013d59": {"doc_hash": "5d95edd2cf20ab74743c33ec56c6f2cd67e59e0220aa7fad2d6a63ca8c50017f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d13602c7-e2fc-4d1b-9e09-cf2aa00c10f3": {"doc_hash": "333295e8dcfc3405d89500cdb27c2304d26a625e61aed097e27aa14fc446a4f3", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "abf580f6-c398-46fb-8fc9-dc85d5cd5ae7": {"doc_hash": "94ec75124265a7b0ab91b6c1e473bb212b45b8a7124e9f4dbdd6e01e8ef8773b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "57e88ba8-6b58-4f81-9a12-9924cdb71b02": {"doc_hash": "70b9ba57ed1af47ab6d1e77891c07955e114a2a970f8074dc5cb526c890faa61", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6e9a4118-c64f-4e63-845b-9be417029609": {"doc_hash": "1cbed606c1f12ec9034ced937821bc8be21f9b15091c02b66f28415c62e995de", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fb93cf69-40da-49da-a3a5-ab55fd505349": {"doc_hash": "72041f5f8f216a33056fba15ca0dd8b921657d611eeb91f4f302336f01a31900", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "03815ba2-b7d6-4a77-ac75-7a0fabd71986": {"doc_hash": "c773d10e56581ede197f77b54c60fd5918278bc69b8ac995661c623700f17c45", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "93a556a6-dfa3-43c2-8393-0c3f2fdfb287": {"doc_hash": "a3ffce74440dbcc1ac7ce73855348c24453ae690b2ecf642dc8f9cbc03a08e2c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "7022c02a-b7c8-4b74-bc05-6d85cc64d970": {"doc_hash": "9ec01c9b6bd3f22b5652eafecb08fd4d71986e78e873178c3c0d3494834209fd", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "59e202f0-43ad-491d-a6b0-341b8c44835e": {"doc_hash": "d1a895470170aa2a347acf4cad7a17815ab988ad020fd8c92c1e2f031d1e30ea", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6ecb7077-e3cc-4273-9d29-aa2ded06b35c": {"doc_hash": "4579e589d83659d5ac232cf6ccc89b4cff94e3504a7c769912cf27f211d7d683", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b9de6d03-c5bf-4188-aa4d-f36b689bf778": {"doc_hash": "d6056c2840b0e2d924b69a6b8faedfec6c0095f216ff605a8269711699a93aa8", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f3e7f4c3-ed31-4b5b-9e1f-2718a2feaf57": {"doc_hash": "db2e0a7927297f0dbd0eef8a7304851ef04e4d4e77bcd34d05e106a90f6bf5cf", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9c944e79-f082-4245-890f-1eb7527c33c3": {"doc_hash": "dc0097b8b89dffb8ada7c2785236d227680fec8e46c510e9d2614c1da22b0ace", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f758398e-1207-4eab-a692-79cb91e16ddb": {"doc_hash": "c2d3f5e310bcc9fe9029a9031c32da29c8ba7e5f235ca4c7e9a212cfaada86dc", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c4a22827-c5ea-4b70-b245-d8d489cf8c4e": {"doc_hash": "0aa96469eaead54440710e6254e42d8ab21a8886bd51734171f74bcfeedc06bb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0370139b-f5bb-4595-8844-24ad6c591e3b": {"doc_hash": "4c6ea1bf0baffb61a433cf335ae8b0c18d4e6d202cf4de783fe905bc0fa26f91", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8f37f03d-6041-47ee-9dac-4ee809cc9a79": {"doc_hash": "c710d16b3b4d17a0feb6428477433e4bd473bb59533e2e97cd383f7e89cfc739", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "49291c0e-8875-40a9-bf5b-6b3669ef40d2": {"doc_hash": "f947251c7df86b6f5127408448c00c42f3f61f7e67da390f6e70209d1ba607c7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "96442ac1-aab5-4bcd-8103-2439404c425f": {"doc_hash": "1b6bf76faa4bd84426b9bd18481b3f38dbf623205708477bf0a8f40a02b9fc67", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a18b94c9-7960-471e-94da-ec912ba378c6": {"doc_hash": "ae0c307aedcd65bd993559df01f3461ee6184d010872f1802a70b47ee49a3335", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8e600726-a887-4f38-980f-8d3994b3cab8": {"doc_hash": "8e8f2d8d94fffdcf24eb7b6d35432ce7bb86e725d271c24fd901047eecc8cb89", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "326849ec-436d-4d08-8669-07fc2943db33": {"doc_hash": "ace3a5fccd71788f649abadc70cd7f16b1fda4d91ac4e43bcf61374f2bc0df8c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5c51e14d-d7e3-43a3-a045-9275372bffff": {"doc_hash": "94f583556ace9afbc2c7db208188d2cf58853bb2ec179f825b7f91a9835eb2ee", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c210d2bd-b338-4521-a789-b2a0fcb0d4ad": {"doc_hash": "6bd0d80040b1124b9a366cc4ddceb781150b85561c2ac229a51606d9d66c927d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a18427c8-588a-4b66-98cc-1e201bb71878": {"doc_hash": "eb7e5389e4f7893d796a54a096bf51eaa41b5efda517271cfb5472ef861d940e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5cbaf369-7254-4cbf-846d-7be657588a32": {"doc_hash": "a948c72b32a16f452c4f624a4cc40cd383cdf49d0f8372d1f326188efbc9dfda", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d6ead0c8-734f-427f-ac69-7127b48b1a90": {"doc_hash": "7f8f0ea3790c7e4abd95a48fb1b7191a937e2dcadca2a932a26554183a9f5e4a", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "24c3e3e8-efc2-4f39-91f6-e829045fb8bd": {"doc_hash": "78cdc89cc73eac27d59c9798b6555842898c2dfe874448acfc3ff3b6a060f76d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8c513cb8-2408-48b8-a891-e003715c0e8c": {"doc_hash": "8fe77e842ffd44808a7fef63e02e119b4b1cb1fbcd70af2e5d4d42ead3b3fa40", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2103f2fe-67fd-48c4-b09f-d307bd588ba7": {"doc_hash": "6ab1f47624ef88b5272d8c918996e7f8c5f684a42ece41a51a9630cebdbbd454", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4b6b8ed5-f11e-4447-af50-c84abe7d6e71": {"doc_hash": "d6d141e4a071c2072a5f20bbea990e1b631e2d43e1d8ebaaad9e3fbe29b1aee8", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1e2164d4-4403-48f2-8cc3-ae9b1f355d0b": {"doc_hash": "4d93a3eea5ce542dc438eb31684fce68f5651022e3b4a4c708224d6ba6f0f3c0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "4ac30dfb-59b8-4569-92b6-97fddd82d9c2": {"doc_hash": "270527d1df631da1e9a491664a22c98d7cbf374ad098ac2002d9eff0b78a5010", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "eaf3ecf2-ec1b-4017-b3bb-f6c559cd0dc3": {"doc_hash": "3eb6e036dbc16a964908fc138df4b60cef67fec85d9c4c07f5bbaf75edcf73e9", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "72d700b1-7006-4036-9e49-ed14f623e07b": {"doc_hash": "f706bc92b4f7d47f236a13628d5f510bb6d669ab45f37081dd3ef698a93c140e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "84e501dd-0d65-4647-a63b-c54d04ba6a23": {"doc_hash": "776e24f60236785d1102d80009a2475c723c619b6f73a6fb9d2c624fde691189", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "037c9411-0b54-46b1-a04f-b2182c60c05f": {"doc_hash": "157074d6a90f7aa1da8d12ca82923a28c7e0915afa8d03603d4091269b521beb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8a509aca-42ce-4374-96f7-dd0cd4c30cf6": {"doc_hash": "de2765d91c8aa33f715f15db6c4492a3ee80681020a813061e77590dedfd2abb", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0c4b8b07-0362-4b22-bef5-48a9ad7aa0ce": {"doc_hash": "f51a2732d5cf5772bfe2abc2cc4a9bdf8fa281eacee3abbcc6d5df1b253f7727", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c8fb9d63-7c1d-4c61-8758-cd4c2d05e129": {"doc_hash": "0c0a6906c6691e632536599276e79139f980cbd0d830d5de688e158cb1e77827", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a453ecb5-4b7e-49ba-8f59-161acbf1b7a5": {"doc_hash": "09d7983c7e5e47ea0085268788e7a3cac76b5eed0577814047923246655d7efd", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "28f008a9-c46d-4bd6-a312-6a61302e9d10": {"doc_hash": "a448594f217798ce753a629a1f1f1db6fb0a63148c367d84ea71daf3f822401c", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5d2972ca-596b-4290-a032-1241ee3e961d": {"doc_hash": "c2424b9d70c483932704774f51e068697da182290f385509b5da143981ec08f6", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "255e4044-0811-4fa6-afbb-e8f9766265fc": {"doc_hash": "12c9c33ba762fa46cc149c73e1c7f5097135fbe55a2758dadb47327b3f905a25", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "c429d203-51f9-4ed0-8feb-c064dd691a26": {"doc_hash": "1c3e68b256356019023146c030eb08e5d1724500af6d896f30b16a43a6c7b9a4", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6256c042-d5a2-4b2e-84ae-e5847c379ea4": {"doc_hash": "12d8fd67b77417cc7d7d3192eab7ac40248d70f888263169924343d2af1a113a", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e5d8d974-4dae-4d0a-acda-8bd2a85fac3a": {"doc_hash": "084bcbe5e6c96f2f9b7b0f4ffef26de6e9e75a07ce54d7f482fbdbb1ee6e50f6", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d10958b9-acf8-4b66-bf0d-119a1bb4c426": {"doc_hash": "30a450f474d6e1ab51a083857bcf7d886e075420ebf2691f029c5f4479cc07cd", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d65e4e5f-2f8b-4008-9690-e238a8296c7a": {"doc_hash": "3c11fb4c099b06fa704c6857e521688f6af87963474c7af72d19e5648f280353", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "3a34442b-61d6-4dae-8b01-92b5efb0094f": {"doc_hash": "fed978c3353b3de403a96bde7e62ddaa4b50d4364eafb61a5863b4d9d12b2468", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a12a086f-8050-42a7-8181-0ac6b85729e1": {"doc_hash": "b77f0f23ec5c280278d6edabac06a82ef6b744bd541533dff29faf5f4fae9183", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0335873e-3aa0-4506-a7b7-4690a2e68bb6": {"doc_hash": "3970b75d13871fa7ed0861acc3ef4fe7316733b18c7a42785dbd5f2e1dadf224", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "2b5a6a95-caaa-4f8a-945c-3d34a6ac882c": {"doc_hash": "466ad1640b98c3b65a61b567353bc320f993ac6538657cd2e7e39059296e70b2", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ee0d5445-16d6-4917-b09b-7906da577015": {"doc_hash": "ea8952a56c0e6e67033f8e55048f112038a5660a0ef0c6981cfad60a497f8db6", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "0ce658c3-b31b-431d-bf98-26cbd5d7f09c": {"doc_hash": "45a9783c0f7a91ae458dd8cfc76fa385776d59c7c3214c807cec17beae903083", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "eb574db1-05b9-43aa-a2a7-1d8a87e840ec": {"doc_hash": "f405de633b68a88f72f44e08c503158b1fce2e7ee29472b09a4f7fa1e1589d77", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "ba55dfa6-9270-4263-9dc6-b2037681f58f": {"doc_hash": "1505c7a449e0dc3f3ecbd9dc34e8cbc641f901285edf3b5a1ad8cf4d0b86b745", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "165373b1-92d7-4268-81c5-dcf60334509e": {"doc_hash": "5c7af8a2879a0826605db27b574c8fe0a9aab6193a344489fc26dd609f37135e", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "85cc280c-d80b-4685-b078-18f6265d3f3a": {"doc_hash": "e04f84a2d7ca6a4be3250d5bd51bfaff2219a5df7856462fafe7e034c7d0a07d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8db87774-86cc-42c3-8c61-f0bb11a4a244": {"doc_hash": "8452fde33c0a87cf95753a12baa396ea2cdd9528d4a298a726693ce577cfe44d", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "35396054-dce0-4d75-9d39-69257b167109": {"doc_hash": "4e9c875316f172e4d95270c8ca92ee28316eed272cf8fe616b457bbcba3d0ce1", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "10b2345f-1cc9-480c-94e6-da748865f8f0": {"doc_hash": "c4c0f6efc290869428d8dd99065960bea9efa33efe2a96dc2d0ed3a0419db074", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "f9cf1f8d-9418-4c80-865d-8ed77c43e5ad": {"doc_hash": "2e476bff762d9d1f11e73a6ce709a32bbb194617b1b093de367a15efe78720e0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "98e918cb-5ed2-4b2a-bca9-c31fe9855542": {"doc_hash": "4839f20b5de63b54a41ed67a0abd76fb677b26b8ec751e627c8a920ea95e9826", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "17a5f90a-3136-48b7-9e74-7bd070e29622": {"doc_hash": "6eb0eac9c1af992503e30d27a113ca4aede333a03aea4bcd2f7adb8ae854ec70", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "25ea7b49-8d3c-45e2-8973-7a180fe8a3c5": {"doc_hash": "03bd3ce65c1e6e1f23093f155c4bdbc4ca7b4c079c90a12e7105d61a7db0ebf4", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "8eb67c40-95d4-435a-abb9-6e05532abed3": {"doc_hash": "9d394c7ee012a249d46c6283f9c00a8b03b14703254c5f0c59228421a8c9aa4a", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "74aec0da-3561-4221-b8b7-06b44613523c": {"doc_hash": "be2cf93b4dbd46c2e641dccd6a7cdc59586f377bffef24a176fd7172f3219e44", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "d482e5c2-06bc-49b1-b0a7-420870fbc5c9": {"doc_hash": "64b5bf1d2809dee1bfa0ee7e33fbb4e6c214b803fc15eae96de7918e197d8ba3", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "6b9500a4-104a-4df4-8af8-69dbd910919d": {"doc_hash": "703927ffe38466d012f4dda92a117d15b2315f395007f51c5d290953577bdc61", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "42172975-271a-4f2c-963c-bf69dc19d372": {"doc_hash": "7674391cc3ba64a1e1093269b294d9a42d9dea28c880065494913055b3f9d6fa", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "65639533-2c06-42b7-942d-a179ca466071": {"doc_hash": "0e363095105e06818068c211ace88c0be8cba6d3d7bd742510d983423c7906e1", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "bf694b2a-ebc0-4104-9ceb-e9135c4e0c9e": {"doc_hash": "b5eac12afc093d4475815a6e4ebc9d9d6f65c196fed675d79ed6a3ae50538556", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "b687cddf-0d26-4b3a-8d22-4599af98a136": {"doc_hash": "1f922a8912a98d46a0441340007caf174ee6b93b492df1af0b37d6a3bba5fbd5", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "fa65661f-b8f0-4bd2-9006-cacc49dbe0b9": {"doc_hash": "dc15b5f753b3b43823488acb8a20fdb32848edfeca47b8f652e9af9b327be30f", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1a4a58ec-1f38-48a2-b681-288ddbf8f9d5": {"doc_hash": "2e750a2a5830d584f1c5b7e9aafcf97690db392bfc9cb14348c42b4ce9b15784", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "87204183-7605-4b7d-9a74-209b6c42f56a": {"doc_hash": "434c029c4fe45cfdb8a593b3856df0804ba73f425d806561f06cda1b05e55fea", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "e3f9cff1-848d-4a6d-aa5d-45a8c42c6460": {"doc_hash": "07111d5b0d63586161f0beade63a5d6d391b5c9911c33997049667b36c70ce5b", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "88a52315-686a-43c3-a74f-b9148b6d1636": {"doc_hash": "e87486e637a6b666d940ece3200a675c4a38c9277f968f02fbcbc6fb4c02c615", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "5a29b8a2-c3d9-4b87-89d8-0f6dfc7463a6": {"doc_hash": "2b8e33760341a0ea68b7800150620919f1c0b556344605aca4c38bd6fdd68e44", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "44d34cf5-c09c-47cf-9e57-07e00fcecc58": {"doc_hash": "1152cb35d3a2f146adcc1d8cb7fb9dbd268adad4a9a1eb92cca5b07087390f33", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "1cf7edbe-fbc2-4a75-be09-0f45826a650a": {"doc_hash": "8f2fa1100308cf5a90834ce354aafe6398129a32449db13b8d069763eee003ec", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "a4ab4967-60f2-4284-bc8b-3dc0d246bae9": {"doc_hash": "23091acfbe716fd9a7a14a1cbb29bf3a6031769bd00ece7c89a81a9916d03ca7", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "be473101-fa7b-47ee-bbf7-16f3a5a33b2d": {"doc_hash": "b58a369194de5e61e6d3b0dc78ab44493a05ddcb94cd74787d3fc2cc7ac12ed0", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "89e282ab-f157-47ac-a376-83421206924b": {"doc_hash": "8f546316752cb4f21ec794d2dca89002218765cbae3b2c78b75b515046b29500", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}, "9b8d4282-4923-40e4-a74d-37303e6ee270": {"doc_hash": "fb5f34827e98e69a23cfb439422d28a24d26001fc709c548a9441100c117c5af", "ref_doc_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075"}}, "docstore/data": {"547f99a0-9005-45f5-8d9c-1416d3ffef92": {"__data__": {"id_": "547f99a0-9005-45f5-8d9c-1416d3ffef92", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a704551f-348d-4902-9fd4-fe0d4dbaffa2", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n", "original_text": "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n"}, "hash": "73c7ae513af7285b3507f2c97c0bb37d193b4adb5f337ed4b69582a94cd7f7de", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n", "start_char_idx": 0, "end_char_idx": 500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a704551f-348d-4902-9fd4-fe0d4dbaffa2": {"__data__": {"id_": "a704551f-348d-4902-9fd4-fe0d4dbaffa2", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n", "original_text": "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "547f99a0-9005-45f5-8d9c-1416d3ffef92", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n"}, "hash": "2bd6e2ffd37d18eb7fd6a04f153818236ad58cc69de988206e1a0a722b98f4ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df2673b4-45d9-4d98-af32-7e6f50c4b1fe", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n", "original_text": "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n"}, "hash": "9e5270cbf15251726380b33a0941bfe2e9196df8be76ef21fb5a620c61f3a2db", "class_name": "RelatedNodeInfo"}}, "text": "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n", "start_char_idx": 500, "end_char_idx": 634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df2673b4-45d9-4d98-af32-7e6f50c4b1fe": {"__data__": {"id_": "df2673b4-45d9-4d98-af32-7e6f50c4b1fe", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n", "original_text": "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a704551f-348d-4902-9fd4-fe0d4dbaffa2", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n", "original_text": "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n"}, "hash": "73c7ae513af7285b3507f2c97c0bb37d193b4adb5f337ed4b69582a94cd7f7de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "330fbd79-b681-4383-8558-f1b285544663", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n", "original_text": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n"}, "hash": "10476fec678e9549d9f44c5f94493134488b40c2f8a04c21b67c841198dec83f", "class_name": "RelatedNodeInfo"}}, "text": "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n", "start_char_idx": 634, "end_char_idx": 896, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "330fbd79-b681-4383-8558-f1b285544663": {"__data__": {"id_": "330fbd79-b681-4383-8558-f1b285544663", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n", "original_text": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df2673b4-45d9-4d98-af32-7e6f50c4b1fe", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n", "original_text": "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n"}, "hash": "9e5270cbf15251726380b33a0941bfe2e9196df8be76ef21fb5a620c61f3a2db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "176923e6-80f8-42ae-87e0-e95abd4fc6ba", "node_type": "1", "metadata": {"window": "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n", "original_text": "BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n"}, "hash": "d2681bac4b10efc0f1ea518754629b55a612d4241cb583db5b299e46659ca4cb", "class_name": "RelatedNodeInfo"}}, "text": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n", "start_char_idx": 896, "end_char_idx": 1125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "176923e6-80f8-42ae-87e0-e95abd4fc6ba": {"__data__": {"id_": "176923e6-80f8-42ae-87e0-e95abd4fc6ba", "embedding": null, "metadata": {"window": "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n", "original_text": "BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "330fbd79-b681-4383-8558-f1b285544663", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n", "original_text": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n"}, "hash": "10476fec678e9549d9f44c5f94493134488b40c2f8a04c21b67c841198dec83f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1347e842-ed25-47ae-ab26-989559fa8cd6", "node_type": "1", "metadata": {"window": "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n", "original_text": "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n"}, "hash": "cdc54048968980a5f8350007ad10da708c33148873b85ee34c15a687d41030cb", "class_name": "RelatedNodeInfo"}}, "text": "BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n", "start_char_idx": 1125, "end_char_idx": 1236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1347e842-ed25-47ae-ab26-989559fa8cd6": {"__data__": {"id_": "1347e842-ed25-47ae-ab26-989559fa8cd6", "embedding": null, "metadata": {"window": "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n", "original_text": "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "176923e6-80f8-42ae-87e0-e95abd4fc6ba", "node_type": "1", "metadata": {"window": "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n", "original_text": "BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n"}, "hash": "d2681bac4b10efc0f1ea518754629b55a612d4241cb583db5b299e46659ca4cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ac97b78-3252-432b-a143-1a49729894c5", "node_type": "1", "metadata": {"window": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n", "original_text": "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n"}, "hash": "352d73a07f0bf5ad435babe3601dcb7527d6e122f3bae34ea1970ac41e847174", "class_name": "RelatedNodeInfo"}}, "text": "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n", "start_char_idx": 1236, "end_char_idx": 1475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ac97b78-3252-432b-a143-1a49729894c5": {"__data__": {"id_": "5ac97b78-3252-432b-a143-1a49729894c5", "embedding": null, "metadata": {"window": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n", "original_text": "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1347e842-ed25-47ae-ab26-989559fa8cd6", "node_type": "1", "metadata": {"window": "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n", "original_text": "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n"}, "hash": "cdc54048968980a5f8350007ad10da708c33148873b85ee34c15a687d41030cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81c5b805-9a95-46c8-9620-f1232187a2b7", "node_type": "1", "metadata": {"window": "BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n", "original_text": "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n"}, "hash": "be62fff35d7b60bed1d260bd72ebe5fa410104773431037d32593a640e41ecd7", "class_name": "RelatedNodeInfo"}}, "text": "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n", "start_char_idx": 1475, "end_char_idx": 1609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81c5b805-9a95-46c8-9620-f1232187a2b7": {"__data__": {"id_": "81c5b805-9a95-46c8-9620-f1232187a2b7", "embedding": null, "metadata": {"window": "BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n", "original_text": "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ac97b78-3252-432b-a143-1a49729894c5", "node_type": "1", "metadata": {"window": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n", "original_text": "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n"}, "hash": "352d73a07f0bf5ad435babe3601dcb7527d6e122f3bae34ea1970ac41e847174", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d22f1c7-5ff2-4376-ae27-1be353771545", "node_type": "1", "metadata": {"window": "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g. ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n"}, "hash": "41ea14f30df58948278104c7d233912ea7759ed02f601bed6415a162980a1df8", "class_name": "RelatedNodeInfo"}}, "text": "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n", "start_char_idx": 1609, "end_char_idx": 1782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d22f1c7-5ff2-4376-ae27-1be353771545": {"__data__": {"id_": "6d22f1c7-5ff2-4376-ae27-1be353771545", "embedding": null, "metadata": {"window": "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g. ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81c5b805-9a95-46c8-9620-f1232187a2b7", "node_type": "1", "metadata": {"window": "BART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n", "original_text": "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n"}, "hash": "be62fff35d7b60bed1d260bd72ebe5fa410104773431037d32593a640e41ecd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60c8a8df-9867-4032-ab78-d0cfe0720527", "node_type": "1", "metadata": {"window": "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc.", "original_text": "The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n"}, "hash": "e076167e6f756430b78cb519fd68b5cf9f5b0ef4727fa5c277dece04232ea6e9", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n", "start_char_idx": 1782, "end_char_idx": 2143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60c8a8df-9867-4032-ab78-d0cfe0720527": {"__data__": {"id_": "60c8a8df-9867-4032-ab78-d0cfe0720527", "embedding": null, "metadata": {"window": "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc.", "original_text": "The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d22f1c7-5ff2-4376-ae27-1be353771545", "node_type": "1", "metadata": {"window": "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g. ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n"}, "hash": "41ea14f30df58948278104c7d233912ea7759ed02f601bed6415a162980a1df8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2670f080-137c-4464-9f0a-43ee1441290f", "node_type": "1", "metadata": {"window": "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n", "original_text": "Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n"}, "hash": "c1602c94b4ba798f2907da8837e0c86f8a9768835cf31a1d6739cd3e5bd596f0", "class_name": "RelatedNodeInfo"}}, "text": "The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n", "start_char_idx": 2143, "end_char_idx": 2346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2670f080-137c-4464-9f0a-43ee1441290f": {"__data__": {"id_": "2670f080-137c-4464-9f0a-43ee1441290f", "embedding": null, "metadata": {"window": "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n", "original_text": "Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60c8a8df-9867-4032-ab78-d0cfe0720527", "node_type": "1", "metadata": {"window": "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc.", "original_text": "The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n"}, "hash": "e076167e6f756430b78cb519fd68b5cf9f5b0ef4727fa5c277dece04232ea6e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57555c7a-62b8-4624-89f2-b79f64a40300", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n", "original_text": "However, these methods typically focus on particular types of end tasks (e.g. "}, "hash": "029fbe68b3e8841bf85ca509271bf297952bd2b6f1d65afbed5a9f58f8e73ae7", "class_name": "RelatedNodeInfo"}}, "text": "Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n", "start_char_idx": 2346, "end_char_idx": 2586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57555c7a-62b8-4624-89f2-b79f64a40300": {"__data__": {"id_": "57555c7a-62b8-4624-89f2-b79f64a40300", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n", "original_text": "However, these methods typically focus on particular types of end tasks (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2670f080-137c-4464-9f0a-43ee1441290f", "node_type": "1", "metadata": {"window": "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most in\ufb02uence end-task performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n", "original_text": "Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n"}, "hash": "c1602c94b4ba798f2907da8837e0c86f8a9768835cf31a1d6739cd3e5bd596f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc6519b1-a119-4d24-bcb8-eaa3ef83c1ec", "node_type": "1", "metadata": {"window": "The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n", "original_text": "span prediction, generation, etc."}, "hash": "fd7c7586f57f7d19fedba6d3e10e3c2d52537ecab00657170420fe7488a110a1", "class_name": "RelatedNodeInfo"}}, "text": "However, these methods typically focus on particular types of end tasks (e.g. ", "start_char_idx": 2586, "end_char_idx": 2664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc6519b1-a119-4d24-bcb8-eaa3ef83c1ec": {"__data__": {"id_": "bc6519b1-a119-4d24-bcb8-eaa3ef83c1ec", "embedding": null, "metadata": {"window": "The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n", "original_text": "span prediction, generation, etc."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57555c7a-62b8-4624-89f2-b79f64a40300", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nmethods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n", "original_text": "However, these methods typically focus on particular types of end tasks (e.g. "}, "hash": "029fbe68b3e8841bf85ca509271bf297952bd2b6f1d65afbed5a9f58f8e73ae7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81f28ccc-9706-488d-afbb-33eef33899c5", "node_type": "1", "metadata": {"window": "Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n", "original_text": "), limiting their applicability.\n\n"}, "hash": "db20817fa108d82185c088adc874c6f8b7ada8fae8b56fb87de7fbe24df0a184", "class_name": "RelatedNodeInfo"}}, "text": "span prediction, generation, etc.", "start_char_idx": 2664, "end_char_idx": 2697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81f28ccc-9706-488d-afbb-33eef33899c5": {"__data__": {"id_": "81f28ccc-9706-488d-afbb-33eef33899c5", "embedding": null, "metadata": {"window": "Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n", "original_text": "), limiting their applicability.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc6519b1-a119-4d24-bcb8-eaa3ef83c1ec", "node_type": "1", "metadata": {"window": "The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n", "original_text": "span prediction, generation, etc."}, "hash": "fd7c7586f57f7d19fedba6d3e10e3c2d52537ecab00657170420fe7488a110a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5b71df2-552f-4e14-b735-350f72c9bc35", "node_type": "1", "metadata": {"window": "However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n"}, "hash": "95f5f64ed550e94f3bdc5d11c5576d435eda63ac5ac4cfad253d4440e6fb50f7", "class_name": "RelatedNodeInfo"}}, "text": "), limiting their applicability.\n\n", "start_char_idx": 2697, "end_char_idx": 2731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5b71df2-552f-4e14-b735-350f72c9bc35": {"__data__": {"id_": "d5b71df2-552f-4e14-b735-350f72c9bc35", "embedding": null, "metadata": {"window": "However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81f28ccc-9706-488d-afbb-33eef33899c5", "node_type": "1", "metadata": {"window": "Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n", "original_text": "), limiting their applicability.\n\n"}, "hash": "db20817fa108d82185c088adc874c6f8b7ada8fae8b56fb87de7fbe24df0a184", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1639d32f-7f09-46a7-9836-66830e316fbc", "node_type": "1", "metadata": {"window": "span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n", "original_text": "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n"}, "hash": "e18494a9bd6792c5a39245586b917dc0c998cae20447982f96cc0bb5fcb1b6dc", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n", "start_char_idx": 2731, "end_char_idx": 3014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1639d32f-7f09-46a7-9836-66830e316fbc": {"__data__": {"id_": "1639d32f-7f09-46a7-9836-66830e316fbc", "embedding": null, "metadata": {"window": "span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n", "original_text": "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5b71df2-552f-4e14-b735-350f72c9bc35", "node_type": "1", "metadata": {"window": "However, these methods typically focus on particular types of end tasks (e.g.  span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n"}, "hash": "95f5f64ed550e94f3bdc5d11c5576d435eda63ac5ac4cfad253d4440e6fb50f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "388685a8-36c3-465a-bfae-c8f9009edd45", "node_type": "1", "metadata": {"window": "), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n", "original_text": "Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n"}, "hash": "56c86d9b2092ee2d02aec6076369abc222b581bbcb98b6721206586c01479587", "class_name": "RelatedNodeInfo"}}, "text": "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n", "start_char_idx": 3014, "end_char_idx": 3140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "388685a8-36c3-465a-bfae-c8f9009edd45": {"__data__": {"id_": "388685a8-36c3-465a-bfae-c8f9009edd45", "embedding": null, "metadata": {"window": "), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n", "original_text": "Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1639d32f-7f09-46a7-9836-66830e316fbc", "node_type": "1", "metadata": {"window": "span prediction, generation, etc. ), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n", "original_text": "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n"}, "hash": "e18494a9bd6792c5a39245586b917dc0c998cae20447982f96cc0bb5fcb1b6dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c8c0ba7-54f9-463c-82ee-5ff4b4c6375d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n", "original_text": "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n"}, "hash": "c5c2874c7c480935820cd5b1d5b3b057daf7f24ad4c68402393e5d3577c92ec5", "class_name": "RelatedNodeInfo"}}, "text": "Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n", "start_char_idx": 3140, "end_char_idx": 3307, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c8c0ba7-54f9-463c-82ee-5ff4b4c6375d": {"__data__": {"id_": "2c8c0ba7-54f9-463c-82ee-5ff4b4c6375d", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n", "original_text": "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "388685a8-36c3-465a-bfae-c8f9009edd45", "node_type": "1", "metadata": {"window": "), limiting their applicability.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n", "original_text": "Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n"}, "hash": "56c86d9b2092ee2d02aec6076369abc222b581bbcb98b6721206586c01479587", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f8372cf-fe56-4ca9-91b8-744d80142b12", "node_type": "1", "metadata": {"window": "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n"}, "hash": "6055470d0fd5a4b44887bb8b7d8dbb74f0fe19e5cd467240bfbba9cad73e158d", "class_name": "RelatedNodeInfo"}}, "text": "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n", "start_char_idx": 3307, "end_char_idx": 3587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f8372cf-fe56-4ca9-91b8-744d80142b12": {"__data__": {"id_": "3f8372cf-fe56-4ca9-91b8-744d80142b12", "embedding": null, "metadata": {"window": "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c8c0ba7-54f9-463c-82ee-5ff4b4c6375d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n", "original_text": "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n"}, "hash": "c5c2874c7c480935820cd5b1d5b3b057daf7f24ad4c68402393e5d3577c92ec5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a47188c-fded-44cf-a828-6595064fe1c2", "node_type": "1", "metadata": {"window": "Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n", "original_text": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n"}, "hash": "91b4fd6d7d4671f723d37297f08ee25afe11ecaa2813cff4524b26876f7db835", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n", "start_char_idx": 3587, "end_char_idx": 3906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a47188c-fded-44cf-a828-6595064fe1c2": {"__data__": {"id_": "7a47188c-fded-44cf-a828-6595064fe1c2", "embedding": null, "metadata": {"window": "Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n", "original_text": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f8372cf-fe56-4ca9-91b8-744d80142b12", "node_type": "1", "metadata": {"window": "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n"}, "hash": "6055470d0fd5a4b44887bb8b7d8dbb74f0fe19e5cd467240bfbba9cad73e158d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "529c262f-c6d1-4328-9782-e160f69a4651", "node_type": "1", "metadata": {"window": "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n", "original_text": "This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n"}, "hash": "86ca4b0746477ad186c8fa1b228888c48a026e45640d0ac70ccf7d4cf76a631d", "class_name": "RelatedNodeInfo"}}, "text": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n", "start_char_idx": 3906, "end_char_idx": 4176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "529c262f-c6d1-4328-9782-e160f69a4651": {"__data__": {"id_": "529c262f-c6d1-4328-9782-e160f69a4651", "embedding": null, "metadata": {"window": "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n", "original_text": "This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a47188c-fded-44cf-a828-6595064fe1c2", "node_type": "1", "metadata": {"window": "Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n", "original_text": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n"}, "hash": "91b4fd6d7d4671f723d37297f08ee25afe11ecaa2813cff4524b26876f7db835", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd87d5d7-e94b-47c0-8873-0d28e7eca961", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n"}, "hash": "561de8570895d75120e76fcd458864175088b62f4cf880750352b13b956e7a2d", "class_name": "RelatedNodeInfo"}}, "text": "This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n", "start_char_idx": 4176, "end_char_idx": 4395, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd87d5d7-e94b-47c0-8873-0d28e7eca961": {"__data__": {"id_": "cd87d5d7-e94b-47c0-8873-0d28e7eca961", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "529c262f-c6d1-4328-9782-e160f69a4651", "node_type": "1", "metadata": {"window": "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n", "original_text": "This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n"}, "hash": "86ca4b0746477ad186c8fa1b228888c48a026e45640d0ac70ccf7d4cf76a631d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72db6013-3836-44fe-8c7d-201fc624d34f", "node_type": "1", "metadata": {"window": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n", "original_text": "It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n"}, "hash": "9bdad05cdaac28a9cc663a37fb3c9961d42386b3742b1cc9ad307d9b35215241", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n", "start_char_idx": 4395, "end_char_idx": 4674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72db6013-3836-44fe-8c7d-201fc624d34f": {"__data__": {"id_": "72db6013-3836-44fe-8c7d-201fc624d34f", "embedding": null, "metadata": {"window": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n", "original_text": "It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd87d5d7-e94b-47c0-8873-0d28e7eca961", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nA key advantage of this setup is the noising \ufb02exibility; arbitrary transformations can be applied to the original text, including changing its length.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n"}, "hash": "561de8570895d75120e76fcd458864175088b62f4cf880750352b13b956e7a2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a072151a-ab27-46fa-b8a3-cc20e8345dba", "node_type": "1", "metadata": {"window": "This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n", "original_text": "For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n"}, "hash": "0886a300a92a5937d8ffbad7e04c4a7c5ffe37443d9be7bc4f61b9c5e1978f42", "class_name": "RelatedNodeInfo"}}, "text": "It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n", "start_char_idx": 4674, "end_char_idx": 4953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a072151a-ab27-46fa-b8a3-cc20e8345dba": {"__data__": {"id_": "a072151a-ab27-46fa-b8a3-cc20e8345dba", "embedding": null, "metadata": {"window": "This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n", "original_text": "For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72db6013-3836-44fe-8c7d-201fc624d34f", "node_type": "1", "metadata": {"window": "We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n", "original_text": "It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n"}, "hash": "9bdad05cdaac28a9cc663a37fb3c9961d42386b3742b1cc9ad307d9b35215241", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "401f55b0-539d-4506-a21f-3e0b83d1d304", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n"}, "hash": "a0fd730278b5966d6ba32374daffeeaf1260e6bb9277e6c46df52e790b7ed4c2", "class_name": "RelatedNodeInfo"}}, "text": "For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n", "start_char_idx": 4953, "end_char_idx": 5053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "401f55b0-539d-4506-a21f-3e0b83d1d304": {"__data__": {"id_": "401f55b0-539d-4506-a21f-3e0b83d1d304", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a072151a-ab27-46fa-b8a3-cc20e8345dba", "node_type": "1", "metadata": {"window": "This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n", "original_text": "For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n"}, "hash": "0886a300a92a5937d8ffbad7e04c4a7c5ffe37443d9be7bc4f61b9c5e1978f42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a0fe853-350c-458b-b4b0-d344fa75973a", "node_type": "1", "metadata": {"window": "It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n", "original_text": "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n"}, "hash": "853fbde1c09da3884d8c77870b04b715f5403e8821adfc3fd8002d9823e22213", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n", "start_char_idx": 5053, "end_char_idx": 5279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a0fe853-350c-458b-b4b0-d344fa75973a": {"__data__": {"id_": "2a0fe853-350c-458b-b4b0-d344fa75973a", "embedding": null, "metadata": {"window": "It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n", "original_text": "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "401f55b0-539d-4506-a21f-3e0b83d1d304", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART is particularly effective when \ufb01ne tuned for text generation but also works well for comprehension tasks.\n It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n"}, "hash": "a0fd730278b5966d6ba32374daffeeaf1260e6bb9277e6c46df52e790b7ed4c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db6ebf2f-1bd8-485f-b807-d7cfe1013727", "node_type": "1", "metadata": {"window": "For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n", "original_text": "These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n"}, "hash": "bc19a3f7b3016dd64c1a6fef00f460b679a5ab2bfd4a23868cd30c6d945b7d7e", "class_name": "RelatedNodeInfo"}}, "text": "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n", "start_char_idx": 5279, "end_char_idx": 5400, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db6ebf2f-1bd8-485f-b807-d7cfe1013727": {"__data__": {"id_": "db6ebf2f-1bd8-485f-b807-d7cfe1013727", "embedding": null, "metadata": {"window": "For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n", "original_text": "These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a0fe853-350c-458b-b4b0-d344fa75973a", "node_type": "1", "metadata": {"window": "It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n", "original_text": "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n"}, "hash": "853fbde1c09da3884d8c77870b04b715f5403e8821adfc3fd8002d9823e22213", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5077afc-82e5-4125-9ca2-d6202cc52226", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n", "original_text": "Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n"}, "hash": "9c893ec86051ec490b3b4b66c7fc5f01d0d1b3abf6a92319765dc90cb4fc8815", "class_name": "RelatedNodeInfo"}}, "text": "These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n", "start_char_idx": 5400, "end_char_idx": 6002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5077afc-82e5-4125-9ca2-d6202cc52226": {"__data__": {"id_": "d5077afc-82e5-4125-9ca2-d6202cc52226", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n", "original_text": "Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db6ebf2f-1bd8-485f-b807-d7cfe1013727", "node_type": "1", "metadata": {"window": "For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n", "original_text": "These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n"}, "hash": "bc19a3f7b3016dd64c1a6fef00f460b679a5ab2bfd4a23868cd30c6d945b7d7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9bcefab-a990-49dd-b7ad-253014eee8e7", "node_type": "1", "metadata": {"window": "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n", "original_text": "However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n"}, "hash": "5ab2d64faeae16ceb86f6a448cc3cc47c85b89997877e2426a99bceb002b4227", "class_name": "RelatedNodeInfo"}}, "text": "Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n", "start_char_idx": 6002, "end_char_idx": 6160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9bcefab-a990-49dd-b7ad-253014eee8e7": {"__data__": {"id_": "b9bcefab-a990-49dd-b7ad-253014eee8e7", "embedding": null, "metadata": {"window": "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n", "original_text": "However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5077afc-82e5-4125-9ca2-d6202cc52226", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction\nBART also opens up new ways of thinking about \ufb01ne tuning.\n We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n", "original_text": "Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n"}, "hash": "9c893ec86051ec490b3b4b66c7fc5f01d0d1b3abf6a92319765dc90cb4fc8815", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef6b795e-15d2-4366-b0cb-05f979ee0072", "node_type": "1", "metadata": {"window": "These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n"}, "hash": "9b53c811b452890e515d4b53f011fc6de6a92867ebe1936566afb46fa000b21d", "class_name": "RelatedNodeInfo"}}, "text": "However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n", "start_char_idx": 6160, "end_char_idx": 6264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef6b795e-15d2-4366-b0cb-05f979ee0072": {"__data__": {"id_": "ef6b795e-15d2-4366-b0cb-05f979ee0072", "embedding": null, "metadata": {"window": "These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9bcefab-a990-49dd-b7ad-253014eee8e7", "node_type": "1", "metadata": {"window": "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n", "original_text": "However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n"}, "hash": "5ab2d64faeae16ceb86f6a448cc3cc47c85b89997877e2426a99bceb002b4227", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "742f0c59-f766-4b35-9088-c1b222883235", "node_type": "1", "metadata": {"window": "Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n"}, "hash": "f2a6a6c39b4e183c030bcadb1c14a6bfe277c67260839a1554656ccb0613fca7", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n", "start_char_idx": 6264, "end_char_idx": 6504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "742f0c59-f766-4b35-9088-c1b222883235": {"__data__": {"id_": "742f0c59-f766-4b35-9088-c1b222883235", "embedding": null, "metadata": {"window": "Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef6b795e-15d2-4366-b0cb-05f979ee0072", "node_type": "1", "metadata": {"window": "These layers are trained to essentially translate the foreign language to noised\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > B D A B C D E\nAutoregressive Decoder\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\n(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n"}, "hash": "9b53c811b452890e515d4b53f011fc6de6a92867ebe1936566afb46fa000b21d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12ef0ec8-b8b8-46c5-b5ce-57d24e1db48e", "node_type": "1", "metadata": {"window": "However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n", "original_text": "Here, a document has been corrupted by replacing spans of text with mask symbols.\n"}, "hash": "8ac98ba19ae2af1ed7f3a4656d830bf0ff9917d1591aca40e9fc6c5d2d3c9d55", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n", "start_char_idx": 6504, "end_char_idx": 7156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12ef0ec8-b8b8-46c5-b5ce-57d24e1db48e": {"__data__": {"id_": "12ef0ec8-b8b8-46c5-b5ce-57d24e1db48e", "embedding": null, "metadata": {"window": "However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n", "original_text": "Here, a document has been corrupted by replacing spans of text with mask symbols.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "742f0c59-f766-4b35-9088-c1b222883235", "node_type": "1", "metadata": {"window": "Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n"}, "hash": "f2a6a6c39b4e183c030bcadb1c14a6bfe277c67260839a1554656ccb0613fca7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5586a07d-3dfb-415b-ae4a-a5c7d6cd688b", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n", "original_text": "The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n"}, "hash": "1bf2c73faacb4d660d52e29d8e9eafc2aa19001130df5c11080f75e6ad05a409", "class_name": "RelatedNodeInfo"}}, "text": "Here, a document has been corrupted by replacing spans of text with mask symbols.\n", "start_char_idx": 7156, "end_char_idx": 7238, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5586a07d-3dfb-415b-ae4a-a5c7d6cd688b": {"__data__": {"id_": "5586a07d-3dfb-415b-ae4a-a5c7d6cd688b", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n", "original_text": "The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12ef0ec8-b8b8-46c5-b5ce-57d24e1db48e", "node_type": "1", "metadata": {"window": "However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n", "original_text": "Here, a document has been corrupted by replacing spans of text with mask symbols.\n"}, "hash": "8ac98ba19ae2af1ed7f3a4656d830bf0ff9917d1591aca40e9fc6c5d2d3c9d55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd04c7ec-e7dd-4b77-b28d-f900980fb8af", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n", "original_text": "For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n"}, "hash": "bb9dc89b58d4d59e7b3011d78d0b4a75dda150638c3b526d6e327c9ab70d4d29", "class_name": "RelatedNodeInfo"}}, "text": "The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n", "start_char_idx": 7238, "end_char_idx": 7410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd04c7ec-e7dd-4b77-b28d-f900980fb8af": {"__data__": {"id_": "bd04c7ec-e7dd-4b77-b28d-f900980fb8af", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n", "original_text": "For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5586a07d-3dfb-415b-ae4a-a5c7d6cd688b", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D\nused for generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n", "original_text": "The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n"}, "hash": "1bf2c73faacb4d660d52e29d8e9eafc2aa19001130df5c11080f75e6ad05a409", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e3bcdc6-9cf7-4470-9422-9aeb5e7592e4", "node_type": "1", "metadata": {"window": "Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n"}, "hash": "afb0506ba364e3a50857f6a12cc235b5b6a379361a2bcb339c110a2dcecd4ea1", "class_name": "RelatedNodeInfo"}}, "text": "For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n", "start_char_idx": 7410, "end_char_idx": 7563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e3bcdc6-9cf7-4470-9422-9aeb5e7592e4": {"__data__": {"id_": "3e3bcdc6-9cf7-4470-9422-9aeb5e7592e4", "embedding": null, "metadata": {"window": "Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd04c7ec-e7dd-4b77-b28d-f900980fb8af", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n | Bidirectional Encoder | Autoregressive Decoder\n | A _ B _ E <s> A B C D\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n", "original_text": "For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n"}, "hash": "bb9dc89b58d4d59e7b3011d78d0b4a75dda150638c3b526d6e327c9ab70d4d29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f65f0633-9e26-48bb-b1ea-dd02dbe3e01b", "node_type": "1", "metadata": {"window": "The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n"}, "hash": "4afac4b14c0bed447f156fb9fca2852cec80ddc0f5598b482816c5105c50c6c1", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n", "start_char_idx": 7563, "end_char_idx": 7899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f65f0633-9e26-48bb-b1ea-dd02dbe3e01b": {"__data__": {"id_": "f65f0633-9e26-48bb-b1ea-dd02dbe3e01b", "embedding": null, "metadata": {"window": "The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e3bcdc6-9cf7-4470-9422-9aeb5e7592e4", "node_type": "1", "metadata": {"window": "Here, a document has been corrupted by replacing spans of text with mask symbols.\n The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n"}, "hash": "afb0506ba364e3a50857f6a12cc235b5b6a379361a2bcb339c110a2dcecd4ea1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e41bf38-5f72-4c16-9ab9-a88497033515", "node_type": "1", "metadata": {"window": "For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n", "original_text": "This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n"}, "hash": "a281e561e33bb7035a0c96f231e15e7adaec93be1ed109def732b5929cc086c1", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n", "start_char_idx": 7899, "end_char_idx": 8231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e41bf38-5f72-4c16-9ab9-a88497033515": {"__data__": {"id_": "5e41bf38-5f72-4c16-9ab9-a88497033515", "embedding": null, "metadata": {"window": "For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n", "original_text": "This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f65f0633-9e26-48bb-b1ea-dd02dbe3e01b", "node_type": "1", "metadata": {"window": "The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n"}, "hash": "4afac4b14c0bed447f156fb9fca2852cec80ddc0f5598b482816c5105c50c6c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aab105bc-aa0c-4c68-b413-4328dd3c6e36", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n"}, "hash": "4683a01a19512858473971abf4b78d9070ee9552079acc0ef50be85b4e44c40b", "class_name": "RelatedNodeInfo"}}, "text": "This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n", "start_char_idx": 8231, "end_char_idx": 8361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aab105bc-aa0c-4c68-b413-4328dd3c6e36": {"__data__": {"id_": "aab105bc-aa0c-4c68-b413-4328dd3c6e36", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e41bf38-5f72-4c16-9ab9-a88497033515", "node_type": "1", "metadata": {"window": "For \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal hidden state of the decoder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n", "original_text": "This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n"}, "hash": "a281e561e33bb7035a0c96f231e15e7adaec93be1ed109def732b5929cc086c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20d1ca9b-f018-4854-baed-4b4cff004803", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n", "original_text": "This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n"}, "hash": "8d8431195a48678287b5a66a9730759697064502673c0fd506398fb5f476971a", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n", "start_char_idx": 8361, "end_char_idx": 8724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20d1ca9b-f018-4854-baed-4b4cff004803": {"__data__": {"id_": "20d1ca9b-f018-4854-baed-4b4cff004803", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n", "original_text": "This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aab105bc-aa0c-4c68-b413-4328dd3c6e36", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n"}, "hash": "4683a01a19512858473971abf4b78d9070ee9552079acc0ef50be85b4e44c40b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "988e70b2-8d3d-46cd-a228-f1ca7e9e6d56", "node_type": "1", "metadata": {"window": "This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n", "original_text": "We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n"}, "hash": "c7ebf573366e4f5452fc9ef2ef2803f92df38e01edf05fc7ea824578f2d31d66", "class_name": "RelatedNodeInfo"}}, "text": "This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n", "start_char_idx": 8724, "end_char_idx": 8960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "988e70b2-8d3d-46cd-a228-f1ca7e9e6d56": {"__data__": {"id_": "988e70b2-8d3d-46cd-a228-f1ca7e9e6d56", "embedding": null, "metadata": {"window": "This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n", "original_text": "We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20d1ca9b-f018-4854-baed-4b4cff004803", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n", "original_text": "This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n"}, "hash": "8d8431195a48678287b5a66a9730759697064502673c0fd506398fb5f476971a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75278cee-8655-491b-ac50-2f87977ef4eb", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n"}, "hash": "413864b3e770c0bd6b1b52daa5d39dc572766a3394a9e0cd12485822de465701", "class_name": "RelatedNodeInfo"}}, "text": "We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n", "start_char_idx": 8960, "end_char_idx": 9072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75278cee-8655-491b-ac50-2f87977ef4eb": {"__data__": {"id_": "75278cee-8655-491b-ac50-2f87977ef4eb", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "988e70b2-8d3d-46cd-a228-f1ca7e9e6d56", "node_type": "1", "metadata": {"window": "This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n", "original_text": "We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n"}, "hash": "c7ebf573366e4f5452fc9ef2ef2803f92df38e01edf05fc7ea824578f2d31d66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ff0f7b0-9beb-482c-889d-6a554935fd76", "node_type": "1", "metadata": {"window": "This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n", "original_text": "It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n"}, "hash": "bf5c90f205069d1b36cccdaa66d5f18a309864c7ce12e1e2df3ec92c4e27eab8", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n", "start_char_idx": 9072, "end_char_idx": 9337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ff0f7b0-9beb-482c-889d-6a554935fd76": {"__data__": {"id_": "0ff0f7b0-9beb-482c-889d-6a554935fd76", "embedding": null, "metadata": {"window": "This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n", "original_text": "It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75278cee-8655-491b-ac50-2f87977ef4eb", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 1 Introduction > Bidirectional Encoder > A _ C _ E > <s> A B C D > A B C D E\nTo better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n"}, "hash": "413864b3e770c0bd6b1b52daa5d39dc572766a3394a9e0cd12485822de465701", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31b435c3-8b65-44fd-8028-a1def5d48cc0", "node_type": "1", "metadata": {"window": "We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n", "original_text": "For pre-training, we optimize the negative log likelihood of the original document.\n\n"}, "hash": "a8fa987af543b423a77f57d0f9eee99cd2db10423d567f1550ebe1a09c4ed26b", "class_name": "RelatedNodeInfo"}}, "text": "It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n", "start_char_idx": 9337, "end_char_idx": 9480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31b435c3-8b65-44fd-8028-a1def5d48cc0": {"__data__": {"id_": "31b435c3-8b65-44fd-8028-a1def5d48cc0", "embedding": null, "metadata": {"window": "We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n", "original_text": "For pre-training, we optimize the negative log likelihood of the original document.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ff0f7b0-9beb-482c-889d-6a554935fd76", "node_type": "1", "metadata": {"window": "This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n", "original_text": "It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n"}, "hash": "bf5c90f205069d1b36cccdaa66d5f18a309864c7ce12e1e2df3ec92c4e27eab8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b926bfe-6217-4111-860f-87600a16e9da", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n"}, "hash": "440b90879e9b7e247995ec282707e6d53b8e6661ef15369b7619ac1c0c7edcff", "class_name": "RelatedNodeInfo"}}, "text": "For pre-training, we optimize the negative log likelihood of the original document.\n\n", "start_char_idx": 9480, "end_char_idx": 9565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b926bfe-6217-4111-860f-87600a16e9da": {"__data__": {"id_": "4b926bfe-6217-4111-860f-87600a16e9da", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31b435c3-8b65-44fd-8028-a1def5d48cc0", "node_type": "1", "metadata": {"window": "We \ufb01nd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n", "original_text": "For pre-training, we optimize the negative log likelihood of the original document.\n\n"}, "hash": "a8fa987af543b423a77f57d0f9eee99cd2db10423d567f1550ebe1a09c4ed26b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "617fc7c9-0a88-4fff-a94c-0c9dd3bd9166", "node_type": "1", "metadata": {"window": "It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n", "original_text": "For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n"}, "hash": "86c06ab4cb17589764eb349697cb47f6f69e14c53292ec5a022448fe235e7af7", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n", "start_char_idx": 9565, "end_char_idx": 9987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "617fc7c9-0a88-4fff-a94c-0c9dd3bd9166": {"__data__": {"id_": "617fc7c9-0a88-4fff-a94c-0c9dd3bd9166", "embedding": null, "metadata": {"window": "It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n", "original_text": "For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b926bfe-6217-4111-860f-87600a16e9da", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model\nis a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n"}, "hash": "440b90879e9b7e247995ec282707e6d53b8e6661ef15369b7619ac1c0c7edcff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e7f7552-472f-43e5-b71f-83b5cd97eeac", "node_type": "1", "metadata": {"window": "For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n", "original_text": "The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n"}, "hash": "77d0780990c925d375e4e4c67ee78986977076c3b9804b4e82768d3581cdb821", "class_name": "RelatedNodeInfo"}}, "text": "For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n", "start_char_idx": 9987, "end_char_idx": 10103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e7f7552-472f-43e5-b71f-83b5cd97eeac": {"__data__": {"id_": "8e7f7552-472f-43e5-b71f-83b5cd97eeac", "embedding": null, "metadata": {"window": "For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n", "original_text": "The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "617fc7c9-0a88-4fff-a94c-0c9dd3bd9166", "node_type": "1", "metadata": {"window": "It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n", "original_text": "For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n"}, "hash": "86c06ab4cb17589764eb349697cb47f6f69e14c53292ec5a022448fe235e7af7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d469d2ee-1416-4b6a-951a-e62b49418299", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n", "original_text": "In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n"}, "hash": "3ed262ad40cb00d5f8aed7d92f0413bc151d1d5ecf58d1f456c0353906a1b2a7", "class_name": "RelatedNodeInfo"}}, "text": "The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n", "start_char_idx": 10103, "end_char_idx": 10452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d469d2ee-1416-4b6a-951a-e62b49418299": {"__data__": {"id_": "d469d2ee-1416-4b6a-951a-e62b49418299", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n", "original_text": "In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e7f7552-472f-43e5-b71f-83b5cd97eeac", "node_type": "1", "metadata": {"window": "For pre-training, we optimize the negative log likelihood of the original document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n", "original_text": "The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n"}, "hash": "77d0780990c925d375e4e4c67ee78986977076c3b9804b4e82768d3581cdb821", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e3fc467-9f60-4880-af4b-96e583e27ce2", "node_type": "1", "metadata": {"window": "For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n"}, "hash": "991ad04d8f585ed6e7fe14d3bc75bf7773c02c0e17c6e4aa4dd32bebfb4d7a23", "class_name": "RelatedNodeInfo"}}, "text": "In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n", "start_char_idx": 10452, "end_char_idx": 10545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e3fc467-9f60-4880-af4b-96e583e27ce2": {"__data__": {"id_": "9e3fc467-9f60-4880-af4b-96e583e27ce2", "embedding": null, "metadata": {"window": "For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d469d2ee-1416-4b6a-951a-e62b49418299", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.1 Architecture\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n", "original_text": "In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n"}, "hash": "3ed262ad40cb00d5f8aed7d92f0413bc151d1d5ecf58d1f456c0353906a1b2a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4befc74-4ea9-4ff6-a214-346c05bc8eba", "node_type": "1", "metadata": {"window": "The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n", "original_text": "Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n"}, "hash": "e05c259f521b9028324959ea2160efc992f76d833e9e8f41a02973d25b1b06b3", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n", "start_char_idx": 10545, "end_char_idx": 10886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4befc74-4ea9-4ff6-a214-346c05bc8eba": {"__data__": {"id_": "f4befc74-4ea9-4ff6-a214-346c05bc8eba", "embedding": null, "metadata": {"window": "The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n", "original_text": "Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e3fc467-9f60-4880-af4b-96e583e27ce2", "node_type": "1", "metadata": {"window": "For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n"}, "hash": "991ad04d8f585ed6e7fe14d3bc75bf7773c02c0e17c6e4aa4dd32bebfb4d7a23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fd81ab0-44f5-4db2-a45d-a36e0eb02a4d", "node_type": "1", "metadata": {"window": "In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n", "original_text": "In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n"}, "hash": "2f806e20a429e42b3a99429a48969ed46c2cb1ac25083d1ca5197f0cf39fc422", "class_name": "RelatedNodeInfo"}}, "text": "Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n", "start_char_idx": 10886, "end_char_idx": 11030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fd81ab0-44f5-4db2-a45d-a36e0eb02a4d": {"__data__": {"id_": "0fd81ab0-44f5-4db2-a45d-a36e0eb02a4d", "embedding": null, "metadata": {"window": "In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n", "original_text": "In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4befc74-4ea9-4ff6-a214-346c05bc8eba", "node_type": "1", "metadata": {"window": "The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the \ufb01nal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n", "original_text": "Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n"}, "hash": "e05c259f521b9028324959ea2160efc992f76d833e9e8f41a02973d25b1b06b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97888f81-147c-45b9-af5b-fe7d6755510a", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n"}, "hash": "0d8d4407aefee091ca04861753e3265635f0cb6b29cf3a56bc2ac92e2b044b44", "class_name": "RelatedNodeInfo"}}, "text": "In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n", "start_char_idx": 11030, "end_char_idx": 11140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97888f81-147c-45b9-af5b-fe7d6755510a": {"__data__": {"id_": "97888f81-147c-45b9-af5b-fe7d6755510a", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fd81ab0-44f5-4db2-a45d-a36e0eb02a4d", "node_type": "1", "metadata": {"window": "In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n", "original_text": "In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n"}, "hash": "2f806e20a429e42b3a99429a48969ed46c2cb1ac25083d1ca5197f0cf39fc422", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42609e00-26a7-47c3-adff-77188ce87923", "node_type": "1", "metadata": {"window": "Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n", "original_text": "The transformations we used are summarized below, and examples are shown in Figure 2.\n\n"}, "hash": "7263eca56535895bbfef083176b873eb1bd5200637d7f2bf191a5d05a4589781", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n", "start_char_idx": 11140, "end_char_idx": 11489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42609e00-26a7-47c3-adff-77188ce87923": {"__data__": {"id_": "42609e00-26a7-47c3-adff-77188ce87923", "embedding": null, "metadata": {"window": "Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n", "original_text": "The transformations we used are summarized below, and examples are shown in Figure 2.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97888f81-147c-45b9-af5b-fe7d6755510a", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nBART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document.\n Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n"}, "hash": "0d8d4407aefee091ca04861753e3265635f0cb6b29cf3a56bc2ac92e2b044b44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e836b2ca-c5e4-4382-8146-482db695aa28", "node_type": "1", "metadata": {"window": "In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C . ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n"}, "hash": "3f5f0be4067925d5f25e60654436c563645ab9bf55db05bf956fef3bcef812ef", "class_name": "RelatedNodeInfo"}}, "text": "The transformations we used are summarized below, and examples are shown in Figure 2.\n\n", "start_char_idx": 11489, "end_char_idx": 11576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e836b2ca-c5e4-4382-8146-482db695aa28": {"__data__": {"id_": "e836b2ca-c5e4-4382-8146-482db695aa28", "embedding": null, "metadata": {"window": "In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C . ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42609e00-26a7-47c3-adff-77188ce87923", "node_type": "1", "metadata": {"window": "Unlike existing denoising autoencoders, which are tailored to speci\ufb01c noising schemes, BART allows us to apply any type of document corruption.\n In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n", "original_text": "The transformations we used are summarized below, and examples are shown in Figure 2.\n\n"}, "hash": "7263eca56535895bbfef083176b873eb1bd5200637d7f2bf191a5d05a4589781", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f20d9cb-b092-4c18-b149-cf207e22a228", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E . ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n"}, "hash": "c75fa05e32d8286e21247d7b75953a751267d1b625cf4b818ec18ecc7ecf998f", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n", "start_char_idx": 11576, "end_char_idx": 11875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f20d9cb-b092-4c18-b149-cf207e22a228": {"__data__": {"id_": "4f20d9cb-b092-4c18-b149-cf207e22a228", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E . ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e836b2ca-c5e4-4382-8146-482db695aa28", "node_type": "1", "metadata": {"window": "In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C . ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n"}, "hash": "3f5f0be4067925d5f25e60654436c563645ab9bf55db05bf956fef3bcef812ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "371ce4fb-cf20-4b1e-9172-0139f172c6a1", "node_type": "1", "metadata": {"window": "The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E . ", "original_text": "In contrast to token masking, the model must decide which positions are missing inputs.\n\n"}, "hash": "44f5cfc8e194beda7d33ce37b4206e2cb83809f1e77cb744758a1f67246b8c42", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n", "start_char_idx": 11875, "end_char_idx": 12117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "371ce4fb-cf20-4b1e-9172-0139f172c6a1": {"__data__": {"id_": "371ce4fb-cf20-4b1e-9172-0139f172c6a1", "embedding": null, "metadata": {"window": "The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E . ", "original_text": "In contrast to token masking, the model must decide which positions are missing inputs.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f20d9cb-b092-4c18-b149-cf207e22a228", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nWe experiment with several previously proposed and novel transformations, but we believe there is a signi\ufb01cant potential for development of other new alternatives.\n The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E . ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n"}, "hash": "c75fa05e32d8286e21247d7b75953a751267d1b625cf4b818ec18ecc7ecf998f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b806241-bebb-4935-8d40-483024eb9fe7", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C . ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C . "}, "hash": "cf98198232ba8a95c46916bbf874e414b6ec5e1a1969ed7be769c0c6f8dde26e", "class_name": "RelatedNodeInfo"}}, "text": "In contrast to token masking, the model must decide which positions are missing inputs.\n\n", "start_char_idx": 12117, "end_char_idx": 12206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b806241-bebb-4935-8d40-483024eb9fe7": {"__data__": {"id_": "0b806241-bebb-4935-8d40-483024eb9fe7", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C . ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "371ce4fb-cf20-4b1e-9172-0139f172c6a1", "node_type": "1", "metadata": {"window": "The transformations we used are summarized below, and examples are shown in Figure 2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E . ", "original_text": "In contrast to token masking, the model must decide which positions are missing inputs.\n\n"}, "hash": "44f5cfc8e194beda7d33ce37b4206e2cb83809f1e77cb744758a1f67246b8c42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "646bb0fe-60fb-4a56-9af1-1914f61869ef", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C .  | C . ", "original_text": "_ E . "}, "hash": "ae57a771eaefe4cc2eab3ce898899d34b933fe5e63a57f3962123f852c514854", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C . ", "start_char_idx": 12206, "end_char_idx": 12401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "646bb0fe-60fb-4a56-9af1-1914f61869ef": {"__data__": {"id_": "646bb0fe-60fb-4a56-9af1-1914f61869ef", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C .  | C . ", "original_text": "_ E . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b806241-bebb-4935-8d40-483024eb9fe7", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C . ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C . "}, "hash": "cf98198232ba8a95c46916bbf874e414b6ec5e1a1969ed7be769c0c6f8dde26e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7d6e4d0-65d0-4f09-9e5c-a67ac14950d8", "node_type": "1", "metadata": {"window": "In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C .  | C .  D E . ", "original_text": "| D E . "}, "hash": "82a42acdb79594ca21ad11de248be9909f11ca51060c7572fe52c837af42ed36", "class_name": "RelatedNodeInfo"}}, "text": "_ E . ", "start_char_idx": 12401, "end_char_idx": 12407, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7d6e4d0-65d0-4f09-9e5c-a67ac14950d8": {"__data__": {"id_": "c7d6e4d0-65d0-4f09-9e5c-a67ac14950d8", "embedding": null, "metadata": {"window": "In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C .  | C .  D E . ", "original_text": "| D E . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "646bb0fe-60fb-4a56-9af1-1914f61869ef", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\nToken Deletion Random tokens are deleted from the input.\n In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C .  | C . ", "original_text": "_ E . "}, "hash": "ae57a771eaefe4cc2eab3ce898899d34b933fe5e63a57f3962123f852c514854", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b72c7c39-8dc5-42ca-aae6-829ceb4de9ff", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C . ", "original_text": "A B C . "}, "hash": "e45c0bdf19643bb80b326ec7ea36db11b9633673f925f3f52e9c85c9660d9530", "class_name": "RelatedNodeInfo"}}, "text": "| D E . ", "start_char_idx": 12407, "end_char_idx": 12415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b72c7c39-8dc5-42ca-aae6-829ceb4de9ff": {"__data__": {"id_": "b72c7c39-8dc5-42ca-aae6-829ceb4de9ff", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C . ", "original_text": "A B C . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7d6e4d0-65d0-4f09-9e5c-a67ac14950d8", "node_type": "1", "metadata": {"window": "In contrast to token masking, the model must decide which positions are missing inputs.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C .  | C .  D E . ", "original_text": "| D E . "}, "hash": "82a42acdb79594ca21ad11de248be9909f11ca51060c7572fe52c837af42ed36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cc0f7c4-ffaf-4a44-9f7f-16eabeb3e703", "node_type": "1", "metadata": {"window": "_ E .  | D E .  A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A . ", "original_text": "| C . "}, "hash": "f886b7d3aaa91159f377d72fbdcb29023262e3a2ebd193c5e358b960fac1d471", "class_name": "RelatedNodeInfo"}}, "text": "A B C . ", "start_char_idx": 12415, "end_char_idx": 12423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cc0f7c4-ffaf-4a44-9f7f-16eabeb3e703": {"__data__": {"id_": "3cc0f7c4-ffaf-4a44-9f7f-16eabeb3e703", "embedding": null, "metadata": {"window": "_ E .  | D E .  A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A . ", "original_text": "| C . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b72c7c39-8dc5-42ca-aae6-829ceb4de9ff", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > 2.2 Pre-training BART\n | A _C .  _ E .  | D E .  A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C . ", "original_text": "A B C . "}, "hash": "e45c0bdf19643bb80b326ec7ea36db11b9633673f925f3f52e9c85c9660d9530", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2f61c67-c169-4b85-8a2f-48bf1045ac2b", "node_type": "1", "metadata": {"window": "| D E .  A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C . ", "original_text": "D E . "}, "hash": "cfe92b1f6158631f00ab0efb0cbb7dc2e7e023c2709b56280b6463d3a9c56911", "class_name": "RelatedNodeInfo"}}, "text": "| C . ", "start_char_idx": 12423, "end_char_idx": 12429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2f61c67-c169-4b85-8a2f-48bf1045ac2b": {"__data__": {"id_": "c2f61c67-c169-4b85-8a2f-48bf1045ac2b", "embedding": null, "metadata": {"window": "| D E .  A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C . ", "original_text": "D E . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cc0f7c4-ffaf-4a44-9f7f-16eabeb3e703", "node_type": "1", "metadata": {"window": "_ E .  | D E .  A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A . ", "original_text": "| C . "}, "hash": "f886b7d3aaa91159f377d72fbdcb29023262e3a2ebd193c5e358b960fac1d471", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4ad0b61-16f7-465a-8d09-88311b46b674", "node_type": "1", "metadata": {"window": "A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E . ", "original_text": "A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C . "}, "hash": "b36621ba4b120aad003c49d17006df2f7717f3ee75772ffac134489dd257a313", "class_name": "RelatedNodeInfo"}}, "text": "D E . ", "start_char_idx": 12409, "end_char_idx": 12415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4ad0b61-16f7-465a-8d09-88311b46b674": {"__data__": {"id_": "f4ad0b61-16f7-465a-8d09-88311b46b674", "embedding": null, "metadata": {"window": "A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E . ", "original_text": "A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2f61c67-c169-4b85-8a2f-48bf1045ac2b", "node_type": "1", "metadata": {"window": "| D E .  A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C . ", "original_text": "D E . "}, "hash": "cfe92b1f6158631f00ab0efb0cbb7dc2e7e023c2709b56280b6463d3a9c56911", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae82fa1d-2588-4b6b-b73a-56a15eba0c31", "node_type": "1", "metadata": {"window": "| C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E .  | A _ . ", "original_text": "D E .A . "}, "hash": "ebd2cddce60111c96ac1d7d704f4654f80179406b9f4bf67f0c150b400b28973", "class_name": "RelatedNodeInfo"}}, "text": "A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C . ", "start_char_idx": 12435, "end_char_idx": 12532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae82fa1d-2588-4b6b-b73a-56a15eba0c31": {"__data__": {"id_": "ae82fa1d-2588-4b6b-b73a-56a15eba0c31", "embedding": null, "metadata": {"window": "| C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E .  | A _ . ", "original_text": "D E .A . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4ad0b61-16f7-465a-8d09-88311b46b674", "node_type": "1", "metadata": {"window": "A B C .  | C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E . ", "original_text": "A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C . "}, "hash": "b36621ba4b120aad003c49d17006df2f7717f3ee75772ffac134489dd257a313", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f16492d-cbf5-4165-b865-8edc18ff3a8e", "node_type": "1", "metadata": {"window": "D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E .  | A _ .  D _ E .\n\n\n", "original_text": "C . "}, "hash": "eaa75858c61e171f8a5b62179cd38029e4fc9168c3a22ee48db61b134aeaf3e9", "class_name": "RelatedNodeInfo"}}, "text": "D E .A . ", "start_char_idx": 12532, "end_char_idx": 12541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f16492d-cbf5-4165-b865-8edc18ff3a8e": {"__data__": {"id_": "1f16492d-cbf5-4165-b865-8edc18ff3a8e", "embedding": null, "metadata": {"window": "D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E .  | A _ .  D _ E .\n\n\n", "original_text": "C . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae82fa1d-2588-4b6b-b73a-56a15eba0c31", "node_type": "1", "metadata": {"window": "| C .  D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E .  | A _ . ", "original_text": "D E .A . "}, "hash": "ebd2cddce60111c96ac1d7d704f4654f80179406b9f4bf67f0c150b400b28973", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0156189-1e63-4a4f-862f-39eacee0613f", "node_type": "1", "metadata": {"window": "A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n", "original_text": "E . "}, "hash": "ea5d44e24b24dc4a323a09aaf85fd53265135d6e7ad44947e2de943d395bebd4", "class_name": "RelatedNodeInfo"}}, "text": "C . ", "start_char_idx": 12397, "end_char_idx": 12401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0156189-1e63-4a4f-862f-39eacee0613f": {"__data__": {"id_": "e0156189-1e63-4a4f-862f-39eacee0613f", "embedding": null, "metadata": {"window": "A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n", "original_text": "E . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f16492d-cbf5-4165-b865-8edc18ff3a8e", "node_type": "1", "metadata": {"window": "D E .  A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E .  | A _ .  D _ E .\n\n\n", "original_text": "C . "}, "hash": "eaa75858c61e171f8a5b62179cd38029e4fc9168c3a22ee48db61b134aeaf3e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1462ffbd-ae4d-48b0-8c24-88cf5574a082", "node_type": "1", "metadata": {"window": "D E .A .  C .  E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n", "original_text": "| A _ . "}, "hash": "aaf8579707140e6a50f105542a31cb5da05fbb0bafc1b335bd757771eee81c14", "class_name": "RelatedNodeInfo"}}, "text": "E . ", "start_char_idx": 12403, "end_char_idx": 12407, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1462ffbd-ae4d-48b0-8c24-88cf5574a082": {"__data__": {"id_": "1462ffbd-ae4d-48b0-8c24-88cf5574a082", "embedding": null, "metadata": {"window": "D E .A .  C .  E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n", "original_text": "| A _ . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0156189-1e63-4a4f-862f-39eacee0613f", "node_type": "1", "metadata": {"window": "A B\n | --- | --- | ---\n |  | Sentence Permutation | Document RotationToken Masking\n |  | A B C .  D E .A .  C .  E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n", "original_text": "E . "}, "hash": "ea5d44e24b24dc4a323a09aaf85fd53265135d6e7ad44947e2de943d395bebd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24977c1d-216f-44a3-8be5-df0545bf8434", "node_type": "1", "metadata": {"window": "C .  E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n", "original_text": "D _ E .\n\n\n"}, "hash": "5eeb0a1e40b17d2cc4ed6766785cde0530f2d78e6b4cba1b8aa407e47045a57e", "class_name": "RelatedNodeInfo"}}, "text": "| A _ . ", "start_char_idx": 12549, "end_char_idx": 12557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24977c1d-216f-44a3-8be5-df0545bf8434": {"__data__": {"id_": "24977c1d-216f-44a3-8be5-df0545bf8434", "embedding": null, "metadata": {"window": "C .  E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n", "original_text": "D _ E .\n\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1462ffbd-ae4d-48b0-8c24-88cf5574a082", "node_type": "1", "metadata": {"window": "D E .A .  C .  E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n", "original_text": "| A _ . "}, "hash": "aaf8579707140e6a50f105542a31cb5da05fbb0bafc1b335bd757771eee81c14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc8e5678-fcc2-4b89-8cc9-4402dd31031c", "node_type": "1", "metadata": {"window": "E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n"}, "hash": "2211bc727b80b58de92bc235afbb96d67936e9781a647d7182a67ebc813c89cb", "class_name": "RelatedNodeInfo"}}, "text": "D _ E .\n\n\n", "start_char_idx": 12557, "end_char_idx": 12567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc8e5678-fcc2-4b89-8cc9-4402dd31031c": {"__data__": {"id_": "fc8e5678-fcc2-4b89-8cc9-4402dd31031c", "embedding": null, "metadata": {"window": "E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24977c1d-216f-44a3-8be5-df0545bf8434", "node_type": "1", "metadata": {"window": "C .  E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n", "original_text": "D _ E .\n\n\n"}, "hash": "5eeb0a1e40b17d2cc4ed6766785cde0530f2d78e6b4cba1b8aa407e47045a57e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14998b1c-e5b4-41c8-857d-6593b7052de8", "node_type": "1", "metadata": {"window": "| A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n", "original_text": "These transformations can be composed.\n\n"}, "hash": "8a4838c0e0189b2a49a43628039df3d4b89750c628603182873c34503f4321d5", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n", "start_char_idx": 12567, "end_char_idx": 12832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14998b1c-e5b4-41c8-857d-6593b7052de8": {"__data__": {"id_": "14998b1c-e5b4-41c8-857d-6593b7052de8", "embedding": null, "metadata": {"window": "| A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n", "original_text": "These transformations can be composed.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc8e5678-fcc2-4b89-8cc9-4402dd31031c", "node_type": "1", "metadata": {"window": "E .  | A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n"}, "hash": "2211bc727b80b58de92bc235afbb96d67936e9781a647d7182a67ebc813c89cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dee275f5-7df6-4765-8362-3783e4192b12", "node_type": "1", "metadata": {"window": "D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n"}, "hash": "e00960ff6ac3d552b5a0ab913ac9d57e6929f15f0ce9e3bdbf5709a9d9526228", "class_name": "RelatedNodeInfo"}}, "text": "These transformations can be composed.\n\n", "start_char_idx": 12832, "end_char_idx": 12872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dee275f5-7df6-4765-8362-3783e4192b12": {"__data__": {"id_": "dee275f5-7df6-4765-8362-3783e4192b12", "embedding": null, "metadata": {"window": "D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14998b1c-e5b4-41c8-857d-6593b7052de8", "node_type": "1", "metadata": {"window": "| A _ .  D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n", "original_text": "These transformations can be composed.\n\n"}, "hash": "8a4838c0e0189b2a49a43628039df3d4b89750c628603182873c34503f4321d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43d84b72-3f06-412e-a3cc-9058f6f7be8e", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n", "original_text": "Each span is replaced with a single [MASK] token.\n"}, "hash": "87578a995d8f1783d4869b88221fa70bcbc3d9f6b7f705ef99f70226588757ea", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n", "start_char_idx": 12872, "end_char_idx": 13175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43d84b72-3f06-412e-a3cc-9058f6f7be8e": {"__data__": {"id_": "43d84b72-3f06-412e-a3cc-9058f6f7be8e", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n", "original_text": "Each span is replaced with a single [MASK] token.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dee275f5-7df6-4765-8362-3783e4192b12", "node_type": "1", "metadata": {"window": "D _ E .\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n"}, "hash": "e00960ff6ac3d552b5a0ab913ac9d57e6929f15f0ce9e3bdbf5709a9d9526228", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b75c5011-6320-4288-8ea2-784d530380a0", "node_type": "1", "metadata": {"window": "These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n", "original_text": "0-length spans correspond to the insertion of [MASK] tokens.\n"}, "hash": "375ea0151f9f1e75dba8c8d64a83ab555623ee763162a22df4691ff57f88b488", "class_name": "RelatedNodeInfo"}}, "text": "Each span is replaced with a single [MASK] token.\n", "start_char_idx": 13175, "end_char_idx": 13225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b75c5011-6320-4288-8ea2-784d530380a0": {"__data__": {"id_": "b75c5011-6320-4288-8ea2-784d530380a0", "embedding": null, "metadata": {"window": "These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n", "original_text": "0-length spans correspond to the insertion of [MASK] tokens.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43d84b72-3f06-412e-a3cc-9058f6f7be8e", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nFigure 2: Transformations for noising the input that we experiment with.\n These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n", "original_text": "Each span is replaced with a single [MASK] token.\n"}, "hash": "87578a995d8f1783d4869b88221fa70bcbc3d9f6b7f705ef99f70226588757ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd12e284-8f3f-4408-9836-e9a1303df12a", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n", "original_text": "Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n"}, "hash": "56b5ec4bd1bb8e5a58d18e34bc228046ca173546f9f8b206543fd181b24dbce3", "class_name": "RelatedNodeInfo"}}, "text": "0-length spans correspond to the insertion of [MASK] tokens.\n", "start_char_idx": 13225, "end_char_idx": 13286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd12e284-8f3f-4408-9836-e9a1303df12a": {"__data__": {"id_": "dd12e284-8f3f-4408-9836-e9a1303df12a", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n", "original_text": "Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b75c5011-6320-4288-8ea2-784d530380a0", "node_type": "1", "metadata": {"window": "These transformations can be composed.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n", "original_text": "0-length spans correspond to the insertion of [MASK] tokens.\n"}, "hash": "375ea0151f9f1e75dba8c8d64a83ab555623ee763162a22df4691ff57f88b488", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a739e16-d63b-4cca-bda8-b313c74c28d1", "node_type": "1", "metadata": {"window": "Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n", "original_text": "Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n"}, "hash": "1c5b69e50de3a87e8e76456b558811eab0d5788b323b1b4fbd2d64d166c3ab7f", "class_name": "RelatedNodeInfo"}}, "text": "Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n", "start_char_idx": 13286, "end_char_idx": 13515, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a739e16-d63b-4cca-bda8-b313c74c28d1": {"__data__": {"id_": "4a739e16-d63b-4cca-bda8-b313c74c28d1", "embedding": null, "metadata": {"window": "Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n", "original_text": "Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd12e284-8f3f-4408-9836-e9a1303df12a", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nText In\ufb01lling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3).\n Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n", "original_text": "Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n"}, "hash": "56b5ec4bd1bb8e5a58d18e34bc228046ca173546f9f8b206543fd181b24dbce3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "953ac142-30df-44c8-b09e-df0b245a59f3", "node_type": "1", "metadata": {"window": "0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n"}, "hash": "ec56ab70cae3202e7b2a78ad313f710974c9dc9e9c7012baa577e5cab933295f", "class_name": "RelatedNodeInfo"}}, "text": "Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n", "start_char_idx": 13515, "end_char_idx": 13600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "953ac142-30df-44c8-b09e-df0b245a59f3": {"__data__": {"id_": "953ac142-30df-44c8-b09e-df0b245a59f3", "embedding": null, "metadata": {"window": "0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a739e16-d63b-4cca-bda8-b313c74c28d1", "node_type": "1", "metadata": {"window": "Each span is replaced with a single [MASK] token.\n 0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n", "original_text": "Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n"}, "hash": "1c5b69e50de3a87e8e76456b558811eab0d5788b323b1b4fbd2d64d166c3ab7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "803d2b1e-80ed-4ffd-9e82-34df0eb70e6f", "node_type": "1", "metadata": {"window": "Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n"}, "hash": "09ec164c3067b42348f9a460ec3fa80effe0320e481ae8cb81ce510f3dfb3211", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n", "start_char_idx": 13600, "end_char_idx": 13923, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "803d2b1e-80ed-4ffd-9e82-34df0eb70e6f": {"__data__": {"id_": "803d2b1e-80ed-4ffd-9e82-34df0eb70e6f", "embedding": null, "metadata": {"window": "Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "953ac142-30df-44c8-b09e-df0b245a59f3", "node_type": "1", "metadata": {"window": "0-length spans correspond to the insertion of [MASK] tokens.\n Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n"}, "hash": "ec56ab70cae3202e7b2a78ad313f710974c9dc9e9c7012baa577e5cab933295f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "127063f6-0ba9-4bbe-bcab-ceb1b726e972", "node_type": "1", "metadata": {"window": "Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n", "original_text": "This task trains the model to identify the start of the document.\n\n"}, "hash": "4e2521ad905c053f9692fba6ec6056fb141b2e9b647e2b5da7e5d69953936ba8", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n", "start_char_idx": 13923, "end_char_idx": 14235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "127063f6-0ba9-4bbe-bcab-ceb1b726e972": {"__data__": {"id_": "127063f6-0ba9-4bbe-bcab-ceb1b726e972", "embedding": null, "metadata": {"window": "Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n", "original_text": "This task trains the model to identify the start of the document.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "803d2b1e-80ed-4ffd-9e82-34df0eb70e6f", "node_type": "1", "metadata": {"window": "Text in\ufb01lling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n"}, "hash": "09ec164c3067b42348f9a460ec3fa80effe0320e481ae8cb81ce510f3dfb3211", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "138113a3-3d83-4093-a42f-e432b51ef0b1", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n"}, "hash": "53f0be8549966ef4b043d1d83b081370591eb381d205fe311cf3c05fcaa9ad28", "class_name": "RelatedNodeInfo"}}, "text": "This task trains the model to identify the start of the document.\n\n", "start_char_idx": 14235, "end_char_idx": 14302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "138113a3-3d83-4093-a42f-e432b51ef0b1": {"__data__": {"id_": "138113a3-3d83-4093-a42f-e432b51ef0b1", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "127063f6-0ba9-4bbe-bcab-ceb1b726e972", "node_type": "1", "metadata": {"window": "Text in\ufb01lling teaches the model to predict how many tokens are missing from a span.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n", "original_text": "This task trains the model to identify the start of the document.\n\n"}, "hash": "4e2521ad905c053f9692fba6ec6056fb141b2e9b647e2b5da7e5d69953936ba8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0a0b59a-76c9-4c90-bb02-4b728c1d0e88", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n"}, "hash": "e2416094db40208cb97c3263aa3b65d955f90c3a11e2fc790c51dae371cc210b", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n", "start_char_idx": 14302, "end_char_idx": 14569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0a0b59a-76c9-4c90-bb02-4b728c1d0e88": {"__data__": {"id_": "b0a0b59a-76c9-4c90-bb02-4b728c1d0e88", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "138113a3-3d83-4093-a42f-e432b51ef0b1", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nSentence Permutation A document is divided into sentences based on full stops, and these sentences are shuf\ufb02ed in a random order.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n"}, "hash": "53f0be8549966ef4b043d1d83b081370591eb381d205fe311cf3c05fcaa9ad28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82dff16a-6bd9-4564-bbe4-87a576b113ac", "node_type": "1", "metadata": {"window": "This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n", "original_text": "This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n"}, "hash": "5f0497b3c6b47653cd49a2154aa4d5a9634a0ab79e1c52a69f4adb1048b62d47", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n", "start_char_idx": 14569, "end_char_idx": 14960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82dff16a-6bd9-4564-bbe4-87a576b113ac": {"__data__": {"id_": "82dff16a-6bd9-4564-bbe4-87a576b113ac", "embedding": null, "metadata": {"window": "This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n", "original_text": "This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0a0b59a-76c9-4c90-bb02-4b728c1d0e88", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 2 Model > Token Deletion Text In\ufb01lling\nDocument Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n"}, "hash": "e2416094db40208cb97c3263aa3b65d955f90c3a11e2fc790c51dae371cc210b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bc86263-faba-4174-8d17-1da1890632f2", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n"}, "hash": "3f24e0ea053a68fdc468bf3a71c6164d12068180bfdbf1ec1d6a3518a16b0f57", "class_name": "RelatedNodeInfo"}}, "text": "This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n", "start_char_idx": 14960, "end_char_idx": 15177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bc86263-faba-4174-8d17-1da1890632f2": {"__data__": {"id_": "6bc86263-faba-4174-8d17-1da1890632f2", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82dff16a-6bd9-4564-bbe4-87a576b113ac", "node_type": "1", "metadata": {"window": "This task trains the model to identify the start of the document.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n", "original_text": "This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n"}, "hash": "5f0497b3c6b47653cd49a2154aa4d5a9634a0ab79e1c52a69f4adb1048b62d47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6002afb9-52ac-4a0e-9750-80a7db473c11", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n", "original_text": "This representation is used to classify the token.\n\n"}, "hash": "c59f0916e8bb70a96389555ef494c51abe98b1c5c47cbeacaefff2b5771ae662", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n", "start_char_idx": 15177, "end_char_idx": 15600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6002afb9-52ac-4a0e-9750-80a7db473c11": {"__data__": {"id_": "6002afb9-52ac-4a0e-9750-80a7db473c11", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n", "original_text": "This representation is used to classify the token.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bc86263-faba-4174-8d17-1da1890632f2", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART\nThe representations produced by BART can be used in several ways for downstream applications.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n"}, "hash": "3f24e0ea053a68fdc468bf3a71c6164d12068180bfdbf1ec1d6a3518a16b0f57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e1adf3f-6e87-4ffd-9658-ad23b60aa15f", "node_type": "1", "metadata": {"window": "This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n"}, "hash": "8334eb4688303ef99e89cdde6ed2628ded36974136c2e887fd9aee261f4902fd", "class_name": "RelatedNodeInfo"}}, "text": "This representation is used to classify the token.\n\n", "start_char_idx": 15600, "end_char_idx": 15652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e1adf3f-6e87-4ffd-9658-ad23b60aa15f": {"__data__": {"id_": "9e1adf3f-6e87-4ffd-9658-ad23b60aa15f", "embedding": null, "metadata": {"window": "This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6002afb9-52ac-4a0e-9750-80a7db473c11", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.1 Sequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed into the encoder and decoder, and the \ufb01nal hidden state of the \ufb01nal decoder token is fed into new multi-class linear classi\ufb01er.\n This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n", "original_text": "This representation is used to classify the token.\n\n"}, "hash": "c59f0916e8bb70a96389555ef494c51abe98b1c5c47cbeacaefff2b5771ae662", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd7210ec-f709-40c8-b0df-83a6cabba803", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n", "original_text": "In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n"}, "hash": "f5da8211f8975f3a73a7d27cce4afc6fb81d1207d9404b36c042caee79c9eb1f", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n", "start_char_idx": 15652, "end_char_idx": 16017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd7210ec-f709-40c8-b0df-83a6cabba803": {"__data__": {"id_": "cd7210ec-f709-40c8-b0df-83a6cabba803", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n", "original_text": "In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e1adf3f-6e87-4ffd-9658-ad23b60aa15f", "node_type": "1", "metadata": {"window": "This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n"}, "hash": "8334eb4688303ef99e89cdde6ed2628ded36974136c2e887fd9aee261f4902fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "819c1497-cf2a-4690-8f16-f3dbc76048df", "node_type": "1", "metadata": {"window": "This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n", "original_text": "Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n"}, "hash": "be5a2f708abc28ca38e922b603268217309abe61e8608d590b65826e247f0cac", "class_name": "RelatedNodeInfo"}}, "text": "In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n", "start_char_idx": 16017, "end_char_idx": 16161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "819c1497-cf2a-4690-8f16-f3dbc76048df": {"__data__": {"id_": "819c1497-cf2a-4690-8f16-f3dbc76048df", "embedding": null, "metadata": {"window": "This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n", "original_text": "Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd7210ec-f709-40c8-b0df-83a6cabba803", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.2 Token Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint classi\ufb01cation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n", "original_text": "In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n"}, "hash": "f5da8211f8975f3a73a7d27cce4afc6fb81d1207d9404b36c042caee79c9eb1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91c48cf5-9875-43a2-a5b4-079ab11b97f2", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n"}, "hash": "5fdc9a8dbd45c69e8afdee38f45d3547b469675f488176f4295a94fa68b77f22", "class_name": "RelatedNodeInfo"}}, "text": "Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n", "start_char_idx": 16161, "end_char_idx": 16261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91c48cf5-9875-43a2-a5b4-079ab11b97f2": {"__data__": {"id_": "91c48cf5-9875-43a2-a5b4-079ab11b97f2", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "819c1497-cf2a-4690-8f16-f3dbc76048df", "node_type": "1", "metadata": {"window": "This representation is used to classify the token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n", "original_text": "Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n"}, "hash": "be5a2f708abc28ca38e922b603268217309abe61e8608d590b65826e247f0cac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e661afe9-8e7d-45d4-af86-55591675fa82", "node_type": "1", "metadata": {"window": "In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n", "original_text": "Previous work Edunov et al.\n"}, "hash": "1a4a945893d61b15b1f9f64e9d506e315e95d2306d252d1b2d6af0b8e8ec663b", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n", "start_char_idx": 16261, "end_char_idx": 16556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e661afe9-8e7d-45d4-af86-55591675fa82": {"__data__": {"id_": "e661afe9-8e7d-45d4-af86-55591675fa82", "embedding": null, "metadata": {"window": "In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n", "original_text": "Previous work Edunov et al.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91c48cf5-9875-43a2-a5b4-079ab11b97f2", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.3 Sequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be directly \ufb01ne tuned for sequence generation tasks such as abstractive question answering and summarization.\n In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n"}, "hash": "5fdc9a8dbd45c69e8afdee38f45d3547b469675f488176f4295a94fa68b77f22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0baca0f-baf3-47db-82d9-5fe2f8f13496", "node_type": "1", "metadata": {"window": "Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n", "original_text": "(2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n"}, "hash": "f9e802ab8f46adbfab7a40c48304b38bdd9fc2814b14ceecca7b1aabde63e55d", "class_name": "RelatedNodeInfo"}}, "text": "Previous work Edunov et al.\n", "start_char_idx": 16556, "end_char_idx": 16584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0baca0f-baf3-47db-82d9-5fe2f8f13496": {"__data__": {"id_": "b0baca0f-baf3-47db-82d9-5fe2f8f13496", "embedding": null, "metadata": {"window": "Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n", "original_text": "(2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e661afe9-8e7d-45d4-af86-55591675fa82", "node_type": "1", "metadata": {"window": "In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n", "original_text": "Previous work Edunov et al.\n"}, "hash": "1a4a945893d61b15b1f9f64e9d506e315e95d2306d252d1b2d6af0b8e8ec663b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f5d2cbc-03df-475a-a591-c3710e4f9767", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n", "original_text": "We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n"}, "hash": "46c74bd75f5dee5c312e0c60c43f6f29338c6f956f128f715bec3b97285d3347", "class_name": "RelatedNodeInfo"}}, "text": "(2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n", "start_char_idx": 16584, "end_char_idx": 16748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f5d2cbc-03df-475a-a591-c3710e4f9767": {"__data__": {"id_": "5f5d2cbc-03df-475a-a591-c3710e4f9767", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n", "original_text": "We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0baca0f-baf3-47db-82d9-5fe2f8f13496", "node_type": "1", "metadata": {"window": "Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n", "original_text": "(2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n"}, "hash": "f9e802ab8f46adbfab7a40c48304b38bdd9fc2814b14ceecca7b1aabde63e55d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81522c83-276b-4b1e-b8f6-12a56ce1ba0a", "node_type": "1", "metadata": {"window": "Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n"}, "hash": "f807166a6b7ac1c62bb7b5623e76cdfc838b7b5a2c8da9904a9b7bc4225fae8c", "class_name": "RelatedNodeInfo"}}, "text": "We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n", "start_char_idx": 16748, "end_char_idx": 16977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81522c83-276b-4b1e-b8f6-12a56ce1ba0a": {"__data__": {"id_": "81522c83-276b-4b1e-b8f6-12a56ce1ba0a", "embedding": null, "metadata": {"window": "Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f5d2cbc-03df-475a-a591-c3710e4f9767", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe also explore using BART to improve machine translation decoders for translating into English.\n Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n", "original_text": "We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n"}, "hash": "46c74bd75f5dee5c312e0c60c43f6f29338c6f956f128f715bec3b97285d3347", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfcd7fb4-1f72-48cf-9375-1203dfd55d58", "node_type": "1", "metadata": {"window": "(2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n", "original_text": "The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n"}, "hash": "3d998bfd640c4c955a1e5e960174eaca5fcbcbabea24b4e450666aff3f2c662c", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n", "start_char_idx": 16977, "end_char_idx": 17274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfcd7fb4-1f72-48cf-9375-1203dfd55d58": {"__data__": {"id_": "dfcd7fb4-1f72-48cf-9375-1203dfd55d58", "embedding": null, "metadata": {"window": "(2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n", "original_text": "The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81522c83-276b-4b1e-b8f6-12a56ce1ba0a", "node_type": "1", "metadata": {"window": "Previous work Edunov et al.\n (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n"}, "hash": "f807166a6b7ac1c62bb7b5623e76cdfc838b7b5a2c8da9904a9b7bc4225fae8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99fcad15-274a-42e0-b907-3c1340ec734b", "node_type": "1", "metadata": {"window": "We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n", "original_text": "The new encoder can use a separate vocabulary from the original BART model.\n\n"}, "hash": "8e513e33408182b8ec5028e390bfbfd5b7ff9bede54b6a08af06815bb9ade480", "class_name": "RelatedNodeInfo"}}, "text": "The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n", "start_char_idx": 17274, "end_char_idx": 17406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99fcad15-274a-42e0-b907-3c1340ec734b": {"__data__": {"id_": "99fcad15-274a-42e0-b907-3c1340ec734b", "embedding": null, "metadata": {"window": "We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n", "original_text": "The new encoder can use a separate vocabulary from the original BART model.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfcd7fb4-1f72-48cf-9375-1203dfd55d58", "node_type": "1", "metadata": {"window": "(2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n", "original_text": "The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n"}, "hash": "3d998bfd640c4c955a1e5e960174eaca5fcbcbabea24b4e450666aff3f2c662c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d345fca-28dc-4568-9795-fc5cf0bde6a1", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n"}, "hash": "8034ca0d31823837f5230607a5edbc26044193a1247827d15d98cefefc733ac0", "class_name": "RelatedNodeInfo"}}, "text": "The new encoder can use a separate vocabulary from the original BART model.\n\n", "start_char_idx": 17406, "end_char_idx": 17483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d345fca-28dc-4568-9795-fc5cf0bde6a1": {"__data__": {"id_": "7d345fca-28dc-4568-9795-fc5cf0bde6a1", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99fcad15-274a-42e0-b907-3c1340ec734b", "node_type": "1", "metadata": {"window": "We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n", "original_text": "The new encoder can use a separate vocabulary from the original BART model.\n\n"}, "hash": "8e513e33408182b8ec5028e390bfbfd5b7ff9bede54b6a08af06815bb9ade480", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72e54b2a-ee06-46b8-974b-9dc82506bb1a", "node_type": "1", "metadata": {"window": "The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n", "original_text": "In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n"}, "hash": "928d51cfc4c27ac49465611128179d329b0fd5c2d9b7604a03b8614c8932ca69", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n", "start_char_idx": 17483, "end_char_idx": 17811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72e54b2a-ee06-46b8-974b-9dc82506bb1a": {"__data__": {"id_": "72e54b2a-ee06-46b8-974b-9dc82506bb1a", "embedding": null, "metadata": {"window": "The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n", "original_text": "In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d345fca-28dc-4568-9795-fc5cf0bde6a1", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nMore precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder.\n The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n"}, "hash": "8034ca0d31823837f5230607a5edbc26044193a1247827d15d98cefefc733ac0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11e4b990-d314-4029-9516-735137186c65", "node_type": "1", "metadata": {"window": "The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n", "original_text": "In the second step, we train all model parameters for a small number of iterations.\n\n"}, "hash": "2bee0b0f3591fb7f4b91841ebf61087d37e3e585d26242bf3d4c9b8f827e90c9", "class_name": "RelatedNodeInfo"}}, "text": "In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n", "start_char_idx": 17811, "end_char_idx": 18029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11e4b990-d314-4029-9516-735137186c65": {"__data__": {"id_": "11e4b990-d314-4029-9516-735137186c65", "embedding": null, "metadata": {"window": "The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n", "original_text": "In the second step, we train all model parameters for a small number of iterations.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72e54b2a-ee06-46b8-974b-9dc82506bb1a", "node_type": "1", "metadata": {"window": "The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n", "original_text": "In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n"}, "hash": "928d51cfc4c27ac49465611128179d329b0fd5c2d9b7604a03b8614c8932ca69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cf55fd4-8253-4390-9abf-9a6ec6d9d0bf", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n"}, "hash": "aab33eb7a28d20e9dcdbf0c9ca98d185c40bb33aff702aed98b0431adb1f23c5", "class_name": "RelatedNodeInfo"}}, "text": "In the second step, we train all model parameters for a small number of iterations.\n\n", "start_char_idx": 18029, "end_char_idx": 18114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cf55fd4-8253-4390-9abf-9a6ec6d9d0bf": {"__data__": {"id_": "3cf55fd4-8253-4390-9abf-9a6ec6d9d0bf", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11e4b990-d314-4029-9516-735137186c65", "node_type": "1", "metadata": {"window": "The new encoder can use a separate vocabulary from the original BART model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n", "original_text": "In the second step, we train all model parameters for a small number of iterations.\n\n"}, "hash": "2bee0b0f3591fb7f4b91841ebf61087d37e3e585d26242bf3d4c9b8f827e90c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8e3aba4-9507-4439-a0da-0b3d5a13fa2b", "node_type": "1", "metadata": {"window": "In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n", "original_text": "We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n"}, "hash": "527fa58e7652147105bf60fdb832dd231cb68804a19f5922539b024060209e4e", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n", "start_char_idx": 18114, "end_char_idx": 18395, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8e3aba4-9507-4439-a0da-0b3d5a13fa2b": {"__data__": {"id_": "d8e3aba4-9507-4439-a0da-0b3d5a13fa2b", "embedding": null, "metadata": {"window": "In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n", "original_text": "We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cf55fd4-8253-4390-9abf-9a6ec6d9d0bf", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 3 Fine-tuning BART > 3.4 Machine Translation\nWe train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n"}, "hash": "aab33eb7a28d20e9dcdbf0c9ca98d185c40bb33aff702aed98b0431adb1f23c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa3256e0-1567-4535-9b9d-c2461811531a", "node_type": "1", "metadata": {"window": "In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n"}, "hash": "2c09c5a56b799de654b870d9aec0237c28a9ea64f6581f6c6312590605083d30", "class_name": "RelatedNodeInfo"}}, "text": "We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n", "start_char_idx": 18395, "end_char_idx": 18621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa3256e0-1567-4535-9b9d-c2461811531a": {"__data__": {"id_": "fa3256e0-1567-4535-9b9d-c2461811531a", "embedding": null, "metadata": {"window": "In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8e3aba4-9507-4439-a0da-0b3d5a13fa2b", "node_type": "1", "metadata": {"window": "In the \ufb01rst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder \ufb01rst layer.\n In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n", "original_text": "We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n"}, "hash": "527fa58e7652147105bf60fdb832dd231cb68804a19f5922539b024060209e4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fff33e16-b299-48ab-85c2-f3f5b9e38e8d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n", "original_text": "We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n"}, "hash": "15da2224480dd2e1130df55ffb287215937c2257062ad5f26aa997572ac1b246", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n", "start_char_idx": 18621, "end_char_idx": 19098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fff33e16-b299-48ab-85c2-f3f5b9e38e8d": {"__data__": {"id_": "fff33e16-b299-48ab-85c2-f3f5b9e38e8d", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n", "original_text": "We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa3256e0-1567-4535-9b9d-c2461811531a", "node_type": "1", "metadata": {"window": "In the second step, we train all model parameters for a small number of iterations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n"}, "hash": "2c09c5a56b799de654b870d9aec0237c28a9ea64f6581f6c6312590605083d30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06ddb055-781e-48a3-909d-a1aba2d94922", "node_type": "1", "metadata": {"window": "We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n", "original_text": "(b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n"}, "hash": "73002fede5425e951863b1e2d2464c796389c839e16a06ec80c757b769bc06c6", "class_name": "RelatedNodeInfo"}}, "text": "We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n", "start_char_idx": 19098, "end_char_idx": 20501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06ddb055-781e-48a3-909d-a1aba2d94922": {"__data__": {"id_": "06ddb055-781e-48a3-909d-a1aba2d94922", "embedding": null, "metadata": {"window": "We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n", "original_text": "(b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fff33e16-b299-48ab-85c2-f3f5b9e38e8d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives\nBART supports a much wider range of noising schemes during pre-training than previous work.\n We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n", "original_text": "We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n"}, "hash": "15da2224480dd2e1130df55ffb287215937c2257062ad5f26aa997572ac1b246", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89eeb99d-27a9-4e88-8e74-10bf15ac7db2", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n", "original_text": "The new encoder can use a disjoint vocabulary.\n\n"}, "hash": "40c32eb123d839ba4f00e1f1fc18d8a96a5b6e1876eb4a4b0f4de86a6615de47", "class_name": "RelatedNodeInfo"}}, "text": "(b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n", "start_char_idx": 20501, "end_char_idx": 20609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89eeb99d-27a9-4e88-8e74-10bf15ac7db2": {"__data__": {"id_": "89eeb99d-27a9-4e88-8e74-10bf15ac7db2", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n", "original_text": "The new encoder can use a disjoint vocabulary.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06ddb055-781e-48a3-909d-a1aba2d94922", "node_type": "1", "metadata": {"window": "We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n", "original_text": "(b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n"}, "hash": "73002fede5425e951863b1e2d2464c796389c839e16a06ec80c757b769bc06c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfebabe9-75b2-42ef-93aa-9ae89ed66882", "node_type": "1", "metadata": {"window": "We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n"}, "hash": "5da2e07f172a9413494e0c0ee1ee17abb5635a4acac4504dda028cc0ac302dab", "class_name": "RelatedNodeInfo"}}, "text": "The new encoder can use a disjoint vocabulary.\n\n", "start_char_idx": 20609, "end_char_idx": 20657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfebabe9-75b2-42ef-93aa-9ae89ed66882": {"__data__": {"id_": "dfebabe9-75b2-42ef-93aa-9ae89ed66882", "embedding": null, "metadata": {"window": "We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89eeb99d-27a9-4e88-8e74-10bf15ac7db2", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nWhile many pre-training objectives have been proposed, fair comparisons between these have been dif\ufb01cult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and \ufb01ne-tuning procedures.\n We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n", "original_text": "The new encoder can use a disjoint vocabulary.\n\n"}, "hash": "40c32eb123d839ba4f00e1f1fc18d8a96a5b6e1876eb4a4b0f4de86a6615de47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "951976aa-bb00-49b7-9f45-6dec9a94e1ea", "node_type": "1", "metadata": {"window": "(b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n"}, "hash": "1acc75102c37540eca154fe32c031ca0f2980d13b9a22c42d0df0af7001c7fd4", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n", "start_char_idx": 20657, "end_char_idx": 20935, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "951976aa-bb00-49b7-9f45-6dec9a94e1ea": {"__data__": {"id_": "951976aa-bb00-49b7-9f45-6dec9a94e1ea", "embedding": null, "metadata": {"window": "(b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfebabe9-75b2-42ef-93aa-9ae89ed66882", "node_type": "1", "metadata": {"window": "We\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.1 Comparison Objectives\nA\nB\nC\nD E\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | Pre-trained Encoder | Pre-trained Decoder\n | --- | ---\n | Pre-trained Encoder | Pre-trained Decoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > label\n | <s> A | B | C | D\n | --- | --- | --- | ---\n | Randomly Initialized Encoder\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n\u03b1\n\u03b2\n\u03b3\n\u03b4 \u03b5\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\n(a) To use BART for classi\ufb01cation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the \ufb01nal output is used.\n (b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n"}, "hash": "5da2e07f172a9413494e0c0ee1ee17abb5635a4acac4504dda028cc0ac302dab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e015ef8-5f98-4aeb-b029-47c8ab64faaf", "node_type": "1", "metadata": {"window": "The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n", "original_text": "We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n"}, "hash": "b89ab9d82fb00b40d81eb777f1a8bd863baef3db6912540ae4294a130e9fce9b", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n", "start_char_idx": 20935, "end_char_idx": 21253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e015ef8-5f98-4aeb-b029-47c8ab64faaf": {"__data__": {"id_": "9e015ef8-5f98-4aeb-b029-47c8ab64faaf", "embedding": null, "metadata": {"window": "The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n", "original_text": "We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "951976aa-bb00-49b7-9f45-6dec9a94e1ea", "node_type": "1", "metadata": {"window": "(b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n"}, "hash": "1acc75102c37540eca154fe32c031ca0f2980d13b9a22c42d0df0af7001c7fd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0fea3ad-36af-4392-abd5-4177de72c488", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n", "original_text": "However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n"}, "hash": "a80edad22917be8d91abb9b90cc198c7c12fd3cfac9cd8a84040691db915de6e", "class_name": "RelatedNodeInfo"}}, "text": "We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n", "start_char_idx": 21253, "end_char_idx": 21350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0fea3ad-36af-4392-abd5-4177de72c488": {"__data__": {"id_": "a0fea3ad-36af-4392-abd5-4177de72c488", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n", "original_text": "However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e015ef8-5f98-4aeb-b029-47c8ab64faaf", "node_type": "1", "metadata": {"window": "The new encoder can use a disjoint vocabulary.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n", "original_text": "We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n"}, "hash": "b89ab9d82fb00b40d81eb777f1a8bd863baef3db6912540ae4294a130e9fce9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fefea734-8549-428e-973c-9f462f99f80c", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n", "original_text": "For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n"}, "hash": "c8d67e22c7061e1f6ceb83dd9c8dd225d2a4dd842703948ee56fff7e6a3bf37a", "class_name": "RelatedNodeInfo"}}, "text": "However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n", "start_char_idx": 21350, "end_char_idx": 21516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fefea734-8549-428e-973c-9f462f99f80c": {"__data__": {"id_": "fefea734-8549-428e-973c-9f462f99f80c", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n", "original_text": "For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0fea3ad-36af-4392-abd5-4177de72c488", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n", "original_text": "However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n"}, "hash": "a80edad22917be8d91abb9b90cc198c7c12fd3cfac9cd8a84040691db915de6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d5c4044-ce5f-453b-b0fc-1ad027eb0d55", "node_type": "1", "metadata": {"window": "We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n", "original_text": "We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n"}, "hash": "75e552ac4473babebeb85ae7eee4cb14cff81ddab3d4324c03e4c1f8e36c0393", "class_name": "RelatedNodeInfo"}}, "text": "For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n", "start_char_idx": 21516, "end_char_idx": 21678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d5c4044-ce5f-453b-b0fc-1ad027eb0d55": {"__data__": {"id_": "6d5c4044-ce5f-453b-b0fc-1ad027eb0d55", "embedding": null, "metadata": {"window": "We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n", "original_text": "We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fefea734-8549-428e-973c-9f462f99f80c", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n", "original_text": "For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n"}, "hash": "c8d67e22c7061e1f6ceb83dd9c8dd225d2a4dd842703948ee56fff7e6a3bf37a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "294eef68-ffad-47bc-b64a-f2121d3e7212", "node_type": "1", "metadata": {"window": "However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n", "original_text": "This model is equivalent to the BART decoder, without cross-attention.\n\n"}, "hash": "f853fc9450309b7afae29ad6604a40a185e416b3a8f6928e06095c50495961c4", "class_name": "RelatedNodeInfo"}}, "text": "We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n", "start_char_idx": 21678, "end_char_idx": 22040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "294eef68-ffad-47bc-b64a-f2121d3e7212": {"__data__": {"id_": "294eef68-ffad-47bc-b64a-f2121d3e7212", "embedding": null, "metadata": {"window": "However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n", "original_text": "This model is equivalent to the BART decoder, without cross-attention.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d5c4044-ce5f-453b-b0fc-1ad027eb0d55", "node_type": "1", "metadata": {"window": "We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n", "original_text": "We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n"}, "hash": "75e552ac4473babebeb85ae7eee4cb14cff81ddab3d4324c03e4c1f8e36c0393", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "298f9310-40cb-4375-b81b-8b2c99cf0347", "node_type": "1", "metadata": {"window": "For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n"}, "hash": "1e506aebe727a8eb856ce517f37e1b5aa1efbdad145781cd2c3ba5417dade6ba", "class_name": "RelatedNodeInfo"}}, "text": "This model is equivalent to the BART decoder, without cross-attention.\n\n", "start_char_idx": 22040, "end_char_idx": 22112, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "298f9310-40cb-4375-b81b-8b2c99cf0347": {"__data__": {"id_": "298f9310-40cb-4375-b81b-8b2c99cf0347", "embedding": null, "metadata": {"window": "For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "294eef68-ffad-47bc-b64a-f2121d3e7212", "node_type": "1", "metadata": {"window": "However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n", "original_text": "This model is equivalent to the BART decoder, without cross-attention.\n\n"}, "hash": "f853fc9450309b7afae29ad6604a40a185e416b3a8f6928e06095c50495961c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c198fa0d-7522-4620-88bd-07bdfa643302", "node_type": "1", "metadata": {"window": "We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n", "original_text": "For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n"}, "hash": "3d984e420a07089b49d3329945912992047b050585431c00ee1b393a705fd41f", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n", "start_char_idx": 22112, "end_char_idx": 22470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c198fa0d-7522-4620-88bd-07bdfa643302": {"__data__": {"id_": "c198fa0d-7522-4620-88bd-07bdfa643302", "embedding": null, "metadata": {"window": "We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n", "original_text": "For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "298f9310-40cb-4375-b81b-8b2c99cf0347", "node_type": "1", "metadata": {"window": "For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n"}, "hash": "1e506aebe727a8eb856ce517f37e1b5aa1efbdad145781cd2c3ba5417dade6ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f43c1320-da0e-4144-889f-e31614c757ac", "node_type": "1", "metadata": {"window": "This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n"}, "hash": "7af619c66d2690e9581c30861c6e87292b2ac2c001d32f337520cd8394c2873b", "class_name": "RelatedNodeInfo"}}, "text": "For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n", "start_char_idx": 22470, "end_char_idx": 22602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f43c1320-da0e-4144-889f-e31614c757ac": {"__data__": {"id_": "f43c1320-da0e-4144-889f-e31614c757ac", "embedding": null, "metadata": {"window": "This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c198fa0d-7522-4620-88bd-07bdfa643302", "node_type": "1", "metadata": {"window": "We compare the following approaches:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n", "original_text": "For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n"}, "hash": "3d984e420a07089b49d3329945912992047b050585431c00ee1b393a705fd41f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aca4d951-e9aa-4877-b8ca-d110beb2e64d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n"}, "hash": "6777192805bfc5e90fd88f033c6486a5b8b487abb7b56bb0b5bcde589cc7111b", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n", "start_char_idx": 22602, "end_char_idx": 22990, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aca4d951-e9aa-4877-b8ca-d110beb2e64d": {"__data__": {"id_": "aca4d951-e9aa-4877-b8ca-d110beb2e64d", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f43c1320-da0e-4144-889f-e31614c757ac", "node_type": "1", "metadata": {"window": "This model is equivalent to the BART decoder, without cross-attention.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n"}, "hash": "7af619c66d2690e9581c30861c6e87292b2ac2c001d32f337520cd8394c2873b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e912e6a0-f5ee-480e-8f78-6d2abb5bc632", "node_type": "1", "metadata": {"window": "For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n", "original_text": "Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n"}, "hash": "5178edb18df39a030b2dc0ba034520d5589da7a47cbc9d659b5c772361b4dd27", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n", "start_char_idx": 22990, "end_char_idx": 23341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e912e6a0-f5ee-480e-8f78-6d2abb5bc632": {"__data__": {"id_": "e912e6a0-f5ee-480e-8f78-6d2abb5bc632", "embedding": null, "metadata": {"window": "For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n", "original_text": "Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aca4d951-e9aa-4877-b8ca-d110beb2e64d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nPermuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n"}, "hash": "6777192805bfc5e90fd88f033c6486a5b8b487abb7b56bb0b5bcde589cc7111b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58ae5b0a-fce6-4459-a97d-f4b642a7984a", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n"}, "hash": "1a5aa8f1da28bb3652e6273f655b8c05bfdd60c375bb4527469f18adef8f25de", "class_name": "RelatedNodeInfo"}}, "text": "Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n", "start_char_idx": 23341, "end_char_idx": 23556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58ae5b0a-fce6-4459-a97d-f4b642a7984a": {"__data__": {"id_": "58ae5b0a-fce6-4459-a97d-f4b642a7984a", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e912e6a0-f5ee-480e-8f78-6d2abb5bc632", "node_type": "1", "metadata": {"window": "For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n", "original_text": "Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n"}, "hash": "5178edb18df39a030b2dc0ba034520d5589da7a47cbc9d659b5c772361b4dd27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8649d834-25c2-40ad-9ed1-013a18467f87", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n"}, "hash": "501eb2ec24287b432bca82ceb3f166420900b92f1b336cd38a5bbe061dc7767b", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n", "start_char_idx": 23556, "end_char_idx": 23938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8649d834-25c2-40ad-9ed1-013a18467f87": {"__data__": {"id_": "8649d834-25c2-40ad-9ed1-013a18467f87", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58ae5b0a-fce6-4459-a97d-f4b642a7984a", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n"}, "hash": "1a5aa8f1da28bb3652e6273f655b8c05bfdd60c375bb4527469f18adef8f25de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4466e77c-3386-43d1-b5c3-d0c5fffdeeea", "node_type": "1", "metadata": {"window": "Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n"}, "hash": "af2eafcbb42b80bade25b2fb53434c7ccfb75062b00304eb68c863e1357f6eae", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n", "start_char_idx": 23938, "end_char_idx": 24412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4466e77c-3386-43d1-b5c3-d0c5fffdeeea": {"__data__": {"id_": "4466e77c-3386-43d1-b5c3-d0c5fffdeeea", "embedding": null, "metadata": {"window": "Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8649d834-25c2-40ad-9ed1-013a18467f87", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMultitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n"}, "hash": "501eb2ec24287b432bca82ceb3f166420900b92f1b336cd38a5bbe061dc7767b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d72e90f-e3ee-48fc-b0d4-6d77f9c97e46", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n", "original_text": "We \ufb01nd the former works better for BART models, and the latter for other models.\n\n"}, "hash": "7c99b3afdb5d8ae41d3a9e6726914f4da5caf45f953528292e9f466cbede4e1a", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n", "start_char_idx": 24412, "end_char_idx": 24905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d72e90f-e3ee-48fc-b0d4-6d77f9c97e46": {"__data__": {"id_": "9d72e90f-e3ee-48fc-b0d4-6d77f9c97e46", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n", "original_text": "We \ufb01nd the former works better for BART models, and the latter for other models.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4466e77c-3386-43d1-b5c3-d0c5fffdeeea", "node_type": "1", "metadata": {"window": "Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n"}, "hash": "af2eafcbb42b80bade25b2fb53434c7ccfb75062b00304eb68c863e1357f6eae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70f02ad8-f45c-4f06-ad3e-2d63801cd0b5", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n"}, "hash": "f6b73650495e240fb0e5b6abc1738a244a55537a0fa4376d2624162b5d1f56f0", "class_name": "RelatedNodeInfo"}}, "text": "We \ufb01nd the former works better for BART models, and the latter for other models.\n\n", "start_char_idx": 24905, "end_char_idx": 24987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70f02ad8-f45c-4f06-ad3e-2d63801cd0b5": {"__data__": {"id_": "70f02ad8-f45c-4f06-ad3e-2d63801cd0b5", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d72e90f-e3ee-48fc-b0d4-6d77f9c97e46", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nMasked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n", "original_text": "We \ufb01nd the former works better for BART models, and the latter for other models.\n\n"}, "hash": "7c99b3afdb5d8ae41d3a9e6726914f4da5caf45f953528292e9f466cbede4e1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6adbb3d9-511d-44e4-8b78-befe1f1d9628", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n"}, "hash": "5f5eda55657091e06690aa622bc32e499dc38c66ab48851b8181aa4fad1ad3b7", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n", "start_char_idx": 24987, "end_char_idx": 25365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6adbb3d9-511d-44e4-8b78-befe1f1d9628": {"__data__": {"id_": "6adbb3d9-511d-44e4-8b78-befe1f1d9628", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70f02ad8-f45c-4f06-ad3e-2d63801cd0b5", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nFor the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n"}, "hash": "f6b73650495e240fb0e5b6abc1738a244a55537a0fa4376d2624162b5d1f56f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "431ca2e5-480f-466c-976c-0d7f059cd6d8", "node_type": "1", "metadata": {"window": "We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n", "original_text": "Answers are text spans extracted from a given document context.\n"}, "hash": "1722117ae6b1ad27383a067fb5853627dfa7054e44341e5761dc68088c44ace1", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n", "start_char_idx": 25365, "end_char_idx": 25661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "431ca2e5-480f-466c-976c-0d7f059cd6d8": {"__data__": {"id_": "431ca2e5-480f-466c-976c-0d7f059cd6d8", "embedding": null, "metadata": {"window": "We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n", "original_text": "Answers are text spans extracted from a given document context.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6adbb3d9-511d-44e4-8b78-befe1f1d9628", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence.\n We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n"}, "hash": "5f5eda55657091e06690aa622bc32e499dc38c66ab48851b8181aa4fad1ad3b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61721f0b-9f03-44be-88b9-d5893725ca43", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n", "original_text": "Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n"}, "hash": "9de61725d709d73c88ac8ecf560943b683e8276f059c727544f100637b3475aa", "class_name": "RelatedNodeInfo"}}, "text": "Answers are text spans extracted from a given document context.\n", "start_char_idx": 25661, "end_char_idx": 25725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61721f0b-9f03-44be-88b9-d5893725ca43": {"__data__": {"id_": "61721f0b-9f03-44be-88b9-d5893725ca43", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n", "original_text": "Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "431ca2e5-480f-466c-976c-0d7f059cd6d8", "node_type": "1", "metadata": {"window": "We \ufb01nd the former works better for BART models, and the latter for other models.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n", "original_text": "Answers are text spans extracted from a given document context.\n"}, "hash": "1722117ae6b1ad27383a067fb5853627dfa7054e44341e5761dc68088c44ace1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb56fc15-cca7-4cdc-883f-9b1ecb39ccf9", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n", "original_text": "The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n"}, "hash": "1f806649bbf5c8ac94586830ef39a6c989ff7d5c27d92352a044cf6effdc50a7", "class_name": "RelatedNodeInfo"}}, "text": "Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n", "start_char_idx": 25725, "end_char_idx": 25881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb56fc15-cca7-4cdc-883f-9b1ecb39ccf9": {"__data__": {"id_": "cb56fc15-cca7-4cdc-883f-9b1ecb39ccf9", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n", "original_text": "The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61721f0b-9f03-44be-88b9-d5893725ca43", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > A B C D E <s> A B C D E\nTo most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n", "original_text": "Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n"}, "hash": "9de61725d709d73c88ac8ecf560943b683e8276f059c727544f100637b3475aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e61c2eaf-977b-44c0-acd1-2b208c4dd282", "node_type": "1", "metadata": {"window": "Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n"}, "hash": "6e5d6c48b602453a9087a5fd4c6e1a7ff1d6a64b69488840488b3bd6da05c019", "class_name": "RelatedNodeInfo"}}, "text": "The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n", "start_char_idx": 25881, "end_char_idx": 25964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e61c2eaf-977b-44c0-acd1-2b208c4dd282": {"__data__": {"id_": "e61c2eaf-977b-44c0-acd1-2b208c4dd282", "embedding": null, "metadata": {"window": "Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb56fc15-cca7-4cdc-883f-9b1ecb39ccf9", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nSQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n", "original_text": "The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n"}, "hash": "1f806649bbf5c8ac94586830ef39a6c989ff7d5c27d92352a044cf6effdc50a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc6a850c-6129-4b6c-9ad0-3df8341f3346", "node_type": "1", "metadata": {"window": "Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n", "original_text": "The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n"}, "hash": "b686ceaef6436c20d6efc04d36a977c8862d02463a83bb59ac5e415c971fafec", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n", "start_char_idx": 25964, "end_char_idx": 26272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc6a850c-6129-4b6c-9ad0-3df8341f3346": {"__data__": {"id_": "fc6a850c-6129-4b6c-9ad0-3df8341f3346", "embedding": null, "metadata": {"window": "Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n", "original_text": "The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e61c2eaf-977b-44c0-acd1-2b208c4dd282", "node_type": "1", "metadata": {"window": "Answers are text spans extracted from a given document context.\n Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n"}, "hash": "6e5d6c48b602453a9087a5fd4c6e1a7ff1d6a64b69488840488b3bd6da05c019", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b62ba0f1-a715-400b-b942-d62cddf74b84", "node_type": "1", "metadata": {"window": "The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n", "original_text": "In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n"}, "hash": "5604a8c59af78d146f7920f45bebeb8633424992a3a699dda722c1b598d273bb", "class_name": "RelatedNodeInfo"}}, "text": "The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n", "start_char_idx": 26272, "end_char_idx": 26405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b62ba0f1-a715-400b-b942-d62cddf74b84": {"__data__": {"id_": "b62ba0f1-a715-400b-b942-d62cddf74b84", "embedding": null, "metadata": {"window": "The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n", "original_text": "In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc6a850c-6129-4b6c-9ad0-3df8341f3346", "node_type": "1", "metadata": {"window": "Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n", "original_text": "The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n"}, "hash": "b686ceaef6436c20d6efc04d36a977c8862d02463a83bb59ac5e415c971fafec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b34a463f-8734-4f1a-8aa2-0219615c2042", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n"}, "hash": "eebe9feb16a8e26288678883951e38453bee933f4b456e471cb72999e9d3654a", "class_name": "RelatedNodeInfo"}}, "text": "In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n", "start_char_idx": 26405, "end_char_idx": 26508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b34a463f-8734-4f1a-8aa2-0219615c2042": {"__data__": {"id_": "b34a463f-8734-4f1a-8aa2-0219615c2042", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b62ba0f1-a715-400b-b942-d62cddf74b84", "node_type": "1", "metadata": {"window": "The model includes classi\ufb01ers to predict the start and end indices of each token.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n", "original_text": "In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n"}, "hash": "5604a8c59af78d146f7920f45bebeb8633424992a3a699dda722c1b598d273bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64cdf3cd-d1e7-41f5-b9ed-3b18129c79ec", "node_type": "1", "metadata": {"window": "The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n", "original_text": "Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n"}, "hash": "e98d98da4cd5fcfddc10137132f280eb18d767c28c4ffa216f5537b934b55b10", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n", "start_char_idx": 26508, "end_char_idx": 26786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64cdf3cd-d1e7-41f5-b9ed-3b18129c79ec": {"__data__": {"id_": "64cdf3cd-d1e7-41f5-b9ed-3b18129c79ec", "embedding": null, "metadata": {"window": "The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n", "original_text": "Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b34a463f-8734-4f1a-8aa2-0219615c2042", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nMNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another.\n The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n"}, "hash": "eebe9feb16a8e26288678883951e38453bee933f4b456e471cb72999e9d3654a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1908889c-6ace-4511-8282-3a56c4141879", "node_type": "1", "metadata": {"window": "In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n"}, "hash": "b264932e3ec58c07a2b674530b99052f8b8e413f7840c956e224377ad228b2cb", "class_name": "RelatedNodeInfo"}}, "text": "Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n", "start_char_idx": 26786, "end_char_idx": 26884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1908889c-6ace-4511-8282-3a56c4141879": {"__data__": {"id_": "1908889c-6ace-4511-8282-3a56c4141879", "embedding": null, "metadata": {"window": "In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64cdf3cd-d1e7-41f5-b9ed-3b18129c79ec", "node_type": "1", "metadata": {"window": "The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n", "original_text": "Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n"}, "hash": "e98d98da4cd5fcfddc10137132f280eb18d767c28c4ffa216f5537b934b55b10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d64a585e-7c6d-4fef-a3d9-ed7bcf0689dc", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n"}, "hash": "eb78584fc2c6ed4304823a2100762d3d1a0c4293de73e8b257c450afc8f912d6", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n", "start_char_idx": 26884, "end_char_idx": 27179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d64a585e-7c6d-4fef-a3d9-ed7bcf0689dc": {"__data__": {"id_": "d64a585e-7c6d-4fef-a3d9-ed7bcf0689dc", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1908889c-6ace-4511-8282-3a56c4141879", "node_type": "1", "metadata": {"window": "In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n"}, "hash": "b264932e3ec58c07a2b674530b99052f8b8e413f7840c956e224377ad228b2cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbebd9be-4a75-4d33-8a71-890c2f2e9dcf", "node_type": "1", "metadata": {"window": "Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n"}, "hash": "659283ef6bed5e16d079be47e3323479a4bab7434ba9ced37ee2476cb7220107", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n", "start_char_idx": 27179, "end_char_idx": 27486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbebd9be-4a75-4d33-8a71-890c2f2e9dcf": {"__data__": {"id_": "fbebd9be-4a75-4d33-8a71-890c2f2e9dcf", "embedding": null, "metadata": {"window": "Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d64a585e-7c6d-4fef-a3d9-ed7bcf0689dc", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n"}, "hash": "eb78584fc2c6ed4304823a2100762d3d1a0c4293de73e8b257c450afc8f912d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1807f7ac-1e86-4f32-87bc-83ae684d4327", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n", "original_text": "Summaries here are typically closely related to source sentences.\n\n"}, "hash": "26b51fb28c2e7f5b35800e12f003d293001ce45c24f2238caac292e1488f1175", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n", "start_char_idx": 27486, "end_char_idx": 27748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1807f7ac-1e86-4f32-87bc-83ae684d4327": {"__data__": {"id_": "1807f7ac-1e86-4f32-87bc-83ae684d4327", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n", "original_text": "Summaries here are typically closely related to source sentences.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbebd9be-4a75-4d33-8a71-890c2f2e9dcf", "node_type": "1", "metadata": {"window": "Models generate answers conditioned on the concatenation of a question and supporting documents.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n"}, "hash": "659283ef6bed5e16d079be47e3323479a4bab7434ba9ced37ee2476cb7220107", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95a43bf2-b620-4f64-84a0-1d03a03139e2", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n"}, "hash": "3164fd1ef5f076ce529280dd10ceebe38644d40cc70ad348dabea89655905af1", "class_name": "RelatedNodeInfo"}}, "text": "Summaries here are typically closely related to source sentences.\n\n", "start_char_idx": 27748, "end_char_idx": 27815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95a43bf2-b620-4f64-84a0-1d03a03139e2": {"__data__": {"id_": "95a43bf2-b620-4f64-84a0-1d03a03139e2", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1807f7ac-1e86-4f32-87bc-83ae684d4327", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nXSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n", "original_text": "Summaries here are typically closely related to source sentences.\n\n"}, "hash": "26b51fb28c2e7f5b35800e12f003d293001ce45c24f2238caac292e1488f1175", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f96a65d0-28bf-431b-9366-6861791d3e41", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n", "original_text": "Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n"}, "hash": "433ba40933912c4c50580296a9bc6314a21ea5747c525baaf2a2015edd359d79", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n", "start_char_idx": 27815, "end_char_idx": 28048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f96a65d0-28bf-431b-9366-6861791d3e41": {"__data__": {"id_": "f96a65d0-28bf-431b-9366-6861791d3e41", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n", "original_text": "Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95a43bf2-b620-4f64-84a0-1d03a03139e2", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n"}, "hash": "3164fd1ef5f076ce529280dd10ceebe38644d40cc70ad348dabea89655905af1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e24b4de-6e12-4d63-97aa-753bdc952f6f", "node_type": "1", "metadata": {"window": "Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n", "original_text": "All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n"}, "hash": "6d9b6b15725bd8f9878b0e0cf19064e5fc28905983e8ef197a9b019cd556f6d6", "class_name": "RelatedNodeInfo"}}, "text": "Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n", "start_char_idx": 28048, "end_char_idx": 29495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e24b4de-6e12-4d63-97aa-753bdc952f6f": {"__data__": {"id_": "4e24b4de-6e12-4d63-97aa-753bdc952f6f", "embedding": null, "metadata": {"window": "Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n", "original_text": "All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f96a65d0-28bf-431b-9366-6861791d3e41", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.2 Tasks\nCNN/DM (Hermann et al., 2015), a news summarization dataset.\n Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n", "original_text": "Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n"}, "hash": "433ba40933912c4c50580296a9bc6314a21ea5747c525baaf2a2015edd359d79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c108fc0-6b3d-4d7e-b3cf-61067f12742c", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n", "original_text": "Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n"}, "hash": "29ffa423bdd2037352f5ae37249668ab7a366724e381bd354d6c7a560576a226", "class_name": "RelatedNodeInfo"}}, "text": "All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n", "start_char_idx": 29495, "end_char_idx": 29604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c108fc0-6b3d-4d7e-b3cf-61067f12742c": {"__data__": {"id_": "0c108fc0-6b3d-4d7e-b3cf-61067f12742c", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n", "original_text": "Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e24b4de-6e12-4d63-97aa-753bdc952f6f", "node_type": "1", "metadata": {"window": "Summaries here are typically closely related to source sentences.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n", "original_text": "All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n"}, "hash": "6d9b6b15725bd8f9878b0e0cf19064e5fc28905983e8ef197a9b019cd556f6d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4518347-48f0-4546-9cc4-0154db6652de", "node_type": "1", "metadata": {"window": "Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n", "original_text": "Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n"}, "hash": "eec15f5ee02cd5e8b6d31106829be4ef223f50a347e5082455a80c652712e940", "class_name": "RelatedNodeInfo"}}, "text": "Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n", "start_char_idx": 29604, "end_char_idx": 29733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4518347-48f0-4546-9cc4-0154db6652de": {"__data__": {"id_": "f4518347-48f0-4546-9cc4-0154db6652de", "embedding": null, "metadata": {"window": "Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n", "original_text": "Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c108fc0-6b3d-4d7e-b3cf-61067f12742c", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nResults are shown in Table 1.\n Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n", "original_text": "Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n"}, "hash": "29ffa423bdd2037352f5ae37249668ab7a366724e381bd354d6c7a560576a226", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "824e5304-1f2d-4538-b1e4-676573024aca", "node_type": "1", "metadata": {"window": "All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n", "original_text": "Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n"}, "hash": "028198e6b3affd945a45d5fb97c877b3ddf1cb8a88c6e68d684c956dac8ad416", "class_name": "RelatedNodeInfo"}}, "text": "Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n", "start_char_idx": 29733, "end_char_idx": 29898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "824e5304-1f2d-4538-b1e4-676573024aca": {"__data__": {"id_": "824e5304-1f2d-4538-b1e4-676573024aca", "embedding": null, "metadata": {"window": "All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n", "original_text": "Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4518347-48f0-4546-9cc4-0154db6652de", "node_type": "1", "metadata": {"window": "Several trends are clear:\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\n | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n | --- | --- | --- | --- | --- | --- | ---\n | F1 Acc PPL PPL PPL PPL\n | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n | w/ Text In\ufb01lling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n | w/ Sentence Shuf\ufb02ing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n | w/ Text In\ufb01lling + Sentence Shuf\ufb02ing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nTable 1: Comparison of pre-training objectives.\n All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n", "original_text": "Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n"}, "hash": "eec15f5ee02cd5e8b6d31106829be4ef223f50a347e5082455a80c652712e940", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "997ea78f-7c5b-4259-8087-d5b2a35e4c44", "node_type": "1", "metadata": {"window": "Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n"}, "hash": "89eabf88b68fbc9ee0c82fca2940295d43d930cd93374d638802fe0cc9fb5ab5", "class_name": "RelatedNodeInfo"}}, "text": "Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n", "start_char_idx": 29898, "end_char_idx": 30038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "997ea78f-7c5b-4259-8087-d5b2a35e4c44": {"__data__": {"id_": "997ea78f-7c5b-4259-8087-d5b2a35e4c44", "embedding": null, "metadata": {"window": "Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "824e5304-1f2d-4538-b1e4-676573024aca", "node_type": "1", "metadata": {"window": "All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n", "original_text": "Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n"}, "hash": "028198e6b3affd945a45d5fb97c877b3ddf1cb8a88c6e68d684c956dac8ad416", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7901f16-872b-48c0-8410-870a7b801725", "node_type": "1", "metadata": {"window": "Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n", "original_text": "For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n"}, "hash": "1d04a4cd648a24f77f930aa9f085ace0e8ad4b470c325b7f1929be6651aa17cc", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n", "start_char_idx": 30038, "end_char_idx": 30385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7901f16-872b-48c0-8410-870a7b801725": {"__data__": {"id_": "d7901f16-872b-48c0-8410-870a7b801725", "embedding": null, "metadata": {"window": "Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n", "original_text": "For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "997ea78f-7c5b-4259-8087-d5b2a35e4c44", "node_type": "1", "metadata": {"window": "Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures.\n Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n"}, "hash": "89eabf88b68fbc9ee0c82fca2940295d43d930cd93374d638802fe0cc9fb5ab5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05c7232d-627c-498b-9be8-81832a1cf585", "node_type": "1", "metadata": {"window": "Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n"}, "hash": "75cb00d10da7bb8156803602791103c99a8a307acc5538ad11392cae134f3913", "class_name": "RelatedNodeInfo"}}, "text": "For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n", "start_char_idx": 30385, "end_char_idx": 30488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05c7232d-627c-498b-9be8-81832a1cf585": {"__data__": {"id_": "05c7232d-627c-498b-9be8-81832a1cf585", "embedding": null, "metadata": {"window": "Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7901f16-872b-48c0-8410-870a7b801725", "node_type": "1", "metadata": {"window": "Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1).\n Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n", "original_text": "For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n"}, "hash": "1d04a4cd648a24f77f930aa9f085ace0e8ad4b470c325b7f1929be6651aa17cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "222e4b78-4982-407d-aeaf-dca16f5360a7", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n", "original_text": "The successful methods either use token deletion or masking, or self-attention masks.\n"}, "hash": "6fe24f054836983a37173e2a130962e189b748b22ecb4998527ebeab3c0d7a7d", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n", "start_char_idx": 30488, "end_char_idx": 30820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "222e4b78-4982-407d-aeaf-dca16f5360a7": {"__data__": {"id_": "222e4b78-4982-407d-aeaf-dca16f5360a7", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n", "original_text": "The successful methods either use token deletion or masking, or self-attention masks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05c7232d-627c-498b-9be8-81832a1cf585", "node_type": "1", "metadata": {"window": "Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n"}, "hash": "75cb00d10da7bb8156803602791103c99a8a307acc5538ad11392cae134f3913", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c2bbbf5-d038-4643-8927-d0339a7140d5", "node_type": "1", "metadata": {"window": "For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n", "original_text": "Deletion appears to outperform masking on generation tasks.\n\n"}, "hash": "38fcd2884e2630a6261956dd7a22ead2999a934fb47482f7aa3fec5445ba2186", "class_name": "RelatedNodeInfo"}}, "text": "The successful methods either use token deletion or masking, or self-attention masks.\n", "start_char_idx": 30820, "end_char_idx": 30906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c2bbbf5-d038-4643-8927-d0339a7140d5": {"__data__": {"id_": "9c2bbbf5-d038-4643-8927-d0339a7140d5", "embedding": null, "metadata": {"window": "For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n", "original_text": "Deletion appears to outperform masking on generation tasks.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "222e4b78-4982-407d-aeaf-dca16f5360a7", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPerformance of pre-training methods varies signi\ufb01cantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n", "original_text": "The successful methods either use token deletion or masking, or self-attention masks.\n"}, "hash": "6fe24f054836983a37173e2a130962e189b748b22ecb4998527ebeab3c0d7a7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b99ab72d-ee41-41d6-9042-2c6401f9ce9e", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n"}, "hash": "c9249e9a78450f50bc229e36eb6b54599a15c90c570cb3748c79da310bbd54a4", "class_name": "RelatedNodeInfo"}}, "text": "Deletion appears to outperform masking on generation tasks.\n\n", "start_char_idx": 30906, "end_char_idx": 30967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b99ab72d-ee41-41d6-9042-2c6401f9ce9e": {"__data__": {"id_": "b99ab72d-ee41-41d6-9042-2c6401f9ce9e", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c2bbbf5-d038-4643-8927-d0339a7140d5", "node_type": "1", "metadata": {"window": "For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n", "original_text": "Deletion appears to outperform masking on generation tasks.\n\n"}, "hash": "38fcd2884e2630a6261956dd7a22ead2999a934fb47482f7aa3fec5445ba2186", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "337d05fe-4019-452b-b08e-e8af51508ade", "node_type": "1", "metadata": {"window": "The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n"}, "hash": "d351bb615bc9435516874160511b4b0192b9d41fcbbc43baf0ac14e5efbf79a6", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n", "start_char_idx": 30967, "end_char_idx": 31447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "337d05fe-4019-452b-b08e-e8af51508ade": {"__data__": {"id_": "337d05fe-4019-452b-b08e-e8af51508ade", "embedding": null, "metadata": {"window": "The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b99ab72d-ee41-41d6-9042-2c6401f9ce9e", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nToken masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n"}, "hash": "c9249e9a78450f50bc229e36eb6b54599a15c90c570cb3748c79da310bbd54a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9dd879c-a983-4a07-8516-7454d5257988", "node_type": "1", "metadata": {"window": "Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n", "original_text": "However, BART achieves similar performance with only half the number of bidirectional layers.\n\n"}, "hash": "5969758806a6097976adeb3abf95c479e9efd06e7cfdf8898957d4fe3698e834", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n", "start_char_idx": 31447, "end_char_idx": 31859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9dd879c-a983-4a07-8516-7454d5257988": {"__data__": {"id_": "c9dd879c-a983-4a07-8516-7454d5257988", "embedding": null, "metadata": {"window": "Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n", "original_text": "However, BART achieves similar performance with only half the number of bidirectional layers.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "337d05fe-4019-452b-b08e-e8af51508ade", "node_type": "1", "metadata": {"window": "The successful methods either use token deletion or masking, or self-attention masks.\n Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n"}, "hash": "d351bb615bc9435516874160511b4b0192b9d41fcbbc43baf0ac14e5efbf79a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e67ebded-242a-4922-9eb7-e4f587ce8aca", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n"}, "hash": "1259230ced005f939ec13e92f537057aa1fdec954fa53112c21b1f10c5e7ed10", "class_name": "RelatedNodeInfo"}}, "text": "However, BART achieves similar performance with only half the number of bidirectional layers.\n\n", "start_char_idx": 31859, "end_char_idx": 31954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e67ebded-242a-4922-9eb7-e4f587ce8aca": {"__data__": {"id_": "e67ebded-242a-4922-9eb7-e4f587ce8aca", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9dd879c-a983-4a07-8516-7454d5257988", "node_type": "1", "metadata": {"window": "Deletion appears to outperform masking on generation tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n", "original_text": "However, BART achieves similar performance with only half the number of bidirectional layers.\n\n"}, "hash": "5969758806a6097976adeb3abf95c479e9efd06e7cfdf8898957d4fe3698e834", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9779bebf-26a6-4c5d-93c8-aeddeb1874a0", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n", "original_text": "Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n"}, "hash": "2dc2aa890a13931ac27b5b8229da0bfbbc279b8fd50a18d1efe7e93458f14a1e", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n", "start_char_idx": 31954, "end_char_idx": 32296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9779bebf-26a6-4c5d-93c8-aeddeb1874a0": {"__data__": {"id_": "9779bebf-26a6-4c5d-93c8-aeddeb1874a0", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n", "original_text": "Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e67ebded-242a-4922-9eb7-e4f587ce8aca", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nLeft-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n"}, "hash": "1259230ced005f939ec13e92f537057aa1fdec954fa53112c21b1f10c5e7ed10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87bfd103-7e85-451e-9561-b60d37567d57", "node_type": "1", "metadata": {"window": "However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n"}, "hash": "3945cd448a8465a6104af400f94dd3dd6f4223fdaf348104a26a6aa080bedd66", "class_name": "RelatedNodeInfo"}}, "text": "Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n", "start_char_idx": 32296, "end_char_idx": 32452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87bfd103-7e85-451e-9561-b60d37567d57": {"__data__": {"id_": "87bfd103-7e85-451e-9561-b60d37567d57", "embedding": null, "metadata": {"window": "However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9779bebf-26a6-4c5d-93c8-aeddeb1874a0", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classi\ufb01cation decisions.\n However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n", "original_text": "Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n"}, "hash": "2dc2aa890a13931ac27b5b8229da0bfbbc279b8fd50a18d1efe7e93458f14a1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acc04eca-e503-49ee-879f-291846f10c3d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n", "original_text": "A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n"}, "hash": "c0bb45ff10e16ab2ae03703241e524c4b94b2713ab8cc5cba18813ae38951d61", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n", "start_char_idx": 32452, "end_char_idx": 32845, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acc04eca-e503-49ee-879f-291846f10c3d": {"__data__": {"id_": "acc04eca-e503-49ee-879f-291846f10c3d", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n", "original_text": "A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87bfd103-7e85-451e-9561-b60d37567d57", "node_type": "1", "metadata": {"window": "However, BART achieves similar performance with only half the number of bidirectional layers.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n"}, "hash": "3945cd448a8465a6104af400f94dd3dd6f4223fdaf348104a26a6aa080bedd66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d5e0bb8-e856-483f-b62c-da69dfae1a48", "node_type": "1", "metadata": {"window": "Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n"}, "hash": "d27cfada0c50bd558760835d10c515cfeb9f68cef93bb42a81a041eb728b7fcf", "class_name": "RelatedNodeInfo"}}, "text": "A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n", "start_char_idx": 32845, "end_char_idx": 32980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d5e0bb8-e856-483f-b62c-da69dfae1a48": {"__data__": {"id_": "2d5e0bb8-e856-483f-b62c-da69dfae1a48", "embedding": null, "metadata": {"window": "Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acc04eca-e503-49ee-879f-291846f10c3d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nThe pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n", "original_text": "A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n"}, "hash": "c0bb45ff10e16ab2ae03703241e524c4b94b2713ab8cc5cba18813ae38951d61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a15859d4-293d-4076-8667-6eea8653d569", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n", "original_text": "With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n"}, "hash": "242694acf98350e9739c587652b37e2be744d809d8ca3f5cd15ec7ff0d530993", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n", "start_char_idx": 32980, "end_char_idx": 33239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a15859d4-293d-4076-8667-6eea8653d569": {"__data__": {"id_": "a15859d4-293d-4076-8667-6eea8653d569", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n", "original_text": "With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d5e0bb8-e856-483f-b62c-da69dfae1a48", "node_type": "1", "metadata": {"window": "Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n"}, "hash": "d27cfada0c50bd558760835d10c515cfeb9f68cef93bb42a81a041eb728b7fcf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea7ce394-8ce7-4d90-adf4-04be8005c1b2", "node_type": "1", "metadata": {"window": "A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n"}, "hash": "40ea45b7cbbe474478d48703a8b337ad907952af5b957a5dac48cb2ca9dd6a76", "class_name": "RelatedNodeInfo"}}, "text": "With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n", "start_char_idx": 33239, "end_char_idx": 33327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea7ce394-8ce7-4d90-adf4-04be8005c1b2": {"__data__": {"id_": "ea7ce394-8ce7-4d90-adf4-04be8005c1b2", "embedding": null, "metadata": {"window": "A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a15859d4-293d-4076-8667-6eea8653d569", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nPure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n", "original_text": "With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n"}, "hash": "242694acf98350e9739c587652b37e2be744d809d8ca3f5cd15ec7ff0d530993", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ceacb7f-fd65-48cf-b669-21f3d95e36f3", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n", "original_text": "To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n"}, "hash": "2b5dd58708d30bba804110d94e9ab6e5b24aaeee2d6a8461a3ef1721f7c67eac", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n", "start_char_idx": 33327, "end_char_idx": 33694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ceacb7f-fd65-48cf-b669-21f3d95e36f3": {"__data__": {"id_": "1ceacb7f-fd65-48cf-b669-21f3d95e36f3", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n", "original_text": "To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea7ce394-8ce7-4d90-adf4-04be8005c1b2", "node_type": "1", "metadata": {"window": "A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n"}, "hash": "40ea45b7cbbe474478d48703a8b337ad907952af5b957a5dac48cb2ca9dd6a76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7590f3b9-f176-4232-bb89-2644ce7e1a79", "node_type": "1", "metadata": {"window": "With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n"}, "hash": "af1070cc60694a5918b38c3f4822b762533c64aabe8261381d3be6f3f3d21719", "class_name": "RelatedNodeInfo"}}, "text": "To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n", "start_char_idx": 33694, "end_char_idx": 33852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7590f3b9-f176-4232-bb89-2644ce7e1a79": {"__data__": {"id_": "7590f3b9-f176-4232-bb89-2644ce7e1a79", "embedding": null, "metadata": {"window": "With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ceacb7f-fd65-48cf-b669-21f3d95e36f3", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 4 Comparing Pre-training Objectives > 4.3 Results\nBART achieves the most consistently strong performance.\n With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n", "original_text": "To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n"}, "hash": "2b5dd58708d30bba804110d94e9ab6e5b24aaeee2d6a8461a3ef1721f7c67eac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "414b568b-a768-4621-83bf-bae1d1ae7a59", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n", "original_text": "Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n"}, "hash": "939e6911cb30fe62036670ae71d3db398b64662d854493d7648faf341418d85d", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n", "start_char_idx": 33852, "end_char_idx": 34174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "414b568b-a768-4621-83bf-bae1d1ae7a59": {"__data__": {"id_": "414b568b-a768-4621-83bf-bae1d1ae7a59", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n", "original_text": "Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7590f3b9-f176-4232-bb89-2644ce7e1a79", "node_type": "1", "metadata": {"window": "With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n"}, "hash": "af1070cc60694a5918b38c3f4822b762533c64aabe8261381d3be6f3f3d21719", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9033fbc-42bd-4157-8a82-e9b2682431fc", "node_type": "1", "metadata": {"window": "To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n", "original_text": "Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n"}, "hash": "2f0d62debe4653de18165741ca3c11477d1f58468acb1fa72d3c50d39b53e802", "class_name": "RelatedNodeInfo"}}, "text": "Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n", "start_char_idx": 34174, "end_char_idx": 34279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9033fbc-42bd-4157-8a82-e9b2682431fc": {"__data__": {"id_": "b9033fbc-42bd-4157-8a82-e9b2682431fc", "embedding": null, "metadata": {"window": "To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n", "original_text": "Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "414b568b-a768-4621-83bf-bae1d1ae7a59", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments\nRecent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n", "original_text": "Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n"}, "hash": "939e6911cb30fe62036670ae71d3db398b64662d854493d7648faf341418d85d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04ecb3b0-044d-442e-9407-83b12b77b1bd", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n", "original_text": "Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n"}, "hash": "8a96a803b57cbc953bf99a14eced30e436e0c97f553cb83e2435af517500fb51", "class_name": "RelatedNodeInfo"}}, "text": "Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n", "start_char_idx": 34279, "end_char_idx": 34369, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04ecb3b0-044d-442e-9407-83b12b77b1bd": {"__data__": {"id_": "04ecb3b0-044d-442e-9407-83b12b77b1bd", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n", "original_text": "Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9033fbc-42bd-4157-8a82-e9b2682431fc", "node_type": "1", "metadata": {"window": "To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n", "original_text": "Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n"}, "hash": "2f0d62debe4653de18165741ca3c11477d1f58468acb1fa72d3c50d39b53e802", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0088b0ca-7379-4364-b4d6-e199e54268c6", "node_type": "1", "metadata": {"window": "Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n", "original_text": "We mask 30% of tokens in each document, and permute all sentences.\n"}, "hash": "8f3f22f6421a9fbbaf8e27461914575daf3e7b438dfe4ab6161607c2c66e1cde", "class_name": "RelatedNodeInfo"}}, "text": "Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n", "start_char_idx": 34369, "end_char_idx": 34469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0088b0ca-7379-4364-b4d6-e199e54268c6": {"__data__": {"id_": "0088b0ca-7379-4364-b4d6-e199e54268c6", "embedding": null, "metadata": {"window": "Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n", "original_text": "We mask 30% of tokens in each document, and permute all sentences.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04ecb3b0-044d-442e-9407-83b12b77b1bd", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nWe pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n", "original_text": "Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n"}, "hash": "8a96a803b57cbc953bf99a14eced30e436e0c97f553cb83e2435af517500fb51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4820c030-ecb5-4577-99d9-f063bb7ec589", "node_type": "1", "metadata": {"window": "Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n", "original_text": "Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n"}, "hash": "1555b78ba58075964ac122fa13929b81e8e8d700a1638b1f1f96a9eda071a796", "class_name": "RelatedNodeInfo"}}, "text": "We mask 30% of tokens in each document, and permute all sentences.\n", "start_char_idx": 34469, "end_char_idx": 34536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4820c030-ecb5-4577-99d9-f063bb7ec589": {"__data__": {"id_": "4820c030-ecb5-4577-99d9-f063bb7ec589", "embedding": null, "metadata": {"window": "Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n", "original_text": "Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0088b0ca-7379-4364-b4d6-e199e54268c6", "node_type": "1", "metadata": {"window": "Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n", "original_text": "We mask 30% of tokens in each document, and permute all sentences.\n"}, "hash": "8f3f22f6421a9fbbaf8e27461914575daf3e7b438dfe4ab6161607c2c66e1cde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cde89eb-7d8c-424f-a57c-78e503197265", "node_type": "1", "metadata": {"window": "Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n", "original_text": "BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n"}, "hash": "21a740a1d9a87413ddb4540dc16d703f773487f645da0ef22eb2c2fa4188f527", "class_name": "RelatedNodeInfo"}}, "text": "Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n", "start_char_idx": 34536, "end_char_idx": 35742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8cde89eb-7d8c-424f-a57c-78e503197265": {"__data__": {"id_": "8cde89eb-7d8c-424f-a57c-78e503197265", "embedding": null, "metadata": {"window": "Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n", "original_text": "BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4820c030-ecb5-4577-99d9-f063bb7ec589", "node_type": "1", "metadata": {"window": "Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n", "original_text": "Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n"}, "hash": "1555b78ba58075964ac122fa13929b81e8e8d700a1638b1f1f96a9eda071a796", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbfe8567-48e1-4545-bfd5-9bc68cb1f3cf", "node_type": "1", "metadata": {"window": "We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n"}, "hash": "1154fa65df9b7ca3fc7c635ca70c68159ccd05ff0f94ba976d29d461db7aebd9", "class_name": "RelatedNodeInfo"}}, "text": "BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n", "start_char_idx": 35742, "end_char_idx": 35895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbfe8567-48e1-4545-bfd5-9bc68cb1f3cf": {"__data__": {"id_": "fbfe8567-48e1-4545-bfd5-9bc68cb1f3cf", "embedding": null, "metadata": {"window": "We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cde89eb-7d8c-424f-a57c-78e503197265", "node_type": "1", "metadata": {"window": "Based on the results in Section \u00a74, we use a combination of text in\ufb01lling and sentence permutation.\n We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n", "original_text": "BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n"}, "hash": "21a740a1d9a87413ddb4540dc16d703f773487f645da0ef22eb2c2fa4188f527", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe215daa-5d7f-4529-9ff9-cf57f2de54dc", "node_type": "1", "metadata": {"window": "Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n", "original_text": "BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n"}, "hash": "fea70d504044d560f3568012a0be30f0b11e69b7b7d664d3c92a4014fff9822e", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n", "start_char_idx": 35895, "end_char_idx": 36992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe215daa-5d7f-4529-9ff9-cf57f2de54dc": {"__data__": {"id_": "fe215daa-5d7f-4529-9ff9-cf57f2de54dc", "embedding": null, "metadata": {"window": "Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n", "original_text": "BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbfe8567-48e1-4545-bfd5-9bc68cb1f3cf", "node_type": "1", "metadata": {"window": "We mask 30% of tokens in each document, and permute all sentences.\n Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n"}, "hash": "1154fa65df9b7ca3fc7c635ca70c68159ccd05ff0f94ba976d29d461db7aebd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20e6f4f8-621e-4f45-a9bd-4920b2f32628", "node_type": "1", "metadata": {"window": "BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n"}, "hash": "2a3a8d7ff3eea50e50d94507162071172786b731a51a483578b65457d3a675b8", "class_name": "RelatedNodeInfo"}}, "text": "BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n", "start_char_idx": 36992, "end_char_idx": 37135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20e6f4f8-621e-4f45-a9bd-4920b2f32628": {"__data__": {"id_": "20e6f4f8-621e-4f45-a9bd-4920b2f32628", "embedding": null, "metadata": {"window": "BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe215daa-5d7f-4529-9ff9-cf57f2de54dc", "node_type": "1", "metadata": {"window": "Although sentence permutation only shows signi\ufb01cant additive gains\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 2: Results for large models on SQuAD and GLUE tasks.\n BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n", "original_text": "BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n"}, "hash": "fea70d504044d560f3568012a0be30f0b11e69b7b7d664d3c92a4014fff9822e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97649038-9eb7-430a-a618-b292110fc983", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n", "original_text": "To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n"}, "hash": "175c7140ab0c0fa5788a43a9cb47bd27db94e3e915d49124720dbad1bb7c6a17", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n", "start_char_idx": 37135, "end_char_idx": 37480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97649038-9eb7-430a-a618-b292110fc983": {"__data__": {"id_": "97649038-9eb7-430a-a618-b292110fc983", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n", "original_text": "To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20e6f4f8-621e-4f45-a9bd-4920b2f32628", "node_type": "1", "metadata": {"window": "BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n"}, "hash": "2a3a8d7ff3eea50e50d94507162071172786b731a51a483578b65457d3a675b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e30baacb-fd04-42ec-ad6f-783fb65214b9", "node_type": "1", "metadata": {"window": "BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n", "original_text": "We use the same pre-training data as Liu et al.\n"}, "hash": "1cf78e2fd95406c1222fa08b21ab29734ac31b11729bcb537095e83464c1a43b", "class_name": "RelatedNodeInfo"}}, "text": "To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n", "start_char_idx": 37480, "end_char_idx": 37574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e30baacb-fd04-42ec-ad6f-783fb65214b9": {"__data__": {"id_": "e30baacb-fd04-42ec-ad6f-783fb65214b9", "embedding": null, "metadata": {"window": "BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n", "original_text": "We use the same pre-training data as Liu et al.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97649038-9eb7-430a-a618-b292110fc983", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\n |  | CNN/DailyMail | XSum\n | --- | --- | ---\n |  | R1 | R2 | RL | R1 | R2 | RL\n | --- | --- | --- | --- | --- | --- | ---\n | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\nTable 3: Results on two standard summarization datasets.\n BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n", "original_text": "To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n"}, "hash": "175c7140ab0c0fa5788a43a9cb47bd27db94e3e915d49124720dbad1bb7c6a17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aff23419-1adc-4fbe-82d0-2f8a7b60b137", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n", "original_text": "(2019), consisting of 160Gb of news, books, stories, and web text.\n\n"}, "hash": "0713caab9f8c66d673b76160e7a466d6d42abaa3ffe01f21e3b52b3b2103e84c", "class_name": "RelatedNodeInfo"}}, "text": "We use the same pre-training data as Liu et al.\n", "start_char_idx": 37574, "end_char_idx": 37622, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aff23419-1adc-4fbe-82d0-2f8a7b60b137": {"__data__": {"id_": "aff23419-1adc-4fbe-82d0-2f8a7b60b137", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n", "original_text": "(2019), consisting of 160Gb of news, books, stories, and web text.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e30baacb-fd04-42ec-ad6f-783fb65214b9", "node_type": "1", "metadata": {"window": "BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n", "original_text": "We use the same pre-training data as Liu et al.\n"}, "hash": "1cf78e2fd95406c1222fa08b21ab29734ac31b11729bcb537095e83464c1a43b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db090263-2d71-420a-82f0-90ab34352e11", "node_type": "1", "metadata": {"window": "To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n"}, "hash": "d72bcd67987f6d2883700a3aded0511297e58b18ffae71089f2c79ff1ec90cc9", "class_name": "RelatedNodeInfo"}}, "text": "(2019), consisting of 160Gb of news, books, stories, and web text.\n\n", "start_char_idx": 37622, "end_char_idx": 37690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db090263-2d71-420a-82f0-90ab34352e11": {"__data__": {"id_": "db090263-2d71-420a-82f0-90ab34352e11", "embedding": null, "metadata": {"window": "To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aff23419-1adc-4fbe-82d0-2f8a7b60b137", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.1 Experimental Setup\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n", "original_text": "(2019), consisting of 160Gb of news, books, stories, and web text.\n\n"}, "hash": "0713caab9f8c66d673b76160e7a466d6d42abaa3ffe01f21e3b52b3b2103e84c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "628243f3-2dc8-458c-a2c5-f977392fb0d4", "node_type": "1", "metadata": {"window": "We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n"}, "hash": "148a460b752fabc401155446c62e5272da9b8199bdb6e09a2a88d4e480b2ab13", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n", "start_char_idx": 37690, "end_char_idx": 38180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "628243f3-2dc8-458c-a2c5-f977392fb0d4": {"__data__": {"id_": "628243f3-2dc8-458c-a2c5-f977392fb0d4", "embedding": null, "metadata": {"window": "We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db090263-2d71-420a-82f0-90ab34352e11", "node_type": "1", "metadata": {"window": "To help the model better \ufb01t the data, we disabled dropout for the \ufb01nal 10% of training steps.\n We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n"}, "hash": "d72bcd67987f6d2883700a3aded0511297e58b18ffae71089f2c79ff1ec90cc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f00a85e5-a2a2-4b10-90e2-75fc8579c00e", "node_type": "1", "metadata": {"window": "(2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n", "original_text": "Overall, BART performs similarly, with only small differences between the models on most tasks.\n"}, "hash": "0f9b55d88ab4ff0bdb9f683b213f00eefd6d61b112dcef2f2dea6b9748041975", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n", "start_char_idx": 38180, "end_char_idx": 38523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f00a85e5-a2a2-4b10-90e2-75fc8579c00e": {"__data__": {"id_": "f00a85e5-a2a2-4b10-90e2-75fc8579c00e", "embedding": null, "metadata": {"window": "(2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n", "original_text": "Overall, BART performs similarly, with only small differences between the models on most tasks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "628243f3-2dc8-458c-a2c5-f977392fb0d4", "node_type": "1", "metadata": {"window": "We use the same pre-training data as Liu et al.\n (2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n"}, "hash": "148a460b752fabc401155446c62e5272da9b8199bdb6e09a2a88d4e480b2ab13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c880acad-48aa-4c59-acd6-5b168a6fe355", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n", "original_text": "suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n"}, "hash": "22284dabeafe61bf4a8d19946a2ad0d53159e10b56a001febc65df5df7016713", "class_name": "RelatedNodeInfo"}}, "text": "Overall, BART performs similarly, with only small differences between the models on most tasks.\n", "start_char_idx": 38523, "end_char_idx": 38619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c880acad-48aa-4c59-acd6-5b168a6fe355": {"__data__": {"id_": "c880acad-48aa-4c59-acd6-5b168a6fe355", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n", "original_text": "suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f00a85e5-a2a2-4b10-90e2-75fc8579c00e", "node_type": "1", "metadata": {"window": "(2019), consisting of 160Gb of news, books, stories, and web text.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n", "original_text": "Overall, BART performs similarly, with only small differences between the models on most tasks.\n"}, "hash": "0f9b55d88ab4ff0bdb9f683b213f00eefd6d61b112dcef2f2dea6b9748041975", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21734035-2713-41b0-8028-819e718595c1", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n"}, "hash": "4e11500fa2c9c8d4d58c01360f8358652653aa2a1697832a04b788105a07d97c", "class_name": "RelatedNodeInfo"}}, "text": "suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n", "start_char_idx": 38619, "end_char_idx": 38733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21734035-2713-41b0-8028-819e718595c1": {"__data__": {"id_": "21734035-2713-41b0-8028-819e718595c1", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c880acad-48aa-4c59-acd6-5b168a6fe355", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n", "original_text": "suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n"}, "hash": "22284dabeafe61bf4a8d19946a2ad0d53159e10b56a001febc65df5df7016713", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ab17891-63ce-440c-ab57-d516372bed56", "node_type": "1", "metadata": {"window": "Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n", "original_text": "BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n"}, "hash": "a52b261571b20fb29421a38668a8ca505debed3f49d5ba177349d856d01a94bb", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n", "start_char_idx": 38733, "end_char_idx": 39003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ab17891-63ce-440c-ab57-d516372bed56": {"__data__": {"id_": "9ab17891-63ce-440c-ab57-d516372bed56", "embedding": null, "metadata": {"window": "Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n", "original_text": "BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21734035-2713-41b0-8028-819e718595c1", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.2 Discriminative Tasks\nThe most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n"}, "hash": "4e11500fa2c9c8d4d58c01360f8358652653aa2a1697832a04b788105a07d97c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88f0dfc5-8089-48ee-8136-195f86136e48", "node_type": "1", "metadata": {"window": "suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n", "original_text": "During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n"}, "hash": "429a5ea67de8b48cabcd8193bb6ebb00847ad70a5c8d77317ea908a07ac3e532", "class_name": "RelatedNodeInfo"}}, "text": "BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n", "start_char_idx": 39003, "end_char_idx": 39097, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88f0dfc5-8089-48ee-8136-195f86136e48": {"__data__": {"id_": "88f0dfc5-8089-48ee-8136-195f86136e48", "embedding": null, "metadata": {"window": "suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n", "original_text": "During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ab17891-63ce-440c-ab57-d516372bed56", "node_type": "1", "metadata": {"window": "Overall, BART performs similarly, with only small differences between the models on most tasks.\n suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n", "original_text": "BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n"}, "hash": "a52b261571b20fb29421a38668a8ca505debed3f49d5ba177349d856d01a94bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c699add8-ae04-4aef-ad01-3c84a296ed71", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n", "original_text": "During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n"}, "hash": "6ea9c82f6340d359bd686a956ee9651b7a78e9f5a7ca7d1d4f1a60d1c1f46c32", "class_name": "RelatedNodeInfo"}}, "text": "During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n", "start_char_idx": 39097, "end_char_idx": 39222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c699add8-ae04-4aef-ad01-3c84a296ed71": {"__data__": {"id_": "c699add8-ae04-4aef-ad01-3c84a296ed71", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n", "original_text": "During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88f0dfc5-8089-48ee-8136-195f86136e48", "node_type": "1", "metadata": {"window": "suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classi\ufb01cation performance.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n", "original_text": "During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n"}, "hash": "429a5ea67de8b48cabcd8193bb6ebb00847ad70a5c8d77317ea908a07ac3e532", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11fe265f-2e0a-40af-aea6-727a0e11b372", "node_type": "1", "metadata": {"window": "BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n"}, "hash": "2ff2effc4bbb556e064d8adb13b521510d85be23dcc25f83cca8d48b5ea18a1b", "class_name": "RelatedNodeInfo"}}, "text": "During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n", "start_char_idx": 39222, "end_char_idx": 39408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11fe265f-2e0a-40af-aea6-727a0e11b372": {"__data__": {"id_": "11fe265f-2e0a-40af-aea6-727a0e11b372", "embedding": null, "metadata": {"window": "BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c699add8-ae04-4aef-ad01-3c84a296ed71", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nWe also experiment with several text generation tasks.\n BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n", "original_text": "During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n"}, "hash": "6ea9c82f6340d359bd686a956ee9651b7a78e9f5a7ca7d1d4f1a60d1c1f46c32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6739b1f4-a215-4bc8-b7b6-a283836831f5", "node_type": "1", "metadata": {"window": "During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n", "original_text": "Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n"}, "hash": "9a1321bada0a54486a99643fed01489d8426ed12e028a1b0d1af2feeca53bd31", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n", "start_char_idx": 39408, "end_char_idx": 40086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6739b1f4-a215-4bc8-b7b6-a283836831f5": {"__data__": {"id_": "6739b1f4-a215-4bc8-b7b6-a283836831f5", "embedding": null, "metadata": {"window": "During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n", "original_text": "Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11fe265f-2e0a-40af-aea6-727a0e11b372", "node_type": "1", "metadata": {"window": "BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text.\n During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n"}, "hash": "2ff2effc4bbb556e064d8adb13b521510d85be23dcc25f83cca8d48b5ea18a1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed8fc279-0b74-4cf7-95d6-370b8c3d4947", "node_type": "1", "metadata": {"window": "During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n"}, "hash": "d8fb0922ffada62b212b55d4a2c51ede35045b33e57d5160ea4937eb3d4735e4", "class_name": "RelatedNodeInfo"}}, "text": "Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n", "start_char_idx": 40086, "end_char_idx": 40157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed8fc279-0b74-4cf7-95d6-370b8c3d4947": {"__data__": {"id_": "ed8fc279-0b74-4cf7-95d6-370b8c3d4947", "embedding": null, "metadata": {"window": "During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6739b1f4-a215-4bc8-b7b6-a283836831f5", "node_type": "1", "metadata": {"window": "During \ufb01netuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n", "original_text": "Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n"}, "hash": "9a1321bada0a54486a99643fed01489d8426ed12e028a1b0d1af2feeca53bd31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74c63ce1-903d-4ca4-8946-93693d9b20f0", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n"}, "hash": "a25088991313303a76dd289ebfe8836d925f97eccf50662a348e78ee8f01dd30", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n", "start_char_idx": 40157, "end_char_idx": 40561, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74c63ce1-903d-4ca4-8946-93693d9b20f0": {"__data__": {"id_": "74c63ce1-903d-4ca4-8946-93693d9b20f0", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed8fc279-0b74-4cf7-95d6-370b8c3d4947", "node_type": "1", "metadata": {"window": "During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n"}, "hash": "d8fb0922ffada62b212b55d4a2c51ede35045b33e57d5160ea4937eb3d4735e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ae66b9d-f22b-44c4-97b8-36a786893308", "node_type": "1", "metadata": {"window": "Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n", "original_text": "Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n"}, "hash": "9cc783c04448ba01601cd9451ed02ca7327474ec5904c6053e2c35f2db1c01ba", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n", "start_char_idx": 40561, "end_char_idx": 40842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ae66b9d-f22b-44c4-97b8-36a786893308": {"__data__": {"id_": "1ae66b9d-f22b-44c4-97b8-36a786893308", "embedding": null, "metadata": {"window": "Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n", "original_text": "Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74c63ce1-903d-4ca4-8946-93693d9b20f0", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\n |  | ConvAI2 | Valid F1 Valid PPL\n | --- | --- | ---\n | Seq2Seq + Attention | 16.02 | 35.07\n | --- | --- | ---\n | Best System | 19.09 | 17.51\n | BART | 20.72 | 11.85\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nTable 4: BART outperforms previous work on conversational response generation.\n Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n"}, "hash": "a25088991313303a76dd289ebfe8836d925f97eccf50662a348e78ee8f01dd30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2aa4fdc6-66ad-42a4-945d-03e69b64ee2b", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n", "original_text": "Nevertheless, BART outperforms all existing work.\n\n"}, "hash": "d054117d9f614cae5b829cb8b4dd0323cb091e7ccfd0977ba3891cb5cb06d407", "class_name": "RelatedNodeInfo"}}, "text": "Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n", "start_char_idx": 40842, "end_char_idx": 40954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2aa4fdc6-66ad-42a4-945d-03e69b64ee2b": {"__data__": {"id_": "2aa4fdc6-66ad-42a4-945d-03e69b64ee2b", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n", "original_text": "Nevertheless, BART outperforms all existing work.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ae66b9d-f22b-44c4-97b8-36a786893308", "node_type": "1", "metadata": {"window": "Perplexities are renormalized based on of\ufb01cial tokenizer for ConvAI2.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n", "original_text": "Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n"}, "hash": "9cc783c04448ba01601cd9451ed02ca7327474ec5904c6053e2c35f2db1c01ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47b5be23-9e06-4af9-9f41-35549a10b10d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n"}, "hash": "42657660262243a3217245413a0a43ddf1dba6f0017199843da8af1e74b00353", "class_name": "RelatedNodeInfo"}}, "text": "Nevertheless, BART outperforms all existing work.\n\n", "start_char_idx": 40954, "end_char_idx": 41005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47b5be23-9e06-4af9-9f41-35549a10b10d": {"__data__": {"id_": "47b5be23-9e06-4af9-9f41-35549a10b10d", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2aa4fdc6-66ad-42a4-945d-03e69b64ee2b", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n", "original_text": "Nevertheless, BART outperforms all existing work.\n\n"}, "hash": "d054117d9f614cae5b829cb8b4dd0323cb091e7ccfd0977ba3891cb5cb06d407", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8707c936-00f5-4c06-899c-b4f0bf6e37e7", "node_type": "1", "metadata": {"window": "Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n", "original_text": "BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n"}, "hash": "b14e65a8b1cc620bcb765ffae993cbd8b1803fe854d9223b9af2bad0a474480c", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n", "start_char_idx": 41005, "end_char_idx": 41299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8707c936-00f5-4c06-899c-b4f0bf6e37e7": {"__data__": {"id_": "8707c936-00f5-4c06-899c-b4f0bf6e37e7", "embedding": null, "metadata": {"window": "Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n", "original_text": "BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47b5be23-9e06-4af9-9f41-35549a10b10d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nSummaries in the CNN/DailyMail tend to resemble source sentences.\n Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n"}, "hash": "42657660262243a3217245413a0a43ddf1dba6f0017199843da8af1e74b00353", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc1562e9-94b7-46b4-a5af-bed32a601635", "node_type": "1", "metadata": {"window": "Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n", "original_text": "Qualitatively, sample quality is high (see \u00a76).\n\n"}, "hash": "1a53338002d70d571757a06262d83bc34adcf6a4643602b281a6597730a64904", "class_name": "RelatedNodeInfo"}}, "text": "BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n", "start_char_idx": 41299, "end_char_idx": 41471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc1562e9-94b7-46b4-a5af-bed32a601635": {"__data__": {"id_": "cc1562e9-94b7-46b4-a5af-bed32a601635", "embedding": null, "metadata": {"window": "Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n", "original_text": "Qualitatively, sample quality is high (see \u00a76).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8707c936-00f5-4c06-899c-b4f0bf6e37e7", "node_type": "1", "metadata": {"window": "Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive.\n Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n", "original_text": "BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n"}, "hash": "b14e65a8b1cc620bcb765ffae993cbd8b1803fe854d9223b9af2bad0a474480c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "106181b1-d011-493a-8f44-3c522e0f6001", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n"}, "hash": "18061973689fb0f4f9ca1b287e1a356661700b09f7ebb2ecc9e96773df2b6544", "class_name": "RelatedNodeInfo"}}, "text": "Qualitatively, sample quality is high (see \u00a76).\n\n", "start_char_idx": 41471, "end_char_idx": 41520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "106181b1-d011-493a-8f44-3c522e0f6001": {"__data__": {"id_": "106181b1-d011-493a-8f44-3c522e0f6001", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc1562e9-94b7-46b4-a5af-bed32a601635", "node_type": "1", "metadata": {"window": "Nevertheless, BART outperforms all existing work.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n", "original_text": "Qualitatively, sample quality is high (see \u00a76).\n\n"}, "hash": "1a53338002d70d571757a06262d83bc34adcf6a4643602b281a6597730a64904", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0db5f796-3afd-4ea1-af06-858425a9b453", "node_type": "1", "metadata": {"window": "BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n", "original_text": "BART outperforms previous work on two automated metrics.\n\n"}, "hash": "80bd4cd10f91c47584db78c64842d1ac833d5400f13a81d1d9ddd2671e6b7320", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n", "start_char_idx": 41520, "end_char_idx": 41933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0db5f796-3afd-4ea1-af06-858425a9b453": {"__data__": {"id_": "0db5f796-3afd-4ea1-af06-858425a9b453", "embedding": null, "metadata": {"window": "BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n", "original_text": "BART outperforms previous work on two automated metrics.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "106181b1-d011-493a-8f44-3c522e0f6001", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nIn contrast, XSum is highly abstractive, and extractive models perform poorly.\n BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n"}, "hash": "18061973689fb0f4f9ca1b287e1a356661700b09f7ebb2ecc9e96773df2b6544", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c2bb0a0-a437-4fa0-8798-e6c70b96a812", "node_type": "1", "metadata": {"window": "Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data. ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n"}, "hash": "17f30b3aaf68bb9982dbe77088a99b96b6b1b106ad51cd770c4d1aec7b2589a9", "class_name": "RelatedNodeInfo"}}, "text": "BART outperforms previous work on two automated metrics.\n\n", "start_char_idx": 41933, "end_char_idx": 41991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c2bb0a0-a437-4fa0-8798-e6c70b96a812": {"__data__": {"id_": "4c2bb0a0-a437-4fa0-8798-e6c70b96a812", "embedding": null, "metadata": {"window": "Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data. ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0db5f796-3afd-4ea1-af06-858425a9b453", "node_type": "1", "metadata": {"window": "BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a signi\ufb01cant advance in performance on this problem.\n Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n", "original_text": "BART outperforms previous work on two automated metrics.\n\n"}, "hash": "80bd4cd10f91c47584db78c64842d1ac833d5400f13a81d1d9ddd2671e6b7320", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac675ba0-5541-4aae-8e78-16be6378afa3", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n ", "original_text": "Comparison models are from Fan et al.\n"}, "hash": "6cb19a2703a75fcc238a6b7ad6816da4a15be42a83863da1e99953175c3976eb", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n", "start_char_idx": 41991, "end_char_idx": 42689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac675ba0-5541-4aae-8e78-16be6378afa3": {"__data__": {"id_": "ac675ba0-5541-4aae-8e78-16be6378afa3", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n ", "original_text": "Comparison models are from Fan et al.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c2bb0a0-a437-4fa0-8798-e6c70b96a812", "node_type": "1", "metadata": {"window": "Qualitatively, sample quality is high (see \u00a76).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data. ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n"}, "hash": "17f30b3aaf68bb9982dbe77088a99b96b6b1b106ad51cd770c4d1aec7b2589a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e50f682-48b8-4b6c-9d69-43188569c5fa", "node_type": "1", "metadata": {"window": "BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. ", "original_text": "(2019).\n\n"}, "hash": "c353d6e800a99843dc937cbceced32c00e7a37123e4d77f72de5ab600da80cc4", "class_name": "RelatedNodeInfo"}}, "text": "Comparison models are from Fan et al.\n", "start_char_idx": 42689, "end_char_idx": 42727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e50f682-48b8-4b6c-9d69-43188569c5fa": {"__data__": {"id_": "0e50f682-48b8-4b6c-9d69-43188569c5fa", "embedding": null, "metadata": {"window": "BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. ", "original_text": "(2019).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac675ba0-5541-4aae-8e78-16be6378afa3", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > 5.3 Generation Tasks\nDialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speci\ufb01ed persona.\n BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n ", "original_text": "Comparison models are from Fan et al.\n"}, "hash": "6cb19a2703a75fcc238a6b7ad6816da4a15be42a83863da1e99953175c3976eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a357e2bd-d2ff-41cc-8ee6-b94767dea07b", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data. "}, "hash": "1929fe15174ea06e41f8196c051fb7da61fc14c2ecfa5209f94a6cd0bba9370e", "class_name": "RelatedNodeInfo"}}, "text": "(2019).\n\n", "start_char_idx": 42727, "end_char_idx": 42736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a357e2bd-d2ff-41cc-8ee6-b94767dea07b": {"__data__": {"id_": "a357e2bd-d2ff-41cc-8ee6-b94767dea07b", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e50f682-48b8-4b6c-9d69-43188569c5fa", "node_type": "1", "metadata": {"window": "BART outperforms previous work on two automated metrics.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. ", "original_text": "(2019).\n\n"}, "hash": "c353d6e800a99843dc937cbceced32c00e7a37123e4d77f72de5ab600da80cc4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a36a69a1-4a79-48ed-abad-5aa841ce7775", "node_type": "1", "metadata": {"window": "Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. ", "original_text": "BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n "}, "hash": "2d30a3cd345076b80511beaed63113f231d346fc3aff92a685082783b08458a7", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data. ", "start_char_idx": 42736, "end_char_idx": 43354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a36a69a1-4a79-48ed-abad-5aa841ce7775": {"__data__": {"id_": "a36a69a1-4a79-48ed-abad-5aa841ce7775", "embedding": null, "metadata": {"window": "Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. ", "original_text": "BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a357e2bd-d2ff-41cc-8ee6-b94767dea07b", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\n | Best Extractive | 23.5 | 3.1 | 17.5\n | --- | --- | --- | ---\n | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL\nTable 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n ", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data. "}, "hash": "1929fe15174ea06e41f8196c051fb7da61fc14c2ecfa5209f94a6cd0bba9370e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27a7693a-7691-4b80-acd0-a2a75c10b84f", "node_type": "1", "metadata": {"window": "(2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016). ", "original_text": "| Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. "}, "hash": "1ddcb365daa8d89a50f7f9034d8ee2078531282258ceac96606ef72e26156d2e", "class_name": "RelatedNodeInfo"}}, "text": "BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n ", "start_char_idx": 43354, "end_char_idx": 43456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27a7693a-7691-4b80-acd0-a2a75c10b84f": {"__data__": {"id_": "27a7693a-7691-4b80-acd0-a2a75c10b84f", "embedding": null, "metadata": {"window": "(2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016). ", "original_text": "| Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a36a69a1-4a79-48ed-abad-5aa841ce7775", "node_type": "1", "metadata": {"window": "Comparison models are from Fan et al.\n (2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. ", "original_text": "BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n "}, "hash": "2d30a3cd345076b80511beaed63113f231d346fc3aff92a685082783b08458a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "628baf0b-57d6-4fbe-938a-75efa6958749", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. ", "original_text": "We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n "}, "hash": "daf5954921b8647e9901b4aeebc66fee0a72517432e1699b70ddef2fc6939a2a", "class_name": "RelatedNodeInfo"}}, "text": "| Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. ", "start_char_idx": 43456, "end_char_idx": 43578, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "628baf0b-57d6-4fbe-938a-75efa6958749": {"__data__": {"id_": "628baf0b-57d6-4fbe-938a-75efa6958749", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. ", "original_text": "We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27a7693a-7691-4b80-acd0-a2a75c10b84f", "node_type": "1", "metadata": {"window": "(2019).\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016). ", "original_text": "| Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. "}, "hash": "1ddcb365daa8d89a50f7f9034d8ee2078531282258ceac96606ef72e26156d2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdec1b10-2761-4c16-be59-a0d20bcc6032", "node_type": "1", "metadata": {"window": "BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6. ", "original_text": "| 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. "}, "hash": "bcb32d9c406d696792b2cd336dcbc7f330dd04e935c7818a5b07388049f856f0", "class_name": "RelatedNodeInfo"}}, "text": "We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n ", "start_char_idx": 43578, "end_char_idx": 43739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fdec1b10-2761-4c16-be59-a0d20bcc6032": {"__data__": {"id_": "fdec1b10-2761-4c16-be59-a0d20bcc6032", "embedding": null, "metadata": {"window": "BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6. ", "original_text": "| 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "628baf0b-57d6-4fbe-938a-75efa6958749", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\n | 7 Related Work | \n | --- | ---\n | Baseline | 36.80\n | Fixed BART 36.29 Tuned BART 37.96\n | Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with backtranslation data.  BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. ", "original_text": "We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n "}, "hash": "daf5954921b8647e9901b4aeebc66fee0a72517432e1699b70ddef2fc6939a2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f592164c-a8ab-435c-a478-3faad276ca8f", "node_type": "1", "metadata": {"window": "| Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). ", "original_text": "(2016). "}, "hash": "5109222dea30110e6926e046557e2d646b2ff452b405ed5d4eaa594e3955668f", "class_name": "RelatedNodeInfo"}}, "text": "| 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. ", "start_char_idx": 43739, "end_char_idx": 43874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f592164c-a8ab-435c-a478-3faad276ca8f": {"__data__": {"id_": "f592164c-a8ab-435c-a478-3faad276ca8f", "embedding": null, "metadata": {"window": "| Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). ", "original_text": "(2016). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdec1b10-2761-4c16-be59-a0d20bcc6032", "node_type": "1", "metadata": {"window": "BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n  | Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6. ", "original_text": "| 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. "}, "hash": "bcb32d9c406d696792b2cd336dcbc7f330dd04e935c7818a5b07388049f856f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e1f96bf-f22b-4bfd-b6d9-f0aaaecbdbf0", "node_type": "1", "metadata": {"window": "We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows. ", "original_text": "We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. "}, "hash": "7ca95fefb8c57b4a75e8ddbbd58dbdaf597db4248457228ed5faadccfe7a5c25", "class_name": "RelatedNodeInfo"}}, "text": "(2016). ", "start_char_idx": 43874, "end_char_idx": 43882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e1f96bf-f22b-4bfd-b6d9-f0aaaecbdbf0": {"__data__": {"id_": "9e1f96bf-f22b-4bfd-b6d9-f0aaaecbdbf0", "embedding": null, "metadata": {"window": "We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows. ", "original_text": "We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f592164c-a8ab-435c-a478-3faad276ca8f", "node_type": "1", "metadata": {"window": "| Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers.  We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). ", "original_text": "(2016). "}, "hash": "5109222dea30110e6926e046557e2d646b2ff452b405ed5d4eaa594e3955668f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b982fdad-9671-43e9-be32-d523ba375968", "node_type": "1", "metadata": {"window": "| 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. ", "original_text": "Experiment results are presented in Table 6. "}, "hash": "de5457cd02b9b3a270876d03f639a65416a7fc41ec06d3c93aa49bb2ad48cc5c", "class_name": "RelatedNodeInfo"}}, "text": "We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. ", "start_char_idx": 43882, "end_char_idx": 44051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b982fdad-9671-43e9-be32-d523ba375968": {"__data__": {"id_": "b982fdad-9671-43e9-be32-d523ba375968", "embedding": null, "metadata": {"window": "| 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. ", "original_text": "Experiment results are presented in Table 6. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e1f96bf-f22b-4bfd-b6d9-f0aaaecbdbf0", "node_type": "1", "metadata": {"window": "We \ufb01nd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speci\ufb01ed by the question.\n  | 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows. ", "original_text": "We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. "}, "hash": "7ca95fefb8c57b4a75e8ddbbd58dbdaf597db4248457228ed5faadccfe7a5c25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "094fe451-2ad4-4459-b64b-9d8aaff77ab4", "node_type": "1", "metadata": {"window": "(2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1. ", "original_text": "We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). "}, "hash": "73881924c17dc1fd5b8ed0bb3793ddcf3629ae3b4c36f3f2f5b376ad191b3a62", "class_name": "RelatedNodeInfo"}}, "text": "Experiment results are presented in Table 6. ", "start_char_idx": 44051, "end_char_idx": 44096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "094fe451-2ad4-4459-b64b-9d8aaff77ab4": {"__data__": {"id_": "094fe451-2ad4-4459-b64b-9d8aaff77ab4", "embedding": null, "metadata": {"window": "(2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1. ", "original_text": "We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b982fdad-9671-43e9-be32-d523ba375968", "node_type": "1", "metadata": {"window": "| 5.4 Translation\n | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al.  (2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. ", "original_text": "Experiment results are presented in Table 6. "}, "hash": "de5457cd02b9b3a270876d03f639a65416a7fc41ec06d3c93aa49bb2ad48cc5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9958109-b101-4252-a517-7a95b1ac6eca", "node_type": "1", "metadata": {"window": "We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n ", "original_text": "We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows. "}, "hash": "c88036a4940d0052c3cbf4a13207efa2a2ccec46c65f1113dbd0a6d0d37e0392", "class_name": "RelatedNodeInfo"}}, "text": "We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). ", "start_char_idx": 44096, "end_char_idx": 44237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9958109-b101-4252-a517-7a95b1ac6eca": {"__data__": {"id_": "c9958109-b101-4252-a517-7a95b1ac6eca", "embedding": null, "metadata": {"window": "We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n ", "original_text": "We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "094fe451-2ad4-4459-b64b-9d8aaff77ab4", "node_type": "1", "metadata": {"window": "(2016).  We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1. ", "original_text": "We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). "}, "hash": "73881924c17dc1fd5b8ed0bb3793ddcf3629ae3b4c36f3f2f5b376ad191b3a62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4374f124-646e-4603-a50e-3cbd2f6ab560", "node_type": "1", "metadata": {"window": "Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. ", "original_text": "For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. "}, "hash": "386dee69cfee323203216e14f96180678d8bf20a8a5c51fb7c5955c9cd62ee01", "class_name": "RelatedNodeInfo"}}, "text": "We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows. ", "start_char_idx": 44237, "end_char_idx": 44326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4374f124-646e-4603-a50e-3cbd2f6ab560": {"__data__": {"id_": "4374f124-646e-4603-a50e-3cbd2f6ab560", "embedding": null, "metadata": {"window": "Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. ", "original_text": "For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9958109-b101-4252-a517-7a95b1ac6eca", "node_type": "1", "metadata": {"window": "We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4.  Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n ", "original_text": "We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows. "}, "hash": "c88036a4940d0052c3cbf4a13207efa2a2ccec46c65f1113dbd0a6d0d37e0392", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b2d69eb-edac-4d43-89a9-15e94830542a", "node_type": "1", "metadata": {"window": "We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n ", "original_text": "We use a beam width of 5 and a length penalty of \u03b1 = 1. "}, "hash": "a49d9f0fa29b02d3a76b3546b0187e6f59fb7478d12725cc5cdae3b0bd57cfba", "class_name": "RelatedNodeInfo"}}, "text": "For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. ", "start_char_idx": 44326, "end_char_idx": 44430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b2d69eb-edac-4d43-89a9-15e94830542a": {"__data__": {"id_": "8b2d69eb-edac-4d43-89a9-15e94830542a", "embedding": null, "metadata": {"window": "We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n ", "original_text": "We use a beam width of 5 and a length penalty of \u03b1 = 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4374f124-646e-4603-a50e-3cbd2f6ab560", "node_type": "1", "metadata": {"window": "Experiment results are presented in Table 6.  We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. ", "original_text": "For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. "}, "hash": "386dee69cfee323203216e14f96180678d8bf20a8a5c51fb7c5955c9cd62ee01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c34196f-f916-444b-9f42-869e6b7985db", "node_type": "1", "metadata": {"window": "We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART. ", "original_text": "Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n "}, "hash": "26cc0d01d4c85287761e6e35a210711bb243e20a74c3dea67d55367402b6ba38", "class_name": "RelatedNodeInfo"}}, "text": "We use a beam width of 5 and a length penalty of \u03b1 = 1. ", "start_char_idx": 44430, "end_char_idx": 44486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c34196f-f916-444b-9f42-869e6b7985db": {"__data__": {"id_": "2c34196f-f916-444b-9f42-869e6b7985db", "embedding": null, "metadata": {"window": "We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART. ", "original_text": "Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b2d69eb-edac-4d43-89a9-15e94830542a", "node_type": "1", "metadata": {"window": "We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row).  We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n ", "original_text": "We use a beam width of 5 and a length penalty of \u03b1 = 1. "}, "hash": "a49d9f0fa29b02d3a76b3546b0187e6f59fb7478d12725cc5cdae3b0bd57cfba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d699d0e-c727-4b35-b367-d393b51776a2", "node_type": "1", "metadata": {"window": "For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data. ", "original_text": "| 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. "}, "hash": "88515b37ff41a6545ca841fd6990bcee54f5a170ed13bf8fb3aa9f822d04881c", "class_name": "RelatedNodeInfo"}}, "text": "Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n ", "start_char_idx": 44486, "end_char_idx": 44674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d699d0e-c727-4b35-b367-d393b51776a2": {"__data__": {"id_": "3d699d0e-c727-4b35-b367-d393b51776a2", "embedding": null, "metadata": {"window": "For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data. ", "original_text": "| 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c34196f-f916-444b-9f42-869e6b7985db", "node_type": "1", "metadata": {"window": "We show the performance of both steps of our model in the \ufb01xed BART and tuned BART rows.  For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART. ", "original_text": "Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n "}, "hash": "26cc0d01d4c85287761e6e35a210711bb243e20a74c3dea67d55367402b6ba38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99666033-5b1d-4c46-94e7-1d052aaef210", "node_type": "1", "metadata": {"window": "We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al. ", "original_text": "To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n "}, "hash": "2dc1528b0d4e8cb6514cc6a69ad365fc6d82531c45399a943d2d3bd167b473ec", "class_name": "RelatedNodeInfo"}}, "text": "| 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. ", "start_char_idx": 44674, "end_char_idx": 44809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99666033-5b1d-4c46-94e7-1d052aaef210": {"__data__": {"id_": "99666033-5b1d-4c46-94e7-1d052aaef210", "embedding": null, "metadata": {"window": "We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al. ", "original_text": "To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d699d0e-c727-4b35-b367-d393b51776a2", "node_type": "1", "metadata": {"window": "For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data.  We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data. ", "original_text": "| 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. "}, "hash": "88515b37ff41a6545ca841fd6990bcee54f5a170ed13bf8fb3aa9f822d04881c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4be2129-2f4c-4e37-a259-a89b638ed331", "node_type": "1", "metadata": {"window": "Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n ", "original_text": "| Table 7 shows example summaries generated by BART. "}, "hash": "9adc5f7d3219de48fdea6613b7b6b5439c861bf9216c8813d59ef850e81c6791", "class_name": "RelatedNodeInfo"}}, "text": "To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n ", "start_char_idx": 44809, "end_char_idx": 44911, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4be2129-2f4c-4e37-a259-a89b638ed331": {"__data__": {"id_": "b4be2129-2f4c-4e37-a259-a89b638ed331", "embedding": null, "metadata": {"window": "Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n ", "original_text": "| Table 7 shows example summaries generated by BART. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99666033-5b1d-4c46-94e7-1d052aaef210", "node_type": "1", "metadata": {"window": "We use a beam width of 5 and a length penalty of \u03b1 = 1.  Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al. ", "original_text": "To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n "}, "hash": "2dc1528b0d4e8cb6514cc6a69ad365fc6d82531c45399a943d2d3bd167b473ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1187b050-891d-4705-8bed-3260d00376b1", "node_type": "1", "metadata": {"window": "| 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English. ", "original_text": "Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data. "}, "hash": "f819f1fb80adcb335696daf48c7214d7dbd7eee0cd0207b8f6fe084f32b79b44", "class_name": "RelatedNodeInfo"}}, "text": "| Table 7 shows example summaries generated by BART. ", "start_char_idx": 44911, "end_char_idx": 44964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1187b050-891d-4705-8bed-3260d00376b1": {"__data__": {"id_": "1187b050-891d-4705-8bed-3260d00376b1", "embedding": null, "metadata": {"window": "| 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English. ", "original_text": "Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4be2129-2f4c-4e37-a259-a89b638ed331", "node_type": "1", "metadata": {"window": "Preliminary results suggested that our approach was less effective without back-translation data, and prone to over\ufb01tting\u2014future work should explore additional regularization techniques.\n  | 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n ", "original_text": "| Table 7 shows example summaries generated by BART. "}, "hash": "9adc5f7d3219de48fdea6613b7b6b5439c861bf9216c8813d59ef850e81c6791", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e0122d6-a04e-4b6b-8f4f-e1ce16cc2411", "node_type": "1", "metadata": {"window": "To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input. ", "original_text": "Following Narayan et al. "}, "hash": "8942fecbf4cdf7dec17f1a282d16b2fb210905ba97bc4b1b868af4a0b829398e", "class_name": "RelatedNodeInfo"}}, "text": "Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data. ", "start_char_idx": 44964, "end_char_idx": 45160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e0122d6-a04e-4b6b-8f4f-e1ce16cc2411": {"__data__": {"id_": "8e0122d6-a04e-4b6b-8f4f-e1ce16cc2411", "embedding": null, "metadata": {"window": "To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input. ", "original_text": "Following Narayan et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1187b050-891d-4705-8bed-3260d00376b1", "node_type": "1", "metadata": {"window": "| 6 Qualitative Analysis\n | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art.  To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English. ", "original_text": "Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data. "}, "hash": "f819f1fb80adcb335696daf48c7214d7dbd7eee0cd0207b8f6fe084f32b79b44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9142af83-a119-4d22-9625-6ee2f3d695cd", "node_type": "1", "metadata": {"window": "| Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). ", "original_text": "(2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n "}, "hash": "790cbe8f0219244ad888ec307de042e0263bb825e4c44336ec753d24b4212a07", "class_name": "RelatedNodeInfo"}}, "text": "Following Narayan et al. ", "start_char_idx": 45160, "end_char_idx": 45185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9142af83-a119-4d22-9625-6ee2f3d695cd": {"__data__": {"id_": "9142af83-a119-4d22-9625-6ee2f3d695cd", "embedding": null, "metadata": {"window": "| Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). ", "original_text": "(2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e0122d6-a04e-4b6b-8f4f-e1ce16cc2411", "node_type": "1", "metadata": {"window": "To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively.\n  | Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input. ", "original_text": "Following Narayan et al. "}, "hash": "8942fecbf4cdf7dec17f1a282d16b2fb210905ba97bc4b1b868af4a0b829398e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2cc09fc-7357-4138-b0b2-04412c26f38c", "node_type": "1", "metadata": {"window": "Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text. ", "original_text": "| Unsurprisingly, model output is \ufb02uent and grammatical English. "}, "hash": "1b1d7334bdb92474f5e28bbf44b843735d9c777ebe78eae033ce85ee03f76f8b", "class_name": "RelatedNodeInfo"}}, "text": "(2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n ", "start_char_idx": 45185, "end_char_idx": 45318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2cc09fc-7357-4138-b0b2-04412c26f38c": {"__data__": {"id_": "a2cc09fc-7357-4138-b0b2-04412c26f38c", "embedding": null, "metadata": {"window": "Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text. ", "original_text": "| Unsurprisingly, model output is \ufb02uent and grammatical English. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9142af83-a119-4d22-9625-6ee2f3d695cd", "node_type": "1", "metadata": {"window": "| Table 7 shows example summaries generated by BART.  Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). ", "original_text": "(2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n "}, "hash": "790cbe8f0219244ad888ec307de042e0263bb825e4c44336ec753d24b4212a07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ce084b9-df7f-4e67-a48b-793abbf668eb", "node_type": "1", "metadata": {"window": "Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n ", "original_text": "However, model output is also highly abstractive, with few phrases copied from the input. "}, "hash": "f8c35c15831de534759a92c69e5b6321e4529c85d5a0691f9d76978ac296f1cd", "class_name": "RelatedNodeInfo"}}, "text": "| Unsurprisingly, model output is \ufb02uent and grammatical English. ", "start_char_idx": 45318, "end_char_idx": 45383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ce084b9-df7f-4e67-a48b-793abbf668eb": {"__data__": {"id_": "1ce084b9-df7f-4e67-a48b-793abbf668eb", "embedding": null, "metadata": {"window": "Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n ", "original_text": "However, model output is also highly abstractive, with few phrases copied from the input. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2cc09fc-7357-4138-b0b2-04412c26f38c", "node_type": "1", "metadata": {"window": "Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.  Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text. ", "original_text": "| Unsurprisingly, model output is \ufb02uent and grammatical English. "}, "hash": "1b1d7334bdb92474f5e28bbf44b843735d9c777ebe78eae033ce85ee03f76f8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b87cc37-621d-4ee3-b38f-9bc769c24bf8", "node_type": "1", "metadata": {"window": "(2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n ", "original_text": "The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). "}, "hash": "500fcb3760bddc2ac65b7d9c323374cf406c4f66c0470819bebac09fa3a24f22", "class_name": "RelatedNodeInfo"}}, "text": "However, model output is also highly abstractive, with few phrases copied from the input. ", "start_char_idx": 45383, "end_char_idx": 45473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b87cc37-621d-4ee3-b38f-9bc769c24bf8": {"__data__": {"id_": "9b87cc37-621d-4ee3-b38f-9bc769c24bf8", "embedding": null, "metadata": {"window": "(2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n ", "original_text": "The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ce084b9-df7f-4e67-a48b-793abbf668eb", "node_type": "1", "metadata": {"window": "Following Narayan et al.  (2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n ", "original_text": "However, model output is also highly abstractive, with few phrases copied from the input. "}, "hash": "f8c35c15831de534759a92c69e5b6321e4529c85d5a0691f9d76978ac296f1cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06389bd4-0980-4b2e-987c-13042428129a", "node_type": "1", "metadata": {"window": "| Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models. ", "original_text": "In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text. "}, "hash": "7678a32c6b168bdf08422d6979ce5f9c76f589daee12538dfc35f424d311b156", "class_name": "RelatedNodeInfo"}}, "text": "The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). ", "start_char_idx": 45473, "end_char_idx": 45704, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06389bd4-0980-4b2e-987c-13042428129a": {"__data__": {"id_": "06389bd4-0980-4b2e-987c-13042428129a", "embedding": null, "metadata": {"window": "| Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models. ", "original_text": "In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b87cc37-621d-4ee3-b38f-9bc769c24bf8", "node_type": "1", "metadata": {"window": "(2018), we remove the \ufb01rst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n  | Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n ", "original_text": "The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). "}, "hash": "500fcb3760bddc2ac65b7d9c323374cf406c4f66c0470819bebac09fa3a24f22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d75bbcc9-36c5-49b5-ac24-3de68abaef96", "node_type": "1", "metadata": {"window": "However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks. ", "original_text": "However, the claim that the work was published in Science is not supported by the source.\n "}, "hash": "c8184b259292f2eb05e771a992ef4893a00d3ec87ed0906847957310140e0b12", "class_name": "RelatedNodeInfo"}}, "text": "In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text. ", "start_char_idx": 45704, "end_char_idx": 45831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d75bbcc9-36c5-49b5-ac24-3de68abaef96": {"__data__": {"id_": "d75bbcc9-36c5-49b5-ac24-3de68abaef96", "embedding": null, "metadata": {"window": "However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks. ", "original_text": "However, the claim that the work was published in Science is not supported by the source.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06389bd4-0980-4b2e-987c-13042428129a", "node_type": "1", "metadata": {"window": "| Unsurprisingly, model output is \ufb02uent and grammatical English.  However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models. ", "original_text": "In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text. "}, "hash": "7678a32c6b168bdf08422d6979ce5f9c76f589daee12538dfc35f424d311b156", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a568791-ba68-48e3-93c0-4f5c90f7a846", "node_type": "1", "metadata": {"window": "The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features. ", "original_text": "| These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n "}, "hash": "34246b70ce5ac308e508cca9316db9778fa108478b9dce652d11f81792371810", "class_name": "RelatedNodeInfo"}}, "text": "However, the claim that the work was published in Science is not supported by the source.\n ", "start_char_idx": 45831, "end_char_idx": 45922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a568791-ba68-48e3-93c0-4f5c90f7a846": {"__data__": {"id_": "7a568791-ba68-48e3-93c0-4f5c90f7a846", "embedding": null, "metadata": {"window": "The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features. ", "original_text": "| These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d75bbcc9-36c5-49b5-ac24-3de68abaef96", "node_type": "1", "metadata": {"window": "However, model output is also highly abstractive, with few phrases copied from the input.  The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks. ", "original_text": "However, the claim that the work was published in Science is not supported by the source.\n "}, "hash": "c8184b259292f2eb05e771a992ef4893a00d3ec87ed0906847957310140e0b12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1653500f-7229-4cb9-acab-b0f854653929", "node_type": "1", "metadata": {"window": "In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al. ", "original_text": "| Early methods for pretraining were based on language models. "}, "hash": "c32dac2cf47f132c3ca8b16c3fec53b7e75342bd45ae508094393151fa535474", "class_name": "RelatedNodeInfo"}}, "text": "| These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n ", "start_char_idx": 45922, "end_char_idx": 46060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1653500f-7229-4cb9-acab-b0f854653929": {"__data__": {"id_": "1653500f-7229-4cb9-acab-b0f854653929", "embedding": null, "metadata": {"window": "In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al. ", "original_text": "| Early methods for pretraining were based on language models. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a568791-ba68-48e3-93c0-4f5c90f7a846", "node_type": "1", "metadata": {"window": "The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California).  In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features. ", "original_text": "| These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n "}, "hash": "34246b70ce5ac308e508cca9316db9778fa108478b9dce652d11f81792371810", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "138559ae-0287-4566-8af9-d5733a46bd06", "node_type": "1", "metadata": {"window": "However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n ", "original_text": "GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks. "}, "hash": "fa1105d3674e215e66085684894930bc496dd31fd87622e4ad473ef6ba0e1ac5", "class_name": "RelatedNodeInfo"}}, "text": "| Early methods for pretraining were based on language models. ", "start_char_idx": 46060, "end_char_idx": 46123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "138559ae-0287-4566-8af9-d5733a46bd06": {"__data__": {"id_": "138559ae-0287-4566-8af9-d5733a46bd06", "embedding": null, "metadata": {"window": "However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n ", "original_text": "GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1653500f-7229-4cb9-acab-b0f854653929", "node_type": "1", "metadata": {"window": "In the \ufb01rst example, inferring that \ufb01sh are protecting reefs from global warming requires non-trivial inference from the text.  However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al. ", "original_text": "| Early methods for pretraining were based on language models. "}, "hash": "c32dac2cf47f132c3ca8b16c3fec53b7e75342bd45ae508094393151fa535474", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7f9eab7-bfd8-49dd-9b20-62a2c0d0dc22", "node_type": "1", "metadata": {"window": "| These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words. ", "original_text": "ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features. "}, "hash": "c41df95b26a53c2755a8b46f400ebbacf9aecdf5ce66139657d55ef50c1defc3", "class_name": "RelatedNodeInfo"}}, "text": "GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks. ", "start_char_idx": 46123, "end_char_idx": 46219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7f9eab7-bfd8-49dd-9b20-62a2c0d0dc22": {"__data__": {"id_": "a7f9eab7-bfd8-49dd-9b20-62a2c0d0dc22", "embedding": null, "metadata": {"window": "| These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words. ", "original_text": "ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "138559ae-0287-4566-8af9-d5733a46bd06", "node_type": "1", "metadata": {"window": "However, the claim that the work was published in Science is not supported by the source.\n  | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n ", "original_text": "GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks. "}, "hash": "fa1105d3674e215e66085684894930bc496dd31fd87622e4ad473ef6ba0e1ac5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88b948a0-b65a-406d-be91-70d816eb9b88", "node_type": "1", "metadata": {"window": "| Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). ", "original_text": "Radford et al. "}, "hash": "668409e0e58b3eb24585c70967346712004f5b95435689e6d6b47bd70fb96d95", "class_name": "RelatedNodeInfo"}}, "text": "ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features. ", "start_char_idx": 46219, "end_char_idx": 46363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88b948a0-b65a-406d-be91-70d816eb9b88": {"__data__": {"id_": "88b948a0-b65a-406d-be91-70d816eb9b88", "embedding": null, "metadata": {"window": "| Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). ", "original_text": "Radford et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7f9eab7-bfd8-49dd-9b20-62a2c0d0dc22", "node_type": "1", "metadata": {"window": "| These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n  | Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words. ", "original_text": "ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features. "}, "hash": "c41df95b26a53c2755a8b46f400ebbacf9aecdf5ce66139657d55ef50c1defc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8dc0d2b-9396-4835-b36d-b895e0832c17", "node_type": "1", "metadata": {"window": "GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n ", "original_text": "(2019) demonstrated that very large language models can act as unsupervised multitask models.\n "}, "hash": "c113663968cf30ee590852ff13da54070a155cead333dd0c09707c9fcb683546", "class_name": "RelatedNodeInfo"}}, "text": "Radford et al. ", "start_char_idx": 46363, "end_char_idx": 46378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8dc0d2b-9396-4835-b36d-b895e0832c17": {"__data__": {"id_": "b8dc0d2b-9396-4835-b36d-b895e0832c17", "embedding": null, "metadata": {"window": "GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n ", "original_text": "(2019) demonstrated that very large language models can act as unsupervised multitask models.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88b948a0-b65a-406d-be91-70d816eb9b88", "node_type": "1", "metadata": {"window": "| Early methods for pretraining were based on language models.  GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). ", "original_text": "Radford et al. "}, "hash": "668409e0e58b3eb24585c70967346712004f5b95435689e6d6b47bd70fb96d95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12950db0-2646-4518-b126-e58a28f3eb2d", "node_type": "1", "metadata": {"window": "ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context. ", "original_text": "| BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words. "}, "hash": "bb501e31904cca7e125fd4bbd6567617e4ca17ea9579019270d8ccdc20b61937", "class_name": "RelatedNodeInfo"}}, "text": "(2019) demonstrated that very large language models can act as unsupervised multitask models.\n ", "start_char_idx": 46378, "end_char_idx": 46473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12950db0-2646-4518-b126-e58a28f3eb2d": {"__data__": {"id_": "12950db0-2646-4518-b126-e58a28f3eb2d", "embedding": null, "metadata": {"window": "ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context. ", "original_text": "| BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8dc0d2b-9396-4835-b36d-b895e0832c17", "node_type": "1", "metadata": {"window": "GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks.  ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n ", "original_text": "(2019) demonstrated that very large language models can act as unsupervised multitask models.\n "}, "hash": "c113663968cf30ee590852ff13da54070a155cead333dd0c09707c9fcb683546", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85e6f023-ff97-4f52-b68a-0bb9c2fc10ed", "node_type": "1", "metadata": {"window": "Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks. ", "original_text": "Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). "}, "hash": "e6ea55780ef994c4773f9b675e3e892d2ba41bfa7f3631dd524d9e34ca0134bb", "class_name": "RelatedNodeInfo"}}, "text": "| BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words. ", "start_char_idx": 46473, "end_char_idx": 46630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85e6f023-ff97-4f52-b68a-0bb9c2fc10ed": {"__data__": {"id_": "85e6f023-ff97-4f52-b68a-0bb9c2fc10ed", "embedding": null, "metadata": {"window": "Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks. ", "original_text": "Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12950db0-2646-4518-b126-e58a28f3eb2d", "node_type": "1", "metadata": {"window": "ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features.  Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context. ", "original_text": "| BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words. "}, "hash": "bb501e31904cca7e125fd4bbd6567617e4ca17ea9579019270d8ccdc20b61937", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf5ef6e3-8946-4ae1-a3ab-128e8775483a", "node_type": "1", "metadata": {"window": "(2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive. ", "original_text": "Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n "}, "hash": "8d0e1cef4c4cb311d3cb5acab566414f31d8f363265385e49dd3eb8c070a6813", "class_name": "RelatedNodeInfo"}}, "text": "Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). ", "start_char_idx": 46630, "end_char_idx": 46856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf5ef6e3-8946-4ae1-a3ab-128e8775483a": {"__data__": {"id_": "bf5ef6e3-8946-4ae1-a3ab-128e8775483a", "embedding": null, "metadata": {"window": "(2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive. ", "original_text": "Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85e6f023-ff97-4f52-b68a-0bb9c2fc10ed", "node_type": "1", "metadata": {"window": "Radford et al.  (2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks. ", "original_text": "Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). "}, "hash": "e6ea55780ef994c4773f9b675e3e892d2ba41bfa7f3631dd524d9e34ca0134bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddd1adc4-3bfd-4c11-8488-718c5c6633ae", "node_type": "1", "metadata": {"window": "| BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n ", "original_text": "| UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context. "}, "hash": "07a5953b20fda92d60f8ae16cef04952b5fa2be3418100cef52fa8c3cad5a160", "class_name": "RelatedNodeInfo"}}, "text": "Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n ", "start_char_idx": 46856, "end_char_idx": 46960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddd1adc4-3bfd-4c11-8488-718c5c6633ae": {"__data__": {"id_": "ddd1adc4-3bfd-4c11-8488-718c5c6633ae", "embedding": null, "metadata": {"window": "| BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n ", "original_text": "| UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf5ef6e3-8946-4ae1-a3ab-128e8775483a", "node_type": "1", "metadata": {"window": "(2019) demonstrated that very large language models can act as unsupervised multitask models.\n  | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive. ", "original_text": "Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n "}, "hash": "8d0e1cef4c4cb311d3cb5acab566414f31d8f363265385e49dd3eb8c070a6813", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40ffc149-c26f-44bc-bb7f-87e1859a7ef2", "node_type": "1", "metadata": {"window": "Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART. ", "original_text": "Like BART, this allows UniLM to be used for both generative and discriminative tasks. "}, "hash": "959a5e7d95be29644e9929faa78e9927e69f43c8513faca125bb8f1adb6835be", "class_name": "RelatedNodeInfo"}}, "text": "| UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context. ", "start_char_idx": 46960, "end_char_idx": 47073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40ffc149-c26f-44bc-bb7f-87e1859a7ef2": {"__data__": {"id_": "40ffc149-c26f-44bc-bb7f-87e1859a7ef2", "embedding": null, "metadata": {"window": "Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART. ", "original_text": "Like BART, this allows UniLM to be used for both generative and discriminative tasks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddd1adc4-3bfd-4c11-8488-718c5c6633ae", "node_type": "1", "metadata": {"window": "| BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words.  Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n ", "original_text": "| UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context. "}, "hash": "07a5953b20fda92d60f8ae16cef04952b5fa2be3418100cef52fa8c3cad5a160", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75db9b41-7ab7-410d-86bc-e431fad9045a", "node_type": "1", "metadata": {"window": "Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens. ", "original_text": "A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive. "}, "hash": "af486214f49ef98f0635eed6779b7e22e50d0fc64a77f243d481899e1af3f989", "class_name": "RelatedNodeInfo"}}, "text": "Like BART, this allows UniLM to be used for both generative and discriminative tasks. ", "start_char_idx": 47073, "end_char_idx": 47159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75db9b41-7ab7-410d-86bc-e431fad9045a": {"__data__": {"id_": "75db9b41-7ab7-410d-86bc-e431fad9045a", "embedding": null, "metadata": {"window": "Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens. ", "original_text": "A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40ffc149-c26f-44bc-bb7f-87e1859a7ef2", "node_type": "1", "metadata": {"window": "Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019).  Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART. ", "original_text": "Like BART, this allows UniLM to be used for both generative and discriminative tasks. "}, "hash": "959a5e7d95be29644e9929faa78e9927e69f43c8513faca125bb8f1adb6835be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c829b6a0-3957-4082-83fc-f95cdc2baff5", "node_type": "1", "metadata": {"window": "| UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n ", "original_text": "BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n "}, "hash": "884dde18458ba5843271f838f4b4bca0961c2b3251190d3788be0e8927b6b571", "class_name": "RelatedNodeInfo"}}, "text": "A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive. ", "start_char_idx": 47159, "end_char_idx": 47266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c829b6a0-3957-4082-83fc-f95cdc2baff5": {"__data__": {"id_": "c829b6a0-3957-4082-83fc-f95cdc2baff5", "embedding": null, "metadata": {"window": "| UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n ", "original_text": "BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75db9b41-7ab7-410d-86bc-e431fad9045a", "node_type": "1", "metadata": {"window": "Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n  | UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens. ", "original_text": "A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive. "}, "hash": "af486214f49ef98f0635eed6779b7e22e50d0fc64a77f243d481899e1af3f989", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f27722b8-21b8-4701-9e21-644db5d20f8e", "node_type": "1", "metadata": {"window": "Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching. ", "original_text": "| MASS (Song et al., 2019) is perhaps the most similar model to BART. "}, "hash": "241b4d777037d3be914668056efc78c6c09cb6af1f5cfc57d079dc4eada8cdc3", "class_name": "RelatedNodeInfo"}}, "text": "BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n ", "start_char_idx": 47266, "end_char_idx": 47404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f27722b8-21b8-4701-9e21-644db5d20f8e": {"__data__": {"id_": "f27722b8-21b8-4701-9e21-644db5d20f8e", "embedding": null, "metadata": {"window": "Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching. ", "original_text": "| MASS (Song et al., 2019) is perhaps the most similar model to BART. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c829b6a0-3957-4082-83fc-f95cdc2baff5", "node_type": "1", "metadata": {"window": "| UniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an ensemble of masks, some of which allow only leftward context.  Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n ", "original_text": "BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n "}, "hash": "884dde18458ba5843271f838f4b4bca0961c2b3251190d3788be0e8927b6b571", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f62e14e1-3795-41bc-aa94-4778af734943", "node_type": "1", "metadata": {"window": "A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae. ", "original_text": "An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens. "}, "hash": "9aeeb6a185a8e2692c48a8a546676c2c4e87c332c566b10a0598d4244a634057", "class_name": "RelatedNodeInfo"}}, "text": "| MASS (Song et al., 2019) is perhaps the most similar model to BART. ", "start_char_idx": 47404, "end_char_idx": 47474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f62e14e1-3795-41bc-aa94-4778af734943": {"__data__": {"id_": "f62e14e1-3795-41bc-aa94-4778af734943", "embedding": null, "metadata": {"window": "A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae. ", "original_text": "An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f27722b8-21b8-4701-9e21-644db5d20f8e", "node_type": "1", "metadata": {"window": "Like BART, this allows UniLM to be used for both generative and discriminative tasks.  A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching. ", "original_text": "| MASS (Song et al., 2019) is perhaps the most similar model to BART. "}, "hash": "241b4d777037d3be914668056efc78c6c09cb6af1f5cfc57d079dc4eada8cdc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f21046b5-32ba-41af-bf41-3e7ba186b0cb", "node_type": "1", "metadata": {"window": "BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n ", "original_text": "MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n "}, "hash": "f935ef720d9466287e8b3d34f15ff6f8cd355bee1b310d08ae223ffe0149324f", "class_name": "RelatedNodeInfo"}}, "text": "An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens. ", "start_char_idx": 47474, "end_char_idx": 47596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f21046b5-32ba-41af-bf41-3e7ba186b0cb": {"__data__": {"id_": "f21046b5-32ba-41af-bf41-3e7ba186b0cb", "embedding": null, "metadata": {"window": "BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n ", "original_text": "MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f62e14e1-3795-41bc-aa94-4778af734943", "node_type": "1", "metadata": {"window": "A difference is that UniLM predictions are conditionally indepen- dent, whereas BART\u2019s are autoregressive.  BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae. ", "original_text": "An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens. "}, "hash": "9aeeb6a185a8e2692c48a8a546676c2c4e87c332c566b10a0598d4244a634057", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea76a5da-62c4-4c27-a892-b96027a527aa", "node_type": "1", "metadata": {"window": "| MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford. ", "original_text": "| XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching. "}, "hash": "d23259cfd15dddd1e92baa4cd4b299ca91f8a27faa39d8eb310b10b1ba27ac74", "class_name": "RelatedNodeInfo"}}, "text": "MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n ", "start_char_idx": 47596, "end_char_idx": 47716, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea76a5da-62c4-4c27-a892-b96027a527aa": {"__data__": {"id_": "ea76a5da-62c4-4c27-a892-b96027a527aa", "embedding": null, "metadata": {"window": "| MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford. ", "original_text": "| XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f21046b5-32ba-41af-bf41-3e7ba186b0cb", "node_type": "1", "metadata": {"window": "BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n  | MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n ", "original_text": "MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n "}, "hash": "f935ef720d9466287e8b3d34f15ff6f8cd355bee1b310d08ae223ffe0149324f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42a1e880-abd9-405f-bcfc-6269c4e96cf7", "node_type": "1", "metadata": {"window": "An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n ", "original_text": "The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae. "}, "hash": "1bb7bc36a3fa32a99a02549f17f7592ee074a69338105c0911b878b4c9a81a73", "class_name": "RelatedNodeInfo"}}, "text": "| XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching. ", "start_char_idx": 47716, "end_char_idx": 48116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42a1e880-abd9-405f-bcfc-6269c4e96cf7": {"__data__": {"id_": "42a1e880-abd9-405f-bcfc-6269c4e96cf7", "embedding": null, "metadata": {"window": "An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n ", "original_text": "The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea76a5da-62c4-4c27-a892-b96027a527aa", "node_type": "1", "metadata": {"window": "| MASS (Song et al., 2019) is perhaps the most similar model to BART.  An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford. ", "original_text": "| XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching. "}, "hash": "d23259cfd15dddd1e92baa4cd4b299ca91f8a27faa39d8eb310b10b1ba27ac74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efae40fa-d98a-473d-b147-f989c42c928b", "node_type": "1", "metadata": {"window": "MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday. ", "original_text": "| Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n "}, "hash": "0799184f63bd4c494eb564892bdcf04fe5b22cc925e3ee39039943e96b5dcd4f", "class_name": "RelatedNodeInfo"}}, "text": "The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae. ", "start_char_idx": 48116, "end_char_idx": 48304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efae40fa-d98a-473d-b147-f989c42c928b": {"__data__": {"id_": "efae40fa-d98a-473d-b147-f989c42c928b", "embedding": null, "metadata": {"window": "MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday. ", "original_text": "| Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42a1e880-abd9-405f-bcfc-6269c4e96cf7", "node_type": "1", "metadata": {"window": "An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens.  MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n ", "original_text": "The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae. "}, "hash": "1bb7bc36a3fa32a99a02549f17f7592ee074a69338105c0911b878b4c9a81a73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "accee453-e9f1-47ae-8737-375f66f7e5b1", "node_type": "1", "metadata": {"window": "| XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone. ", "original_text": "| Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford. "}, "hash": "c99dacf28578c4ab0b71ec460a5998cccb7ef5986bb5efc4e53287d65096c73e", "class_name": "RelatedNodeInfo"}}, "text": "| Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n ", "start_char_idx": 48304, "end_char_idx": 48451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "accee453-e9f1-47ae-8737-375f66f7e5b1": {"__data__": {"id_": "accee453-e9f1-47ae-8737-375f66f7e5b1", "embedding": null, "metadata": {"window": "| XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone. ", "original_text": "| Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efae40fa-d98a-473d-b147-f989c42c928b", "node_type": "1", "metadata": {"window": "MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n  | XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday. ", "original_text": "| Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n "}, "hash": "0799184f63bd4c494eb564892bdcf04fe5b22cc925e3ee39039943e96b5dcd4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d59a0348-2bab-4583-a064-1d5a1edad1cb", "node_type": "1", "metadata": {"window": "The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria . ", "original_text": "He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n "}, "hash": "4e62b617272a63840b3cf1d4205fc2402c30d942b32e5b5f0b2e1b37313543d0", "class_name": "RelatedNodeInfo"}}, "text": "| Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford. ", "start_char_idx": 48451, "end_char_idx": 48646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d59a0348-2bab-4583-a064-1d5a1edad1cb": {"__data__": {"id_": "d59a0348-2bab-4583-a064-1d5a1edad1cb", "embedding": null, "metadata": {"window": "The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria . ", "original_text": "He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "accee453-e9f1-47ae-8737-375f66f7e5b1", "node_type": "1", "metadata": {"window": "| XL-Net (Yang et al., 2019) extends BERT by pre-\n | Source Document (abbreviated) | BART Summary\n | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when \ufb01sh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching.  The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone. ", "original_text": "| Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford. "}, "hash": "c99dacf28578c4ab0b71ec460a5998cccb7ef5986bb5efc4e53287d65096c73e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33e37030-09b0-4d8c-a256-6b5372789fad", "node_type": "1", "metadata": {"window": "| Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion. ", "original_text": "| According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday. "}, "hash": "d2c93b3dfebf0845bbf318abba186b59d865d4c56fc8f3916af66ecfb9340009", "class_name": "RelatedNodeInfo"}}, "text": "He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n ", "start_char_idx": 48646, "end_char_idx": 48923, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33e37030-09b0-4d8c-a256-6b5372789fad": {"__data__": {"id_": "33e37030-09b0-4d8c-a256-6b5372789fad", "embedding": null, "metadata": {"window": "| Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion. ", "original_text": "| According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d59a0348-2bab-4583-a064-1d5a1edad1cb", "node_type": "1", "metadata": {"window": "The researchers sug- gested the algae, like warming temperatures, might render the corals\u2019 chemical defenses less effective, and the \ufb01sh were pro- tecting the coral by removing the algae.  | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria . ", "original_text": "He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n "}, "hash": "4e62b617272a63840b3cf1d4205fc2402c30d942b32e5b5f0b2e1b37313543d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d741be6-e4a0-4691-9f66-a3e2ec830329", "node_type": "1", "metadata": {"window": "| Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region. ", "original_text": "On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone. "}, "hash": "4a810e2a59548495eedbd811da1f3602364e9e30f6c9da4e9e11181c07a237e0", "class_name": "RelatedNodeInfo"}}, "text": "| According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday. ", "start_char_idx": 48923, "end_char_idx": 49046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d741be6-e4a0-4691-9f66-a3e2ec830329": {"__data__": {"id_": "6d741be6-e4a0-4691-9f66-a3e2ec830329", "embedding": null, "metadata": {"window": "| Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region. ", "original_text": "On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33e37030-09b0-4d8c-a256-6b5372789fad", "node_type": "1", "metadata": {"window": "| Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n  | Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion. ", "original_text": "| According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday. "}, "hash": "d2c93b3dfebf0845bbf318abba186b59d865d4c56fc8f3916af66ecfb9340009", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a68da14-ddad-4ca3-bb15-3213f5d87e25", "node_type": "1", "metadata": {"window": "He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time. ", "original_text": "Then both na- tions issued statements speaking of an imminent incursion into northeast Syria . "}, "hash": "5111134a27da4f30675dc73a4c4490e2e96fa58ce3a25fc1bfbbd2c0562716ff", "class_name": "RelatedNodeInfo"}}, "text": "On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone. ", "start_char_idx": 49046, "end_char_idx": 49150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a68da14-ddad-4ca3-bb15-3213f5d87e25": {"__data__": {"id_": "9a68da14-ddad-4ca3-bb15-3213f5d87e25", "embedding": null, "metadata": {"window": "He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time. ", "original_text": "Then both na- tions issued statements speaking of an imminent incursion into northeast Syria . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d741be6-e4a0-4691-9f66-a3e2ec830329", "node_type": "1", "metadata": {"window": "| Sacoolas, who has immunity as a diplomat\u2019s wife, was involved in a traf\ufb01c collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford.  He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region. ", "original_text": "On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone. "}, "hash": "4a810e2a59548495eedbd811da1f3602364e9e30f6c9da4e9e11181c07a237e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bde180a5-a2a6-4265-8e5d-6464768e6d23", "node_type": "1", "metadata": {"window": "| According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF. ", "original_text": "On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion. "}, "hash": "25e641999ee603b96480f04eac2368bed75b1b5f917876d670e06417bfdb4867", "class_name": "RelatedNodeInfo"}}, "text": "Then both na- tions issued statements speaking of an imminent incursion into northeast Syria . ", "start_char_idx": 49150, "end_char_idx": 49245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bde180a5-a2a6-4265-8e5d-6464768e6d23": {"__data__": {"id_": "bde180a5-a2a6-4265-8e5d-6464768e6d23", "embedding": null, "metadata": {"window": "| According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF. ", "original_text": "On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a68da14-ddad-4ca3-bb15-3213f5d87e25", "node_type": "1", "metadata": {"window": "He said, \u201cI hope that Anne Sacoolas will come back if we can\u2019t resolve it then of course I will be raising it myself personally with the White House.\u201d | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas\u2019 diplo- matic immunity with the White House.\n  | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time. ", "original_text": "Then both na- tions issued statements speaking of an imminent incursion into northeast Syria . "}, "hash": "5111134a27da4f30675dc73a4c4490e2e96fa58ce3a25fc1bfbbd2c0562716ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0ff4ede-5af4-4928-a5ed-86a0856ce3b8", "node_type": "1", "metadata": {"window": "On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds. ", "original_text": "Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region. "}, "hash": "d34f6ee0a2f5551ad356d5c0ac618724db8b9eac536428bcc36c5909a19a23af", "class_name": "RelatedNodeInfo"}}, "text": "On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion. ", "start_char_idx": 49245, "end_char_idx": 49340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0ff4ede-5af4-4928-a5ed-86a0856ce3b8": {"__data__": {"id_": "c0ff4ede-5af4-4928-a5ed-86a0856ce3b8", "embedding": null, "metadata": {"window": "On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds. ", "original_text": "Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bde180a5-a2a6-4265-8e5d-6464768e6d23", "node_type": "1", "metadata": {"window": "| According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday.  On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF. ", "original_text": "On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion. "}, "hash": "25e641999ee603b96480f04eac2368bed75b1b5f917876d670e06417bfdb4867", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75784e55-82ce-4f13-b205-aad6ba353694", "node_type": "1", "metadata": {"window": "Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria. ", "original_text": "| \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time. "}, "hash": "711ec24c0a4efa5ea06e43b9028c7e36f4e518d3024306da9f4799b40ab4632e", "class_name": "RelatedNodeInfo"}}, "text": "Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region. ", "start_char_idx": 49340, "end_char_idx": 49492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75784e55-82ce-4f13-b205-aad6ba353694": {"__data__": {"id_": "75784e55-82ce-4f13-b205-aad6ba353694", "embedding": null, "metadata": {"window": "Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria. ", "original_text": "| \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0ff4ede-5af4-4928-a5ed-86a0856ce3b8", "node_type": "1", "metadata": {"window": "On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone.  Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds. ", "original_text": "Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region. "}, "hash": "d34f6ee0a2f5551ad356d5c0ac618724db8b9eac536428bcc36c5909a19a23af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6613249-ce2a-44cc-aaf7-2f3b091dec62", "node_type": "1", "metadata": {"window": "On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier. ", "original_text": "It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF. "}, "hash": "738f9867ffc61d4854f34520255b654060c59ba04c4096f64316bb51b2544b3f", "class_name": "RelatedNodeInfo"}}, "text": "| \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time. ", "start_char_idx": 49492, "end_char_idx": 49648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6613249-ce2a-44cc-aaf7-2f3b091dec62": {"__data__": {"id_": "e6613249-ce2a-44cc-aaf7-2f3b091dec62", "embedding": null, "metadata": {"window": "On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier. ", "original_text": "It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75784e55-82ce-4f13-b205-aad6ba353694", "node_type": "1", "metadata": {"window": "Then both na- tions issued statements speaking of an imminent incursion into northeast Syria .  On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria. ", "original_text": "| \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time. "}, "hash": "711ec24c0a4efa5ea06e43b9028c7e36f4e518d3024306da9f4799b40ab4632e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec312de2-659f-4b0a-ae74-40b2bb52510f", "node_type": "1", "metadata": {"window": "Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n ", "original_text": "His time was 1 hour 59 minutes 40.2 seconds. "}, "hash": "e95a34bbc96912f151d452f41ebb13ac9385b2785ec0c153268524bb3d37ce72", "class_name": "RelatedNodeInfo"}}, "text": "It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF. ", "start_char_idx": 49648, "end_char_idx": 49748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec312de2-659f-4b0a-ae74-40b2bb52510f": {"__data__": {"id_": "ec312de2-659f-4b0a-ae74-40b2bb52510f", "embedding": null, "metadata": {"window": "Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n ", "original_text": "His time was 1 hour 59 minutes 40.2 seconds. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6613249-ce2a-44cc-aaf7-2f3b091dec62", "node_type": "1", "metadata": {"window": "On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion.  Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier. ", "original_text": "It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF. "}, "hash": "738f9867ffc61d4854f34520255b654060c59ba04c4096f64316bb51b2544b3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b3a24ce-7745-4efc-884f-29b7b4d06514", "node_type": "1", "metadata": {"window": "| \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. ", "original_text": "Kipchoge ran in Vienna, Austria. "}, "hash": "3696f33b0c631bd13e39d7017b929375896aefe2d7bd628e97484e553ed51f1a", "class_name": "RelatedNodeInfo"}}, "text": "His time was 1 hour 59 minutes 40.2 seconds. ", "start_char_idx": 49748, "end_char_idx": 49793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b3a24ce-7745-4efc-884f-29b7b4d06514": {"__data__": {"id_": "8b3a24ce-7745-4efc-884f-29b7b4d06514", "embedding": null, "metadata": {"window": "| \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. ", "original_text": "Kipchoge ran in Vienna, Austria. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec312de2-659f-4b0a-ae74-40b2bb52510f", "node_type": "1", "metadata": {"window": "Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey\u2019s incursion into the region.  | \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n ", "original_text": "His time was 1 hour 59 minutes 40.2 seconds. "}, "hash": "e95a34bbc96912f151d452f41ebb13ac9385b2785ec0c153268524bb3d37ce72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "532dd7cb-6964-41f4-8a71-d609a5dc6462", "node_type": "1", "metadata": {"window": "It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res. ", "original_text": "It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier. "}, "hash": "656de0576b9357c75ce86701f5d0f8c2037122fd0cbac438190937227a66f273", "class_name": "RelatedNodeInfo"}}, "text": "Kipchoge ran in Vienna, Austria. ", "start_char_idx": 49793, "end_char_idx": 49826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "532dd7cb-6964-41f4-8a71-d609a5dc6462": {"__data__": {"id_": "532dd7cb-6964-41f4-8a71-d609a5dc6462", "embedding": null, "metadata": {"window": "It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res. ", "original_text": "It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b3a24ce-7745-4efc-884f-29b7b4d06514", "node_type": "1", "metadata": {"window": "| \n | This is the \ufb01rst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time.  It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. ", "original_text": "Kipchoge ran in Vienna, Austria. "}, "hash": "3696f33b0c631bd13e39d7017b929375896aefe2d7bd628e97484e553ed51f1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bec0056-d0e6-4968-b8a4-888e28d931c3", "node_type": "1", "metadata": {"window": "His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. ", "original_text": "| Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n "}, "hash": "e24b25b9a26f4d2f8888b652de338d12bc3a487cf031c0e5e7fe19f086e64a65", "class_name": "RelatedNodeInfo"}}, "text": "It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier. ", "start_char_idx": 49826, "end_char_idx": 49908, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bec0056-d0e6-4968-b8a4-888e28d931c3": {"__data__": {"id_": "3bec0056-d0e6-4968-b8a4-888e28d931c3", "embedding": null, "metadata": {"window": "His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. ", "original_text": "| Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "532dd7cb-6964-41f4-8a71-d609a5dc6462", "node_type": "1", "metadata": {"window": "It was not, however, an of\ufb01cially sanctioned world record, as it was not an\u201dopen race\u201d of the IAAF.  His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res. ", "original_text": "It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier. "}, "hash": "656de0576b9357c75ce86701f5d0f8c2037122fd0cbac438190937227a66f273", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c99b0a0-06a3-4dd8-aa03-e38afa073a24", "node_type": "1", "metadata": {"window": "Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n", "original_text": "| PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. "}, "hash": "ec2d8fc6644ddc2e3a9816cc2139bb8ceced332044653d32baf8709ab295a2bc", "class_name": "RelatedNodeInfo"}}, "text": "| Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n ", "start_char_idx": 49908, "end_char_idx": 49983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c99b0a0-06a3-4dd8-aa03-e38afa073a24": {"__data__": {"id_": "2c99b0a0-06a3-4dd8-aa03-e38afa073a24", "embedding": null, "metadata": {"window": "Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n", "original_text": "| PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bec0056-d0e6-4968-b8a4-888e28d931c3", "node_type": "1", "metadata": {"window": "His time was 1 hour 59 minutes 40.2 seconds.  Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. ", "original_text": "| Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n "}, "hash": "e24b25b9a26f4d2f8888b652de338d12bc3a487cf031c0e5e7fe19f086e64a65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d03636a0-375f-4510-a332-2074242815e2", "node_type": "1", "metadata": {"window": "It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n", "original_text": "The aim is to reduce the risk of wild\ufb01res. "}, "hash": "1cdd022c3d3b60cfb83d4b846b7aa0ba16465970ef2dd73db78285dc269418f5", "class_name": "RelatedNodeInfo"}}, "text": "| PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. ", "start_char_idx": 49983, "end_char_idx": 50085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d03636a0-375f-4510-a332-2074242815e2": {"__data__": {"id_": "d03636a0-375f-4510-a332-2074242815e2", "embedding": null, "metadata": {"window": "It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n", "original_text": "The aim is to reduce the risk of wild\ufb01res. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c99b0a0-06a3-4dd8-aa03-e38afa073a24", "node_type": "1", "metadata": {"window": "Kipchoge ran in Vienna, Austria.  It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n", "original_text": "| PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. "}, "hash": "ec2d8fc6644ddc2e3a9816cc2139bb8ceced332044653d32baf8709ab295a2bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e263d69b-6fc9-4c4d-802c-8eca6cee3de3", "node_type": "1", "metadata": {"window": "| Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n", "original_text": "Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. "}, "hash": "e558e06ac0647f1665ed617faf09ea5554a38daf84997c0f863245cccd7e41c8", "class_name": "RelatedNodeInfo"}}, "text": "The aim is to reduce the risk of wild\ufb01res. ", "start_char_idx": 50085, "end_char_idx": 50128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e263d69b-6fc9-4c4d-802c-8eca6cee3de3": {"__data__": {"id_": "e263d69b-6fc9-4c4d-802c-8eca6cee3de3", "embedding": null, "metadata": {"window": "| Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n", "original_text": "Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d03636a0-375f-4510-a332-2074242815e2", "node_type": "1", "metadata": {"window": "It was an event speci\ufb01cally designed to help Kipchoge break the two hour barrier.  | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n", "original_text": "The aim is to reduce the risk of wild\ufb01res. "}, "hash": "1cdd022c3d3b60cfb83d4b846b7aa0ba16465970ef2dd73db78285dc269418f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23039490-0bbc-45c0-972a-d8bba6dbe2ff", "node_type": "1", "metadata": {"window": "| PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n", "original_text": "| Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n"}, "hash": "f16582729259a1ba2603308e64a24eee2e40db4d86c79a136af585f07433ee22", "class_name": "RelatedNodeInfo"}}, "text": "Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. ", "start_char_idx": 50128, "end_char_idx": 50266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23039490-0bbc-45c0-972a-d8bba6dbe2ff": {"__data__": {"id_": "23039490-0bbc-45c0-972a-d8bba6dbe2ff", "embedding": null, "metadata": {"window": "| PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n", "original_text": "| Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e263d69b-6fc9-4c4d-802c-8eca6cee3de3", "node_type": "1", "metadata": {"window": "| Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n  | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n", "original_text": "Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. "}, "hash": "e558e06ac0647f1665ed617faf09ea5554a38daf84997c0f863245cccd7e41c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "420b0c1e-f5db-4009-b628-70fc56dee777", "node_type": "1", "metadata": {"window": "The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n"}, "hash": "364a098479f8104a2bc878ccbbd3067a02f0fa0d689065410aa4d5d82795d99e", "class_name": "RelatedNodeInfo"}}, "text": "| Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n", "start_char_idx": 50266, "end_char_idx": 50368, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "420b0c1e-f5db-4009-b628-70fc56dee777": {"__data__": {"id_": "420b0c1e-f5db-4009-b628-70fc56dee777", "embedding": null, "metadata": {"window": "The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23039490-0bbc-45c0-972a-d8bba6dbe2ff", "node_type": "1", "metadata": {"window": "| PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.  The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n", "original_text": "| Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n"}, "hash": "f16582729259a1ba2603308e64a24eee2e40db4d86c79a136af585f07433ee22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d41be0a5-45cb-45f3-932e-92cb40ce8f7b", "node_type": "1", "metadata": {"window": "Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n", "original_text": "For clarity, only relevant excerpts of the source are shown.\n"}, "hash": "ae86f3f62a6efc70cc3049c5efb36fc65113f63f4b68cba5f2fd9066fd182de5", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n", "start_char_idx": 50368, "end_char_idx": 50656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d41be0a5-45cb-45f3-932e-92cb40ce8f7b": {"__data__": {"id_": "d41be0a5-45cb-45f3-932e-92cb40ce8f7b", "embedding": null, "metadata": {"window": "Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n", "original_text": "For clarity, only relevant excerpts of the source are shown.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "420b0c1e-f5db-4009-b628-70fc56dee777", "node_type": "1", "metadata": {"window": "The aim is to reduce the risk of wild\ufb01res.  Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n"}, "hash": "364a098479f8104a2bc878ccbbd3067a02f0fa0d689065410aa4d5d82795d99e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00409930-21fb-4c06-9f09-0d202f60a222", "node_type": "1", "metadata": {"window": "| Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n", "original_text": "Summaries combine information from across the article and prior knowledge.\n\n"}, "hash": "3465d067d14eb93c78c4f26f387d66b58fd86ec6b6fe2b5b4be7016cef953c53", "class_name": "RelatedNodeInfo"}}, "text": "For clarity, only relevant excerpts of the source are shown.\n", "start_char_idx": 50656, "end_char_idx": 50717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00409930-21fb-4c06-9f09-0d202f60a222": {"__data__": {"id_": "00409930-21fb-4c06-9f09-0d202f60a222", "embedding": null, "metadata": {"window": "| Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n", "original_text": "Summaries combine information from across the article and prior knowledge.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d41be0a5-45cb-45f3-932e-92cb40ce8f7b", "node_type": "1", "metadata": {"window": "Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.  | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n", "original_text": "For clarity, only relevant excerpts of the source are shown.\n"}, "hash": "ae86f3f62a6efc70cc3049c5efb36fc65113f63f4b68cba5f2fd9066fd182de5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30a68976-0a54-4426-bf39-a2931902602e", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n"}, "hash": "8c019088420dbe4800ecffe5f02cfcba51ab02b1b62ed2c149ee1ebd5978c2fa", "class_name": "RelatedNodeInfo"}}, "text": "Summaries combine information from across the article and prior knowledge.\n\n", "start_char_idx": 50717, "end_char_idx": 50793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30a68976-0a54-4426-bf39-a2931902602e": {"__data__": {"id_": "30a68976-0a54-4426-bf39-a2931902602e", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00409930-21fb-4c06-9f09-0d202f60a222", "node_type": "1", "metadata": {"window": "| Power has been turned off to millions of customers in California as part of a power shutoff plan.\n\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n", "original_text": "Summaries combine information from across the article and prior knowledge.\n\n"}, "hash": "3465d067d14eb93c78c4f26f387d66b58fd86ec6b6fe2b5b4be7016cef953c53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f288f02-f821-4a60-9448-1b89db8228d5", "node_type": "1", "metadata": {"window": "For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n", "original_text": "This objective allows predictions to condition on both left and right context.\n"}, "hash": "42d91dd76cb8113016cd2e998d2a559aac9752ef365540e562eedb5a08cc49c5", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n", "start_char_idx": 50793, "end_char_idx": 51062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f288f02-f821-4a60-9448-1b89db8228d5": {"__data__": {"id_": "5f288f02-f821-4a60-9448-1b89db8228d5", "embedding": null, "metadata": {"window": "For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n", "original_text": "This objective allows predictions to condition on both left and right context.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30a68976-0a54-4426-bf39-a2931902602e", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n"}, "hash": "8c019088420dbe4800ecffe5f02cfcba51ab02b1b62ed2c149ee1ebd5978c2fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc14d6ec-6f8d-489c-952a-e6469e55cde0", "node_type": "1", "metadata": {"window": "Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n", "original_text": "In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n"}, "hash": "69bbb08896058af6e1c29d4b6d29ecf63147b00ce3b9f616eefb846fefe649d2", "class_name": "RelatedNodeInfo"}}, "text": "This objective allows predictions to condition on both left and right context.\n", "start_char_idx": 51062, "end_char_idx": 51141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc14d6ec-6f8d-489c-952a-e6469e55cde0": {"__data__": {"id_": "fc14d6ec-6f8d-489c-952a-e6469e55cde0", "embedding": null, "metadata": {"window": "Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n", "original_text": "In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f288f02-f821-4a60-9448-1b89db8228d5", "node_type": "1", "metadata": {"window": "For clarity, only relevant excerpts of the source are shown.\n Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n", "original_text": "This objective allows predictions to condition on both left and right context.\n"}, "hash": "42d91dd76cb8113016cd2e998d2a559aac9752ef365540e562eedb5a08cc49c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "062c1682-bd6e-4eb1-a2c1-7c0e71563322", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n"}, "hash": "74d7e0adbae54a93a0775e8d688407eafe832d86772d09dbe3b2a72dc1a65ee5", "class_name": "RelatedNodeInfo"}}, "text": "In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n", "start_char_idx": 51141, "end_char_idx": 51253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "062c1682-bd6e-4eb1-a2c1-7c0e71563322": {"__data__": {"id_": "062c1682-bd6e-4eb1-a2c1-7c0e71563322", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc14d6ec-6f8d-489c-952a-e6469e55cde0", "node_type": "1", "metadata": {"window": "Summaries combine information from across the article and prior knowledge.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n", "original_text": "In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n"}, "hash": "69bbb08896058af6e1c29d4b6d29ecf63147b00ce3b9f616eefb846fefe649d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5699fd5b-61dc-458f-8e38-178607fa0099", "node_type": "1", "metadata": {"window": "This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n", "original_text": "The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n"}, "hash": "5683496c83362fb09a0737eb20a800d4a7ecd298b367aa7833da96e3c125aac0", "class_name": "RelatedNodeInfo"}}, "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n", "start_char_idx": 51253, "end_char_idx": 51556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5699fd5b-61dc-458f-8e38-178607fa0099": {"__data__": {"id_": "5699fd5b-61dc-458f-8e38-178607fa0099", "embedding": null, "metadata": {"window": "This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n", "original_text": "The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "062c1682-bd6e-4eb1-a2c1-7c0e71563322", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\ndicting masked tokens auto-regressively in a permuted order.\n This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n"}, "hash": "74d7e0adbae54a93a0775e8d688407eafe832d86772d09dbe3b2a72dc1a65ee5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e920d973-62a5-4562-808f-3fac72752ad8", "node_type": "1", "metadata": {"window": "In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n", "original_text": "Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n"}, "hash": "61e10d09961e809d7b570e881419aaecd6d1c064b6c4116c3427da1b849ac303", "class_name": "RelatedNodeInfo"}}, "text": "The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n", "start_char_idx": 51556, "end_char_idx": 51750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e920d973-62a5-4562-808f-3fac72752ad8": {"__data__": {"id_": "e920d973-62a5-4562-808f-3fac72752ad8", "embedding": null, "metadata": {"window": "In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n", "original_text": "Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5699fd5b-61dc-458f-8e38-178607fa0099", "node_type": "1", "metadata": {"window": "This objective allows predictions to condition on both left and right context.\n In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n", "original_text": "The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n"}, "hash": "5683496c83362fb09a0737eb20a800d4a7ecd298b367aa7833da96e3c125aac0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85e3746d-3039-415d-9bae-05db41ef764d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n", "original_text": "We show how BART can be used to improve machine translation decoders.\n\n"}, "hash": "4520c7855439e71f389c9f47824b47c79129a93186326b0191e3196c2dd1656d", "class_name": "RelatedNodeInfo"}}, "text": "Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n", "start_char_idx": 51750, "end_char_idx": 51898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85e3746d-3039-415d-9bae-05db41ef764d": {"__data__": {"id_": "85e3746d-3039-415d-9bae-05db41ef764d", "embedding": null, "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n", "original_text": "We show how BART can be used to improve machine translation decoders.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e920d973-62a5-4562-808f-3fac72752ad8", "node_type": "1", "metadata": {"window": "In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n\n BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n", "original_text": "Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n"}, "hash": "61e10d09961e809d7b570e881419aaecd6d1c064b6c4116c3427da1b849ac303", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c04fabe-a4f2-4383-88db-b725b774eb26", "node_type": "1", "metadata": {"window": "The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n", "original_text": "8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n"}, "hash": "5780de3a3883f8d1d38cf923a27779c0aac4b0bf3dae7c8e68b435047ba61087", "class_name": "RelatedNodeInfo"}}, "text": "We show how BART can be used to improve machine translation decoders.\n\n", "start_char_idx": 51898, "end_char_idx": 51969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c04fabe-a4f2-4383-88db-b725b774eb26": {"__data__": {"id_": "8c04fabe-a4f2-4383-88db-b725b774eb26", "embedding": null, "metadata": {"window": "The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n", "original_text": "8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85e3746d-3039-415d-9bae-05db41ef764d", "node_type": "1", "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > 5 Large-scale Pre-training Experiments > R2 RL > RO-EN\nSeveral papers have explored using pre-trained representations to improve machine translation.\n The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n", "original_text": "We show how BART can be used to improve machine translation decoders.\n\n"}, "hash": "4520c7855439e71f389c9f47824b47c79129a93186326b0191e3196c2dd1656d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ff1413b-5f9a-4e1a-a019-025d7a49d225", "node_type": "1", "metadata": {"window": "Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n", "original_text": "BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n"}, "hash": "d23e75707b2f78c520f83e73540808a66618bd01b6f7a7f72b1c68d18425fd69", "class_name": "RelatedNodeInfo"}}, "text": "8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n", "start_char_idx": 51969, "end_char_idx": 52080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ff1413b-5f9a-4e1a-a019-025d7a49d225": {"__data__": {"id_": "0ff1413b-5f9a-4e1a-a019-025d7a49d225", "embedding": null, "metadata": {"window": "Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n", "original_text": "BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c04fabe-a4f2-4383-88db-b725b774eb26", "node_type": "1", "metadata": {"window": "The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n", "original_text": "8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n"}, "hash": "5780de3a3883f8d1d38cf923a27779c0aac4b0bf3dae7c8e68b435047ba61087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7c72136-3670-4b55-8ce3-f6b3f0e789a3", "node_type": "1", "metadata": {"window": "We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n", "original_text": "Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n"}, "hash": "30d060e1997327db6a03d5935fe2ab3a60a27da6210bd9f218998c86221e4be0", "class_name": "RelatedNodeInfo"}}, "text": "BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n", "start_char_idx": 52080, "end_char_idx": 52232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7c72136-3670-4b55-8ce3-f6b3f0e789a3": {"__data__": {"id_": "c7c72136-3670-4b55-8ce3-f6b3f0e789a3", "embedding": null, "metadata": {"window": "We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n", "original_text": "Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ff1413b-5f9a-4e1a-a019-025d7a49d225", "node_type": "1", "metadata": {"window": "Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n", "original_text": "BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n"}, "hash": "d23e75707b2f78c520f83e73540808a66618bd01b6f7a7f72b1c68d18425fd69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6163482f-820a-418b-9661-b0cb8dc43cb9", "node_type": "1", "metadata": {"window": "8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n", "original_text": "References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n"}, "hash": "25bf4661c0386faf432a20fe2d404f232654ee64c6678ed6c8c03c2b7290f255", "class_name": "RelatedNodeInfo"}}, "text": "Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n", "start_char_idx": 52232, "end_char_idx": 52360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6163482f-820a-418b-9661-b0cb8dc43cb9": {"__data__": {"id_": "6163482f-820a-418b-9661-b0cb8dc43cb9", "embedding": null, "metadata": {"window": "8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n", "original_text": "References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7c72136-3670-4b55-8ce3-f6b3f0e789a3", "node_type": "1", "metadata": {"window": "We show how BART can be used to improve machine translation decoders.\n\n 8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n", "original_text": "Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n"}, "hash": "30d060e1997327db6a03d5935fe2ab3a60a27da6210bd9f218998c86221e4be0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1686d47-c496-4758-b9a4-871a97c9cb88", "node_type": "1", "metadata": {"window": "BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n", "original_text": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n"}, "hash": "967ea8231804556dd22f56acd033932752102f6ac3c3b8c821a4210465322aa7", "class_name": "RelatedNodeInfo"}}, "text": "References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n", "start_char_idx": 52360, "end_char_idx": 52434, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1686d47-c496-4758-b9a4-871a97c9cb88": {"__data__": {"id_": "f1686d47-c496-4758-b9a4-871a97c9cb88", "embedding": null, "metadata": {"window": "BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n", "original_text": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6163482f-820a-418b-9661-b0cb8dc43cb9", "node_type": "1", "metadata": {"window": "8 Conclusions\nintroduced BART, a pre-training approach that learns to map corrupted documents to the original.\n BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n", "original_text": "References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n"}, "hash": "25bf4661c0386faf432a20fe2d404f232654ee64c6678ed6c8c03c2b7290f255", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5462c98a-04ba-4403-a0f8-1ebec84e51e6", "node_type": "1", "metadata": {"window": "Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n", "original_text": "Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n"}, "hash": "d9c9328dd3e488ac55420f20f3086fa3d0781ca588477c7b78e7a414b335cab8", "class_name": "RelatedNodeInfo"}}, "text": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n", "start_char_idx": 52434, "end_char_idx": 52522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5462c98a-04ba-4403-a0f8-1ebec84e51e6": {"__data__": {"id_": "5462c98a-04ba-4403-a0f8-1ebec84e51e6", "embedding": null, "metadata": {"window": "Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n", "original_text": "Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1686d47-c496-4758-b9a4-871a97c9cb88", "node_type": "1", "metadata": {"window": "BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks.\n Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n", "original_text": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n"}, "hash": "967ea8231804556dd22f56acd033932752102f6ac3c3b8c821a4210465322aa7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "847e0e08-a132-4209-8307-ced74cb732de", "node_type": "1", "metadata": {"window": "References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n", "original_text": "References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n"}, "hash": "0136e6e7e7fd5cc117de71054594cf386d597d2084cc52eb7711cb14bff950d3", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n", "start_char_idx": 52522, "end_char_idx": 52601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "847e0e08-a132-4209-8307-ced74cb732de": {"__data__": {"id_": "847e0e08-a132-4209-8307-ced74cb732de", "embedding": null, "metadata": {"window": "References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n", "original_text": "References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5462c98a-04ba-4403-a0f8-1ebec84e51e6", "node_type": "1", "metadata": {"window": "Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to speci\ufb01c end tasks.\n\n References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n", "original_text": "Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n"}, "hash": "d9c9328dd3e488ac55420f20f3086fa3d0781ca588477c7b78e7a414b335cab8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72b7c5fa-0ed3-486c-8668-af259444fe20", "node_type": "1", "metadata": {"window": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n", "original_text": "The PASCAL recognising textual entailment challenge.\n"}, "hash": "e6f130841ada63c6a77542c9f0cfeebe9f01dc92711c3a503fa69c882897ed13", "class_name": "RelatedNodeInfo"}}, "text": "References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n", "start_char_idx": 52601, "end_char_idx": 52660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72b7c5fa-0ed3-486c-8668-af259444fe20": {"__data__": {"id_": "72b7c5fa-0ed3-486c-8668-af259444fe20", "embedding": null, "metadata": {"window": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n", "original_text": "The PASCAL recognising textual entailment challenge.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "847e0e08-a132-4209-8307-ced74cb732de", "node_type": "1", "metadata": {"window": "References\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicentowski (eds.).\n Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n", "original_text": "References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n"}, "hash": "0136e6e7e7fd5cc117de71054594cf386d597d2084cc52eb7711cb14bff950d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7a37038-0020-4f69-974d-7197b38efe7b", "node_type": "1", "metadata": {"window": "Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n", "original_text": "In Machine learning challenges.\n"}, "hash": "af6fd0c5bfff75a5c60002f4a4206640955d7363f376853d111190fbd973ea1c", "class_name": "RelatedNodeInfo"}}, "text": "The PASCAL recognising textual entailment challenge.\n", "start_char_idx": 52660, "end_char_idx": 52713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7a37038-0020-4f69-974d-7197b38efe7b": {"__data__": {"id_": "c7a37038-0020-4f69-974d-7197b38efe7b", "embedding": null, "metadata": {"window": "Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n", "original_text": "In Machine learning challenges.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72b7c5fa-0ed3-486c-8668-af259444fe20", "node_type": "1", "metadata": {"window": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007).\n Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n", "original_text": "The PASCAL recognising textual entailment challenge.\n"}, "hash": "e6f130841ada63c6a77542c9f0cfeebe9f01dc92711c3a503fa69c882897ed13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf64f5dc-7e41-47f6-928b-1686c0d8fc08", "node_type": "1", "metadata": {"window": "References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n", "original_text": "evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n"}, "hash": "94940a57f50f40c2e2d63972503708e3c25e75247dfec360a3cd71d413d36de6", "class_name": "RelatedNodeInfo"}}, "text": "In Machine learning challenges.\n", "start_char_idx": 52713, "end_char_idx": 52745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf64f5dc-7e41-47f6-928b-1686c0d8fc08": {"__data__": {"id_": "bf64f5dc-7e41-47f6-928b-1686c0d8fc08", "embedding": null, "metadata": {"window": "References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n", "original_text": "evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7a37038-0020-4f69-974d-7197b38efe7b", "node_type": "1", "metadata": {"window": "Association for Computational Linguistics, Prague, Czech Republic, June 2007.\n\n References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n", "original_text": "In Machine learning challenges.\n"}, "hash": "af6fd0c5bfff75a5c60002f4a4206640955d7363f376853d111190fbd973ea1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "191e9baf-9ac0-461c-89df-7610bd035032", "node_type": "1", "metadata": {"window": "The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n", "original_text": "177\u2013 190.\n"}, "hash": "b6fb2096560a088046123e6fbd2eeff7615570d9b19a571afb0b66c9b21d947d", "class_name": "RelatedNodeInfo"}}, "text": "evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n", "start_char_idx": 52745, "end_char_idx": 52849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "191e9baf-9ac0-461c-89df-7610bd035032": {"__data__": {"id_": "191e9baf-9ac0-461c-89df-7610bd035032", "embedding": null, "metadata": {"window": "The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n", "original_text": "177\u2013 190.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf64f5dc-7e41-47f6-928b-1686c0d8fc08", "node_type": "1", "metadata": {"window": "References\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n", "original_text": "evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n"}, "hash": "94940a57f50f40c2e2d63972503708e3c25e75247dfec360a3cd71d413d36de6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2e8882d-16d3-4aa3-8930-23ddb198e069", "node_type": "1", "metadata": {"window": "In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n", "original_text": "Springer, 2006.\n\n"}, "hash": "52a183d5ada86eddcb8ef47c0d015ee12d4a15057a4d0036dd306d5b4eee420b", "class_name": "RelatedNodeInfo"}}, "text": "177\u2013 190.\n", "start_char_idx": 52849, "end_char_idx": 52859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2e8882d-16d3-4aa3-8930-23ddb198e069": {"__data__": {"id_": "c2e8882d-16d3-4aa3-8930-23ddb198e069", "embedding": null, "metadata": {"window": "In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n", "original_text": "Springer, 2006.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "191e9baf-9ac0-461c-89df-7610bd035032", "node_type": "1", "metadata": {"window": "The PASCAL recognising textual entailment challenge.\n In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n", "original_text": "177\u2013 190.\n"}, "hash": "b6fb2096560a088046123e6fbd2eeff7615570d9b19a571afb0b66c9b21d947d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "459fb65c-c455-404e-b8e9-2f9e02703923", "node_type": "1", "metadata": {"window": "evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n", "original_text": "References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n"}, "hash": "d038ce9a9b7ccb71ca2c0ea0cc14d4aa43a4c46410d9d9e9403f083b0abef047", "class_name": "RelatedNodeInfo"}}, "text": "Springer, 2006.\n\n", "start_char_idx": 52859, "end_char_idx": 52876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "459fb65c-c455-404e-b8e9-2f9e02703923": {"__data__": {"id_": "459fb65c-c455-404e-b8e9-2f9e02703923", "embedding": null, "metadata": {"window": "evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n", "original_text": "References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2e8882d-16d3-4aa3-8930-23ddb198e069", "node_type": "1", "metadata": {"window": "In Machine learning challenges.\n evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n", "original_text": "Springer, 2006.\n\n"}, "hash": "52a183d5ada86eddcb8ef47c0d015ee12d4a15057a4d0036dd306d5b4eee420b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbf4ad60-a8d6-4e9d-a057-97aca6a5200a", "node_type": "1", "metadata": {"window": "177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n", "original_text": "BERT: Pre-training of deep bidirectional transformers for language understanding.\n"}, "hash": "fd7096c80e4c0c3ed662a50dfaba466b48b6c8ef78add369cd1ae0621b4777ac", "class_name": "RelatedNodeInfo"}}, "text": "References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n", "start_char_idx": 52876, "end_char_idx": 52953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbf4ad60-a8d6-4e9d-a057-97aca6a5200a": {"__data__": {"id_": "bbf4ad60-a8d6-4e9d-a057-97aca6a5200a", "embedding": null, "metadata": {"window": "177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n", "original_text": "BERT: Pre-training of deep bidirectional transformers for language understanding.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "459fb65c-c455-404e-b8e9-2f9e02703923", "node_type": "1", "metadata": {"window": "evaluating predictive uncertainty, visual object classi\ufb01cation, and recognising tectual entailment, pp.\n 177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n", "original_text": "References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n"}, "hash": "d038ce9a9b7ccb71ca2c0ea0cc14d4aa43a4c46410d9d9e9403f083b0abef047", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "177c2f40-d84c-4c93-bb4c-ae6b137c9183", "node_type": "1", "metadata": {"window": "Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n", "original_text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n"}, "hash": "82a221b29e7343d729828a43268d659a1e97eb82e873139111db5065dd559644", "class_name": "RelatedNodeInfo"}}, "text": "BERT: Pre-training of deep bidirectional transformers for language understanding.\n", "start_char_idx": 52953, "end_char_idx": 53035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "177c2f40-d84c-4c93-bb4c-ae6b137c9183": {"__data__": {"id_": "177c2f40-d84c-4c93-bb4c-ae6b137c9183", "embedding": null, "metadata": {"window": "Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n", "original_text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbf4ad60-a8d6-4e9d-a057-97aca6a5200a", "node_type": "1", "metadata": {"window": "177\u2013 190.\n Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n", "original_text": "BERT: Pre-training of deep bidirectional transformers for language understanding.\n"}, "hash": "fd7096c80e4c0c3ed662a50dfaba466b48b6c8ef78add369cd1ae0621b4777ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34502be7-bfc7-46c1-9604-50e6dd824f6c", "node_type": "1", "metadata": {"window": "References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n", "original_text": "4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n"}, "hash": "fbbf090e44aa73dbcce179159f2da282108935318ab29bf857a50e85d10ad2b7", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n", "start_char_idx": 53035, "end_char_idx": 53220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34502be7-bfc7-46c1-9604-50e6dd824f6c": {"__data__": {"id_": "34502be7-bfc7-46c1-9604-50e6dd824f6c", "embedding": null, "metadata": {"window": "References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n", "original_text": "4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "177c2f40-d84c-4c93-bb4c-ae6b137c9183", "node_type": "1", "metadata": {"window": "Springer, 2006.\n\n References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n", "original_text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n"}, "hash": "82a221b29e7343d729828a43268d659a1e97eb82e873139111db5065dd559644", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9c6b1bb-c4a8-44fa-acd8-fbaffa2f66e2", "node_type": "1", "metadata": {"window": "BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n", "original_text": "Association for Computational Linguistics.\n"}, "hash": "b8f297dc9993cb4ebf9732dedbfd2ad94df02515b1fb739705570fa40a3360d7", "class_name": "RelatedNodeInfo"}}, "text": "4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n", "start_char_idx": 53220, "end_char_idx": 53267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9c6b1bb-c4a8-44fa-acd8-fbaffa2f66e2": {"__data__": {"id_": "c9c6b1bb-c4a8-44fa-acd8-fbaffa2f66e2", "embedding": null, "metadata": {"window": "BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n", "original_text": "Association for Computational Linguistics.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34502be7-bfc7-46c1-9604-50e6dd824f6c", "node_type": "1", "metadata": {"window": "References\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n", "original_text": "4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n"}, "hash": "fbbf090e44aa73dbcce179159f2da282108935318ab29bf857a50e85d10ad2b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6cc1e19-6600-47e5-8784-657a63fb56ba", "node_type": "1", "metadata": {"window": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n", "original_text": "doi: 10.18653/ v1/N19-1423.\n"}, "hash": "d4d96d983bf0b5bfc517c1e8f4deb1bcce736ad5bfa5523eb06be22e2033907c", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational Linguistics.\n", "start_char_idx": 53267, "end_char_idx": 53310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6cc1e19-6600-47e5-8784-657a63fb56ba": {"__data__": {"id_": "d6cc1e19-6600-47e5-8784-657a63fb56ba", "embedding": null, "metadata": {"window": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n", "original_text": "doi: 10.18653/ v1/N19-1423.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9c6b1bb-c4a8-44fa-acd8-fbaffa2f66e2", "node_type": "1", "metadata": {"window": "BERT: Pre-training of deep bidirectional transformers for language understanding.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n", "original_text": "Association for Computational Linguistics.\n"}, "hash": "b8f297dc9993cb4ebf9732dedbfd2ad94df02515b1fb739705570fa40a3360d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83db6689-2c97-42cd-a930-8cd411607d48", "node_type": "1", "metadata": {"window": "4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n", "original_text": "URL https://www.aclweb.\n"}, "hash": "c84aca7a5acf6f66a15dc743286ac5d842408d329fcb79d96bf86edbb6936a83", "class_name": "RelatedNodeInfo"}}, "text": "doi: 10.18653/ v1/N19-1423.\n", "start_char_idx": 53310, "end_char_idx": 53338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83db6689-2c97-42cd-a930-8cd411607d48": {"__data__": {"id_": "83db6689-2c97-42cd-a930-8cd411607d48", "embedding": null, "metadata": {"window": "4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n", "original_text": "URL https://www.aclweb.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6cc1e19-6600-47e5-8784-657a63fb56ba", "node_type": "1", "metadata": {"window": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n 4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n", "original_text": "doi: 10.18653/ v1/N19-1423.\n"}, "hash": "d4d96d983bf0b5bfc517c1e8f4deb1bcce736ad5bfa5523eb06be22e2033907c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bf7bfaa-a348-4cfb-a5ce-25cd1640b7c3", "node_type": "1", "metadata": {"window": "Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n", "original_text": "org/anthology/N19-1423.\n\n"}, "hash": "1cdaa9a76863e8382ebd28ba19bf66f35b552a87190db94d959002e917bfb6d0", "class_name": "RelatedNodeInfo"}}, "text": "URL https://www.aclweb.\n", "start_char_idx": 53338, "end_char_idx": 53362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bf7bfaa-a348-4cfb-a5ce-25cd1640b7c3": {"__data__": {"id_": "8bf7bfaa-a348-4cfb-a5ce-25cd1640b7c3", "embedding": null, "metadata": {"window": "Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n", "original_text": "org/anthology/N19-1423.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83db6689-2c97-42cd-a930-8cd411607d48", "node_type": "1", "metadata": {"window": "4171\u2013 4186, Minneapolis, Minnesota, June 2019.\n Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n", "original_text": "URL https://www.aclweb.\n"}, "hash": "c84aca7a5acf6f66a15dc743286ac5d842408d329fcb79d96bf86edbb6936a83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9429b9e7-db88-4d6f-b65e-bb0a58b9c4d9", "node_type": "1", "metadata": {"window": "doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n", "original_text": "References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n"}, "hash": "901ca1fcda0b8e85fb0fd5e24f936a0e499fb04beb56f4026bec649f4de59e79", "class_name": "RelatedNodeInfo"}}, "text": "org/anthology/N19-1423.\n\n", "start_char_idx": 53362, "end_char_idx": 53387, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9429b9e7-db88-4d6f-b65e-bb0a58b9c4d9": {"__data__": {"id_": "9429b9e7-db88-4d6f-b65e-bb0a58b9c4d9", "embedding": null, "metadata": {"window": "doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n", "original_text": "References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bf7bfaa-a348-4cfb-a5ce-25cd1640b7c3", "node_type": "1", "metadata": {"window": "Association for Computational Linguistics.\n doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n", "original_text": "org/anthology/N19-1423.\n\n"}, "hash": "1cdaa9a76863e8382ebd28ba19bf66f35b552a87190db94d959002e917bfb6d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53ec3d4f-92a0-4bf1-99b5-857819ffb954", "node_type": "1", "metadata": {"window": "URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n", "original_text": "The second conversational intelligence challenge (convai2).\n"}, "hash": "e64a1dea14ca6932572514b67381fddb4eb5e4bf3a9967f81ce3ce53ac7059f1", "class_name": "RelatedNodeInfo"}}, "text": "References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n", "start_char_idx": 53387, "end_char_idx": 53553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53ec3d4f-92a0-4bf1-99b5-857819ffb954": {"__data__": {"id_": "53ec3d4f-92a0-4bf1-99b5-857819ffb954", "embedding": null, "metadata": {"window": "URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n", "original_text": "The second conversational intelligence challenge (convai2).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9429b9e7-db88-4d6f-b65e-bb0a58b9c4d9", "node_type": "1", "metadata": {"window": "doi: 10.18653/ v1/N19-1423.\n URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n", "original_text": "References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n"}, "hash": "901ca1fcda0b8e85fb0fd5e24f936a0e499fb04beb56f4026bec649f4de59e79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec984e97-9f20-4218-8b92-dd7613a4373c", "node_type": "1", "metadata": {"window": "org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n", "original_text": "arXiv preprint arXiv:1902.00098, 2019.\n\n"}, "hash": "d06c69f248e8d308c54b9b23e59c5c7a818c700d8cdcc5ae9ad30de34ee0b5be", "class_name": "RelatedNodeInfo"}}, "text": "The second conversational intelligence challenge (convai2).\n", "start_char_idx": 53553, "end_char_idx": 53613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec984e97-9f20-4218-8b92-dd7613a4373c": {"__data__": {"id_": "ec984e97-9f20-4218-8b92-dd7613a4373c", "embedding": null, "metadata": {"window": "org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n", "original_text": "arXiv preprint arXiv:1902.00098, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53ec3d4f-92a0-4bf1-99b5-857819ffb954", "node_type": "1", "metadata": {"window": "URL https://www.aclweb.\n org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n", "original_text": "The second conversational intelligence challenge (convai2).\n"}, "hash": "e64a1dea14ca6932572514b67381fddb4eb5e4bf3a9967f81ce3ce53ac7059f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65ada733-c0b5-438b-aed8-eb4b63c86d84", "node_type": "1", "metadata": {"window": "References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n", "original_text": "References\nWilliam B Dolan and Chris Brockett.\n"}, "hash": "47b220423ebd28ce0ce17f730fee742321071232051e34d644743dafd9fd6da3", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1902.00098, 2019.\n\n", "start_char_idx": 53613, "end_char_idx": 53653, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65ada733-c0b5-438b-aed8-eb4b63c86d84": {"__data__": {"id_": "65ada733-c0b5-438b-aed8-eb4b63c86d84", "embedding": null, "metadata": {"window": "References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n", "original_text": "References\nWilliam B Dolan and Chris Brockett.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec984e97-9f20-4218-8b92-dd7613a4373c", "node_type": "1", "metadata": {"window": "org/anthology/N19-1423.\n\n References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n", "original_text": "arXiv preprint arXiv:1902.00098, 2019.\n\n"}, "hash": "d06c69f248e8d308c54b9b23e59c5c7a818c700d8cdcc5ae9ad30de34ee0b5be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f22c501-dad9-4b0f-80d2-80abbad2a24a", "node_type": "1", "metadata": {"window": "The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n", "original_text": "Automatically constructing a corpus of sentential paraphrases.\n"}, "hash": "2021d10cbc480bb1a236d352217c046536713a58e2e4a0558bb23255bef94100", "class_name": "RelatedNodeInfo"}}, "text": "References\nWilliam B Dolan and Chris Brockett.\n", "start_char_idx": 53653, "end_char_idx": 53700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f22c501-dad9-4b0f-80d2-80abbad2a24a": {"__data__": {"id_": "2f22c501-dad9-4b0f-80d2-80abbad2a24a", "embedding": null, "metadata": {"window": "The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n", "original_text": "Automatically constructing a corpus of sentential paraphrases.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65ada733-c0b5-438b-aed8-eb4b63c86d84", "node_type": "1", "metadata": {"window": "References\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al.\n The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n", "original_text": "References\nWilliam B Dolan and Chris Brockett.\n"}, "hash": "47b220423ebd28ce0ce17f730fee742321071232051e34d644743dafd9fd6da3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6873a094-14cf-4617-a09e-61dac8f36718", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n", "original_text": "In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n"}, "hash": "e00f416f3c4a18fe57d636887c601f789fc3155a1ea999a7425f828464da42dc", "class_name": "RelatedNodeInfo"}}, "text": "Automatically constructing a corpus of sentential paraphrases.\n", "start_char_idx": 53700, "end_char_idx": 53763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6873a094-14cf-4617-a09e-61dac8f36718": {"__data__": {"id_": "6873a094-14cf-4617-a09e-61dac8f36718", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n", "original_text": "In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f22c501-dad9-4b0f-80d2-80abbad2a24a", "node_type": "1", "metadata": {"window": "The second conversational intelligence challenge (convai2).\n arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n", "original_text": "Automatically constructing a corpus of sentential paraphrases.\n"}, "hash": "2021d10cbc480bb1a236d352217c046536713a58e2e4a0558bb23255bef94100", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92425318-9c80-489a-9f8a-3a0b8cee8060", "node_type": "1", "metadata": {"window": "References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n", "original_text": "References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n"}, "hash": "5c0628374696490f6fbe8e27040858bd037bfb6b232b143b15afc26cad02a289", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n", "start_char_idx": 53763, "end_char_idx": 53832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92425318-9c80-489a-9f8a-3a0b8cee8060": {"__data__": {"id_": "92425318-9c80-489a-9f8a-3a0b8cee8060", "embedding": null, "metadata": {"window": "References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n", "original_text": "References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6873a094-14cf-4617-a09e-61dac8f36718", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1902.00098, 2019.\n\n References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n", "original_text": "In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n"}, "hash": "e00f416f3c4a18fe57d636887c601f789fc3155a1ea999a7425f828464da42dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a529a13f-9be0-4d92-8f91-f98d197b6c10", "node_type": "1", "metadata": {"window": "Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n", "original_text": "Uni\ufb01ed language model pretraining for natural language understanding and generation.\n"}, "hash": "acb89a112521fb8b3320fddb0b67cf4a06c786bc903ad9edb8fbd30e6f43aa2b", "class_name": "RelatedNodeInfo"}}, "text": "References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n", "start_char_idx": 53832, "end_char_idx": 53953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a529a13f-9be0-4d92-8f91-f98d197b6c10": {"__data__": {"id_": "a529a13f-9be0-4d92-8f91-f98d197b6c10", "embedding": null, "metadata": {"window": "Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n", "original_text": "Uni\ufb01ed language model pretraining for natural language understanding and generation.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92425318-9c80-489a-9f8a-3a0b8cee8060", "node_type": "1", "metadata": {"window": "References\nWilliam B Dolan and Chris Brockett.\n Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n", "original_text": "References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n"}, "hash": "5c0628374696490f6fbe8e27040858bd037bfb6b232b143b15afc26cad02a289", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e35420c-2831-49a2-a160-be38c0744ab4", "node_type": "1", "metadata": {"window": "In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n", "original_text": "arXiv preprint arXiv:1905.03197, 2019.\n\n"}, "hash": "b5a680eb88887e91f267f2a5eb68fa7c904a5348d2285e1f438e542cda7989be", "class_name": "RelatedNodeInfo"}}, "text": "Uni\ufb01ed language model pretraining for natural language understanding and generation.\n", "start_char_idx": 53953, "end_char_idx": 54038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e35420c-2831-49a2-a160-be38c0744ab4": {"__data__": {"id_": "2e35420c-2831-49a2-a160-be38c0744ab4", "embedding": null, "metadata": {"window": "In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n", "original_text": "arXiv preprint arXiv:1905.03197, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a529a13f-9be0-4d92-8f91-f98d197b6c10", "node_type": "1", "metadata": {"window": "Automatically constructing a corpus of sentential paraphrases.\n In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n", "original_text": "Uni\ufb01ed language model pretraining for natural language understanding and generation.\n"}, "hash": "acb89a112521fb8b3320fddb0b67cf4a06c786bc903ad9edb8fbd30e6f43aa2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e28aded2-d5bc-41ed-a6a7-b06b9394eec2", "node_type": "1", "metadata": {"window": "References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n", "original_text": "References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n"}, "hash": "e1cee393bdc21c87dc773b32c7751c9620743a634ac42fc291032b3ca55a7a66", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1905.03197, 2019.\n\n", "start_char_idx": 54038, "end_char_idx": 54078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e28aded2-d5bc-41ed-a6a7-b06b9394eec2": {"__data__": {"id_": "e28aded2-d5bc-41ed-a6a7-b06b9394eec2", "embedding": null, "metadata": {"window": "References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n", "original_text": "References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e35420c-2831-49a2-a160-be38c0744ab4", "node_type": "1", "metadata": {"window": "In Proceedings of the International Workshop on Paraphrasing, 2005.\n\n References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n", "original_text": "arXiv preprint arXiv:1905.03197, 2019.\n\n"}, "hash": "b5a680eb88887e91f267f2a5eb68fa7c904a5348d2285e1f438e542cda7989be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0db4fa9b-7ea0-4d85-a118-e477bb74a518", "node_type": "1", "metadata": {"window": "Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n", "original_text": "Pre-trained language model representations for language generation.\n"}, "hash": "ea7f20bebe8fc168883dbdfd9b2b95161e99907ba2c43826047b80bb31420d28", "class_name": "RelatedNodeInfo"}}, "text": "References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n", "start_char_idx": 54078, "end_char_idx": 54138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0db4fa9b-7ea0-4d85-a118-e477bb74a518": {"__data__": {"id_": "0db4fa9b-7ea0-4d85-a118-e477bb74a518", "embedding": null, "metadata": {"window": "Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n", "original_text": "Pre-trained language model representations for language generation.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e28aded2-d5bc-41ed-a6a7-b06b9394eec2", "node_type": "1", "metadata": {"window": "References\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\n Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n", "original_text": "References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n"}, "hash": "e1cee393bdc21c87dc773b32c7751c9620743a634ac42fc291032b3ca55a7a66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2060ad82-81e9-4cca-a8d8-b7586a14ea3b", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n", "original_text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n"}, "hash": "46d11c61c57ab2b71694869a6711f4c23679e6a8e30184babdc428c8a2d20906", "class_name": "RelatedNodeInfo"}}, "text": "Pre-trained language model representations for language generation.\n", "start_char_idx": 54138, "end_char_idx": 54206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2060ad82-81e9-4cca-a8d8-b7586a14ea3b": {"__data__": {"id_": "2060ad82-81e9-4cca-a8d8-b7586a14ea3b", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n", "original_text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0db4fa9b-7ea0-4d85-a118-e477bb74a518", "node_type": "1", "metadata": {"window": "Uni\ufb01ed language model pretraining for natural language understanding and generation.\n arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n", "original_text": "Pre-trained language model representations for language generation.\n"}, "hash": "ea7f20bebe8fc168883dbdfd9b2b95161e99907ba2c43826047b80bb31420d28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5a04e3c-efdd-4f5a-bd6f-68c0925dc560", "node_type": "1", "metadata": {"window": "References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n", "original_text": "References\nAngela Fan, David Grangier, and Michael Auli.\n"}, "hash": "dfd63b9c4433a4ab99a6233c4509c13604885a9c6696e2773c9047dc0554af97", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n", "start_char_idx": 54206, "end_char_idx": 54394, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5a04e3c-efdd-4f5a-bd6f-68c0925dc560": {"__data__": {"id_": "c5a04e3c-efdd-4f5a-bd6f-68c0925dc560", "embedding": null, "metadata": {"window": "References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n", "original_text": "References\nAngela Fan, David Grangier, and Michael Auli.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2060ad82-81e9-4cca-a8d8-b7586a14ea3b", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1905.03197, 2019.\n\n References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n", "original_text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n"}, "hash": "46d11c61c57ab2b71694869a6711f4c23679e6a8e30184babdc428c8a2d20906", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a64d6e8-0861-4c23-86c3-49333b3019af", "node_type": "1", "metadata": {"window": "Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n", "original_text": "Controllable abstractive summarization.\n"}, "hash": "eb95d6384254fc7c125660b3be408d1db5867bb6ab34f54e2f52bdf6cc7eb85e", "class_name": "RelatedNodeInfo"}}, "text": "References\nAngela Fan, David Grangier, and Michael Auli.\n", "start_char_idx": 54394, "end_char_idx": 54451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a64d6e8-0861-4c23-86c3-49333b3019af": {"__data__": {"id_": "0a64d6e8-0861-4c23-86c3-49333b3019af", "embedding": null, "metadata": {"window": "Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n", "original_text": "Controllable abstractive summarization.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5a04e3c-efdd-4f5a-bd6f-68c0925dc560", "node_type": "1", "metadata": {"window": "References\nSergey Edunov, Alexei Baevski, and Michael Auli.\n Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n", "original_text": "References\nAngela Fan, David Grangier, and Michael Auli.\n"}, "hash": "dfd63b9c4433a4ab99a6233c4509c13604885a9c6696e2773c9047dc0554af97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d27e5f2-34eb-4be9-90be-8a0f5b013d59", "node_type": "1", "metadata": {"window": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n", "original_text": "arXiv preprint arXiv:1711.05217, 2017.\n\n"}, "hash": "5d95edd2cf20ab74743c33ec56c6f2cd67e59e0220aa7fad2d6a63ca8c50017f", "class_name": "RelatedNodeInfo"}}, "text": "Controllable abstractive summarization.\n", "start_char_idx": 54451, "end_char_idx": 54491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d27e5f2-34eb-4be9-90be-8a0f5b013d59": {"__data__": {"id_": "4d27e5f2-34eb-4be9-90be-8a0f5b013d59", "embedding": null, "metadata": {"window": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n", "original_text": "arXiv preprint arXiv:1711.05217, 2017.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a64d6e8-0861-4c23-86c3-49333b3019af", "node_type": "1", "metadata": {"window": "Pre-trained language model representations for language generation.\n In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n", "original_text": "Controllable abstractive summarization.\n"}, "hash": "eb95d6384254fc7c125660b3be408d1db5867bb6ab34f54e2f52bdf6cc7eb85e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d13602c7-e2fc-4d1b-9e09-cf2aa00c10f3", "node_type": "1", "metadata": {"window": "References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n", "original_text": "References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n"}, "hash": "333295e8dcfc3405d89500cdb27c2304d26a625e61aed097e27aa14fc446a4f3", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1711.05217, 2017.\n\n", "start_char_idx": 54491, "end_char_idx": 54531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d13602c7-e2fc-4d1b-9e09-cf2aa00c10f3": {"__data__": {"id_": "d13602c7-e2fc-4d1b-9e09-cf2aa00c10f3", "embedding": null, "metadata": {"window": "References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n", "original_text": "References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d27e5f2-34eb-4be9-90be-8a0f5b013d59", "node_type": "1", "metadata": {"window": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\n References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n", "original_text": "arXiv preprint arXiv:1711.05217, 2017.\n\n"}, "hash": "5d95edd2cf20ab74743c33ec56c6f2cd67e59e0220aa7fad2d6a63ca8c50017f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abf580f6-c398-46fb-8fc9-dc85d5cd5ae7", "node_type": "1", "metadata": {"window": "Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n", "original_text": "Eli5: Long form question answering.\n"}, "hash": "94ec75124265a7b0ab91b6c1e473bb212b45b8a7124e9f4dbdd6e01e8ef8773b", "class_name": "RelatedNodeInfo"}}, "text": "References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n", "start_char_idx": 54531, "end_char_idx": 54631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abf580f6-c398-46fb-8fc9-dc85d5cd5ae7": {"__data__": {"id_": "abf580f6-c398-46fb-8fc9-dc85d5cd5ae7", "embedding": null, "metadata": {"window": "Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n", "original_text": "Eli5: Long form question answering.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d13602c7-e2fc-4d1b-9e09-cf2aa00c10f3", "node_type": "1", "metadata": {"window": "References\nAngela Fan, David Grangier, and Michael Auli.\n Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n", "original_text": "References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n"}, "hash": "333295e8dcfc3405d89500cdb27c2304d26a625e61aed097e27aa14fc446a4f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57e88ba8-6b58-4f81-9a12-9924cdb71b02", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n", "original_text": "arXiv preprint arXiv:1907.09190, 2019.\n\n"}, "hash": "70b9ba57ed1af47ab6d1e77891c07955e114a2a970f8074dc5cb526c890faa61", "class_name": "RelatedNodeInfo"}}, "text": "Eli5: Long form question answering.\n", "start_char_idx": 54631, "end_char_idx": 54667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57e88ba8-6b58-4f81-9a12-9924cdb71b02": {"__data__": {"id_": "57e88ba8-6b58-4f81-9a12-9924cdb71b02", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n", "original_text": "arXiv preprint arXiv:1907.09190, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abf580f6-c398-46fb-8fc9-dc85d5cd5ae7", "node_type": "1", "metadata": {"window": "Controllable abstractive summarization.\n arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n", "original_text": "Eli5: Long form question answering.\n"}, "hash": "94ec75124265a7b0ab91b6c1e473bb212b45b8a7124e9f4dbdd6e01e8ef8773b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e9a4118-c64f-4e63-845b-9be417029609", "node_type": "1", "metadata": {"window": "References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n", "original_text": "References\nDan Hendrycks and Kevin Gimpel.\n"}, "hash": "1cbed606c1f12ec9034ced937821bc8be21f9b15091c02b66f28415c62e995de", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1907.09190, 2019.\n\n", "start_char_idx": 54667, "end_char_idx": 54707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e9a4118-c64f-4e63-845b-9be417029609": {"__data__": {"id_": "6e9a4118-c64f-4e63-845b-9be417029609", "embedding": null, "metadata": {"window": "References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n", "original_text": "References\nDan Hendrycks and Kevin Gimpel.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57e88ba8-6b58-4f81-9a12-9924cdb71b02", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1711.05217, 2017.\n\n References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n", "original_text": "arXiv preprint arXiv:1907.09190, 2019.\n\n"}, "hash": "70b9ba57ed1af47ab6d1e77891c07955e114a2a970f8074dc5cb526c890faa61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb93cf69-40da-49da-a3a5-ab55fd505349", "node_type": "1", "metadata": {"window": "Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n", "original_text": "Gaussian error linear units (gelus).\n"}, "hash": "72041f5f8f216a33056fba15ca0dd8b921657d611eeb91f4f302336f01a31900", "class_name": "RelatedNodeInfo"}}, "text": "References\nDan Hendrycks and Kevin Gimpel.\n", "start_char_idx": 54707, "end_char_idx": 54750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb93cf69-40da-49da-a3a5-ab55fd505349": {"__data__": {"id_": "fb93cf69-40da-49da-a3a5-ab55fd505349", "embedding": null, "metadata": {"window": "Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n", "original_text": "Gaussian error linear units (gelus).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e9a4118-c64f-4e63-845b-9be417029609", "node_type": "1", "metadata": {"window": "References\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\n Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n", "original_text": "References\nDan Hendrycks and Kevin Gimpel.\n"}, "hash": "1cbed606c1f12ec9034ced937821bc8be21f9b15091c02b66f28415c62e995de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03815ba2-b7d6-4a77-ac75-7a0fabd71986", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n", "original_text": "arXiv preprint arXiv:1606.08415, 2016.\n\n"}, "hash": "c773d10e56581ede197f77b54c60fd5918278bc69b8ac995661c623700f17c45", "class_name": "RelatedNodeInfo"}}, "text": "Gaussian error linear units (gelus).\n", "start_char_idx": 54750, "end_char_idx": 54787, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03815ba2-b7d6-4a77-ac75-7a0fabd71986": {"__data__": {"id_": "03815ba2-b7d6-4a77-ac75-7a0fabd71986", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n", "original_text": "arXiv preprint arXiv:1606.08415, 2016.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb93cf69-40da-49da-a3a5-ab55fd505349", "node_type": "1", "metadata": {"window": "Eli5: Long form question answering.\n arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n", "original_text": "Gaussian error linear units (gelus).\n"}, "hash": "72041f5f8f216a33056fba15ca0dd8b921657d611eeb91f4f302336f01a31900", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93a556a6-dfa3-43c2-8393-0c3f2fdfb287", "node_type": "1", "metadata": {"window": "References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n", "original_text": "References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n"}, "hash": "a3ffce74440dbcc1ac7ce73855348c24453ae690b2ecf642dc8f9cbc03a08e2c", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1606.08415, 2016.\n\n", "start_char_idx": 54787, "end_char_idx": 54827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93a556a6-dfa3-43c2-8393-0c3f2fdfb287": {"__data__": {"id_": "93a556a6-dfa3-43c2-8393-0c3f2fdfb287", "embedding": null, "metadata": {"window": "References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n", "original_text": "References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03815ba2-b7d6-4a77-ac75-7a0fabd71986", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1907.09190, 2019.\n\n References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n", "original_text": "arXiv preprint arXiv:1606.08415, 2016.\n\n"}, "hash": "c773d10e56581ede197f77b54c60fd5918278bc69b8ac995661c623700f17c45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7022c02a-b7c8-4b74-bc05-6d85cc64d970", "node_type": "1", "metadata": {"window": "Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n", "original_text": "Teaching machines to read and comprehend.\n"}, "hash": "9ec01c9b6bd3f22b5652eafecb08fd4d71986e78e873178c3c0d3494834209fd", "class_name": "RelatedNodeInfo"}}, "text": "References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n", "start_char_idx": 54827, "end_char_idx": 54957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7022c02a-b7c8-4b74-bc05-6d85cc64d970": {"__data__": {"id_": "7022c02a-b7c8-4b74-bc05-6d85cc64d970", "embedding": null, "metadata": {"window": "Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n", "original_text": "Teaching machines to read and comprehend.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93a556a6-dfa3-43c2-8393-0c3f2fdfb287", "node_type": "1", "metadata": {"window": "References\nDan Hendrycks and Kevin Gimpel.\n Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n", "original_text": "References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n"}, "hash": "a3ffce74440dbcc1ac7ce73855348c24453ae690b2ecf642dc8f9cbc03a08e2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59e202f0-43ad-491d-a6b0-341b8c44835e", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n", "original_text": "In Advances in neural information processing systems, pp.\n"}, "hash": "d1a895470170aa2a347acf4cad7a17815ab988ad020fd8c92c1e2f031d1e30ea", "class_name": "RelatedNodeInfo"}}, "text": "Teaching machines to read and comprehend.\n", "start_char_idx": 54957, "end_char_idx": 54999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59e202f0-43ad-491d-a6b0-341b8c44835e": {"__data__": {"id_": "59e202f0-43ad-491d-a6b0-341b8c44835e", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n", "original_text": "In Advances in neural information processing systems, pp.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7022c02a-b7c8-4b74-bc05-6d85cc64d970", "node_type": "1", "metadata": {"window": "Gaussian error linear units (gelus).\n arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n", "original_text": "Teaching machines to read and comprehend.\n"}, "hash": "9ec01c9b6bd3f22b5652eafecb08fd4d71986e78e873178c3c0d3494834209fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ecb7077-e3cc-4273-9d29-aa2ded06b35c", "node_type": "1", "metadata": {"window": "References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n", "original_text": "1693\u20131701, 2015.\n\n"}, "hash": "4579e589d83659d5ac232cf6ccc89b4cff94e3504a7c769912cf27f211d7d683", "class_name": "RelatedNodeInfo"}}, "text": "In Advances in neural information processing systems, pp.\n", "start_char_idx": 54999, "end_char_idx": 55057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ecb7077-e3cc-4273-9d29-aa2ded06b35c": {"__data__": {"id_": "6ecb7077-e3cc-4273-9d29-aa2ded06b35c", "embedding": null, "metadata": {"window": "References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n", "original_text": "1693\u20131701, 2015.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59e202f0-43ad-491d-a6b0-341b8c44835e", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1606.08415, 2016.\n\n References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n", "original_text": "In Advances in neural information processing systems, pp.\n"}, "hash": "d1a895470170aa2a347acf4cad7a17815ab988ad020fd8c92c1e2f031d1e30ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9de6d03-c5bf-4188-aa4d-f36b689bf778", "node_type": "1", "metadata": {"window": "Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n", "original_text": "References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n"}, "hash": "d6056c2840b0e2d924b69a6b8faedfec6c0095f216ff605a8269711699a93aa8", "class_name": "RelatedNodeInfo"}}, "text": "1693\u20131701, 2015.\n\n", "start_char_idx": 55057, "end_char_idx": 55075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9de6d03-c5bf-4188-aa4d-f36b689bf778": {"__data__": {"id_": "b9de6d03-c5bf-4188-aa4d-f36b689bf778", "embedding": null, "metadata": {"window": "Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n", "original_text": "References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ecb7077-e3cc-4273-9d29-aa2ded06b35c", "node_type": "1", "metadata": {"window": "References\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.\n Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n", "original_text": "1693\u20131701, 2015.\n\n"}, "hash": "4579e589d83659d5ac232cf6ccc89b4cff94e3504a7c769912cf27f211d7d683", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3e7f4c3-ed31-4b5b-9e1f-2718a2feaf57", "node_type": "1", "metadata": {"window": "In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n", "original_text": "Spanbert: Improving pre-training by representing and predicting spans.\n"}, "hash": "db2e0a7927297f0dbd0eef8a7304851ef04e4d4e77bcd34d05e106a90f6bf5cf", "class_name": "RelatedNodeInfo"}}, "text": "References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n", "start_char_idx": 55075, "end_char_idx": 55172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3e7f4c3-ed31-4b5b-9e1f-2718a2feaf57": {"__data__": {"id_": "f3e7f4c3-ed31-4b5b-9e1f-2718a2feaf57", "embedding": null, "metadata": {"window": "In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n", "original_text": "Spanbert: Improving pre-training by representing and predicting spans.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9de6d03-c5bf-4188-aa4d-f36b689bf778", "node_type": "1", "metadata": {"window": "Teaching machines to read and comprehend.\n In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n", "original_text": "References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n"}, "hash": "d6056c2840b0e2d924b69a6b8faedfec6c0095f216ff605a8269711699a93aa8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c944e79-f082-4245-890f-1eb7527c33c3", "node_type": "1", "metadata": {"window": "1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n", "original_text": "arXiv preprint arXiv:1907.10529, 2019.\n\n"}, "hash": "dc0097b8b89dffb8ada7c2785236d227680fec8e46c510e9d2614c1da22b0ace", "class_name": "RelatedNodeInfo"}}, "text": "Spanbert: Improving pre-training by representing and predicting spans.\n", "start_char_idx": 55172, "end_char_idx": 55243, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c944e79-f082-4245-890f-1eb7527c33c3": {"__data__": {"id_": "9c944e79-f082-4245-890f-1eb7527c33c3", "embedding": null, "metadata": {"window": "1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n", "original_text": "arXiv preprint arXiv:1907.10529, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3e7f4c3-ed31-4b5b-9e1f-2718a2feaf57", "node_type": "1", "metadata": {"window": "In Advances in neural information processing systems, pp.\n 1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n", "original_text": "Spanbert: Improving pre-training by representing and predicting spans.\n"}, "hash": "db2e0a7927297f0dbd0eef8a7304851ef04e4d4e77bcd34d05e106a90f6bf5cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f758398e-1207-4eab-a692-79cb91e16ddb", "node_type": "1", "metadata": {"window": "References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n", "original_text": "References\nGuillaume Lample and Alexis Conneau.\n"}, "hash": "c2d3f5e310bcc9fe9029a9031c32da29c8ba7e5f235ca4c7e9a212cfaada86dc", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1907.10529, 2019.\n\n", "start_char_idx": 55243, "end_char_idx": 55283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f758398e-1207-4eab-a692-79cb91e16ddb": {"__data__": {"id_": "f758398e-1207-4eab-a692-79cb91e16ddb", "embedding": null, "metadata": {"window": "References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n", "original_text": "References\nGuillaume Lample and Alexis Conneau.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c944e79-f082-4245-890f-1eb7527c33c3", "node_type": "1", "metadata": {"window": "1693\u20131701, 2015.\n\n References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n", "original_text": "arXiv preprint arXiv:1907.10529, 2019.\n\n"}, "hash": "dc0097b8b89dffb8ada7c2785236d227680fec8e46c510e9d2614c1da22b0ace", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4a22827-c5ea-4b70-b245-d8d489cf8c4e", "node_type": "1", "metadata": {"window": "Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n", "original_text": "Crosslingual language model pretraining.\n"}, "hash": "0aa96469eaead54440710e6254e42d8ab21a8886bd51734171f74bcfeedc06bb", "class_name": "RelatedNodeInfo"}}, "text": "References\nGuillaume Lample and Alexis Conneau.\n", "start_char_idx": 55283, "end_char_idx": 55331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4a22827-c5ea-4b70-b245-d8d489cf8c4e": {"__data__": {"id_": "c4a22827-c5ea-4b70-b245-d8d489cf8c4e", "embedding": null, "metadata": {"window": "Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n", "original_text": "Crosslingual language model pretraining.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f758398e-1207-4eab-a692-79cb91e16ddb", "node_type": "1", "metadata": {"window": "References\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\n Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n", "original_text": "References\nGuillaume Lample and Alexis Conneau.\n"}, "hash": "c2d3f5e310bcc9fe9029a9031c32da29c8ba7e5f235ca4c7e9a212cfaada86dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0370139b-f5bb-4595-8844-24ad6c591e3b", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n", "original_text": "arXiv preprint arXiv:1901.07291, 2019.\n\n"}, "hash": "4c6ea1bf0baffb61a433cf335ae8b0c18d4e6d202cf4de783fe905bc0fa26f91", "class_name": "RelatedNodeInfo"}}, "text": "Crosslingual language model pretraining.\n", "start_char_idx": 55331, "end_char_idx": 55372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0370139b-f5bb-4595-8844-24ad6c591e3b": {"__data__": {"id_": "0370139b-f5bb-4595-8844-24ad6c591e3b", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n", "original_text": "arXiv preprint arXiv:1901.07291, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4a22827-c5ea-4b70-b245-d8d489cf8c4e", "node_type": "1", "metadata": {"window": "Spanbert: Improving pre-training by representing and predicting spans.\n arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n", "original_text": "Crosslingual language model pretraining.\n"}, "hash": "0aa96469eaead54440710e6254e42d8ab21a8886bd51734171f74bcfeedc06bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f37f03d-6041-47ee-9dac-4ee809cc9a79", "node_type": "1", "metadata": {"window": "References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n", "original_text": "References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n"}, "hash": "c710d16b3b4d17a0feb6428477433e4bd473bb59533e2e97cd383f7e89cfc739", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1901.07291, 2019.\n\n", "start_char_idx": 55372, "end_char_idx": 55412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f37f03d-6041-47ee-9dac-4ee809cc9a79": {"__data__": {"id_": "8f37f03d-6041-47ee-9dac-4ee809cc9a79", "embedding": null, "metadata": {"window": "References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n", "original_text": "References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0370139b-f5bb-4595-8844-24ad6c591e3b", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1907.10529, 2019.\n\n References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n", "original_text": "arXiv preprint arXiv:1901.07291, 2019.\n\n"}, "hash": "4c6ea1bf0baffb61a433cf335ae8b0c18d4e6d202cf4de783fe905bc0fa26f91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49291c0e-8875-40a9-bf5b-6b3669ef40d2", "node_type": "1", "metadata": {"window": "Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n", "original_text": "Albert: A lite bert for self-supervised learning of language representations.\n"}, "hash": "f947251c7df86b6f5127408448c00c42f3f61f7e67da390f6e70209d1ba607c7", "class_name": "RelatedNodeInfo"}}, "text": "References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n", "start_char_idx": 55412, "end_char_idx": 55517, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49291c0e-8875-40a9-bf5b-6b3669ef40d2": {"__data__": {"id_": "49291c0e-8875-40a9-bf5b-6b3669ef40d2", "embedding": null, "metadata": {"window": "Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n", "original_text": "Albert: A lite bert for self-supervised learning of language representations.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f37f03d-6041-47ee-9dac-4ee809cc9a79", "node_type": "1", "metadata": {"window": "References\nGuillaume Lample and Alexis Conneau.\n Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n", "original_text": "References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n"}, "hash": "c710d16b3b4d17a0feb6428477433e4bd473bb59533e2e97cd383f7e89cfc739", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96442ac1-aab5-4bcd-8103-2439404c425f", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n", "original_text": "arXiv preprint arXiv:1909.11942, 2019.\n\n"}, "hash": "1b6bf76faa4bd84426b9bd18481b3f38dbf623205708477bf0a8f40a02b9fc67", "class_name": "RelatedNodeInfo"}}, "text": "Albert: A lite bert for self-supervised learning of language representations.\n", "start_char_idx": 55517, "end_char_idx": 55595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96442ac1-aab5-4bcd-8103-2439404c425f": {"__data__": {"id_": "96442ac1-aab5-4bcd-8103-2439404c425f", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n", "original_text": "arXiv preprint arXiv:1909.11942, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49291c0e-8875-40a9-bf5b-6b3669ef40d2", "node_type": "1", "metadata": {"window": "Crosslingual language model pretraining.\n arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n", "original_text": "Albert: A lite bert for self-supervised learning of language representations.\n"}, "hash": "f947251c7df86b6f5127408448c00c42f3f61f7e67da390f6e70209d1ba607c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a18b94c9-7960-471e-94da-ec912ba378c6", "node_type": "1", "metadata": {"window": "References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n", "original_text": "References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n"}, "hash": "ae0c307aedcd65bd993559df01f3461ee6184d010872f1802a70b47ee49a3335", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1909.11942, 2019.\n\n", "start_char_idx": 55595, "end_char_idx": 55635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a18b94c9-7960-471e-94da-ec912ba378c6": {"__data__": {"id_": "a18b94c9-7960-471e-94da-ec912ba378c6", "embedding": null, "metadata": {"window": "References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n", "original_text": "References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96442ac1-aab5-4bcd-8103-2439404c425f", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1901.07291, 2019.\n\n References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n", "original_text": "arXiv preprint arXiv:1909.11942, 2019.\n\n"}, "hash": "1b6bf76faa4bd84426b9bd18481b3f38dbf623205708477bf0a8f40a02b9fc67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e600726-a887-4f38-980f-8d3994b3cab8", "node_type": "1", "metadata": {"window": "Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n", "original_text": "The Winograd schema challenge.\n"}, "hash": "8e8f2d8d94fffdcf24eb7b6d35432ce7bb86e725d271c24fd901047eecc8cb89", "class_name": "RelatedNodeInfo"}}, "text": "References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n", "start_char_idx": 55635, "end_char_idx": 55702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e600726-a887-4f38-980f-8d3994b3cab8": {"__data__": {"id_": "8e600726-a887-4f38-980f-8d3994b3cab8", "embedding": null, "metadata": {"window": "Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n", "original_text": "The Winograd schema challenge.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a18b94c9-7960-471e-94da-ec912ba378c6", "node_type": "1", "metadata": {"window": "References\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n", "original_text": "References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n"}, "hash": "ae0c307aedcd65bd993559df01f3461ee6184d010872f1802a70b47ee49a3335", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "326849ec-436d-4d08-8669-07fc2943db33", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n", "original_text": "In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n"}, "hash": "ace3a5fccd71788f649abadc70cd7f16b1fda4d91ac4e43bcf61374f2bc0df8c", "class_name": "RelatedNodeInfo"}}, "text": "The Winograd schema challenge.\n", "start_char_idx": 55702, "end_char_idx": 55733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "326849ec-436d-4d08-8669-07fc2943db33": {"__data__": {"id_": "326849ec-436d-4d08-8669-07fc2943db33", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n", "original_text": "In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e600726-a887-4f38-980f-8d3994b3cab8", "node_type": "1", "metadata": {"window": "Albert: A lite bert for self-supervised learning of language representations.\n arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n", "original_text": "The Winograd schema challenge.\n"}, "hash": "8e8f2d8d94fffdcf24eb7b6d35432ce7bb86e725d271c24fd901047eecc8cb89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c51e14d-d7e3-43a3-a045-9275372bffff", "node_type": "1", "metadata": {"window": "References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n", "original_text": "47, 2011.\n\n"}, "hash": "94f583556ace9afbc2c7db208188d2cf58853bb2ec179f825b7f91a9835eb2ee", "class_name": "RelatedNodeInfo"}}, "text": "In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n", "start_char_idx": 55733, "end_char_idx": 55823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c51e14d-d7e3-43a3-a045-9275372bffff": {"__data__": {"id_": "5c51e14d-d7e3-43a3-a045-9275372bffff", "embedding": null, "metadata": {"window": "References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n", "original_text": "47, 2011.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "326849ec-436d-4d08-8669-07fc2943db33", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1909.11942, 2019.\n\n References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n", "original_text": "In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n"}, "hash": "ace3a5fccd71788f649abadc70cd7f16b1fda4d91ac4e43bcf61374f2bc0df8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c210d2bd-b338-4521-a789-b2a0fcb0d4ad", "node_type": "1", "metadata": {"window": "The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n", "original_text": "References\nYang Liu and Mirella Lapata.\n"}, "hash": "6bd0d80040b1124b9a366cc4ddceb781150b85561c2ac229a51606d9d66c927d", "class_name": "RelatedNodeInfo"}}, "text": "47, 2011.\n\n", "start_char_idx": 55823, "end_char_idx": 55834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c210d2bd-b338-4521-a789-b2a0fcb0d4ad": {"__data__": {"id_": "c210d2bd-b338-4521-a789-b2a0fcb0d4ad", "embedding": null, "metadata": {"window": "The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n", "original_text": "References\nYang Liu and Mirella Lapata.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c51e14d-d7e3-43a3-a045-9275372bffff", "node_type": "1", "metadata": {"window": "References\nHector J Levesque, Ernest Davis, and Leora Morgenstern.\n The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n", "original_text": "47, 2011.\n\n"}, "hash": "94f583556ace9afbc2c7db208188d2cf58853bb2ec179f825b7f91a9835eb2ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a18427c8-588a-4b66-98cc-1e201bb71878", "node_type": "1", "metadata": {"window": "In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n", "original_text": "Text summarization with pretrained encoders.\n"}, "hash": "eb7e5389e4f7893d796a54a096bf51eaa41b5efda517271cfb5472ef861d940e", "class_name": "RelatedNodeInfo"}}, "text": "References\nYang Liu and Mirella Lapata.\n", "start_char_idx": 55834, "end_char_idx": 55874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a18427c8-588a-4b66-98cc-1e201bb71878": {"__data__": {"id_": "a18427c8-588a-4b66-98cc-1e201bb71878", "embedding": null, "metadata": {"window": "In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n", "original_text": "Text summarization with pretrained encoders.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c210d2bd-b338-4521-a789-b2a0fcb0d4ad", "node_type": "1", "metadata": {"window": "The Winograd schema challenge.\n In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n", "original_text": "References\nYang Liu and Mirella Lapata.\n"}, "hash": "6bd0d80040b1124b9a366cc4ddceb781150b85561c2ac229a51606d9d66c927d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5cbaf369-7254-4cbf-846d-7be657588a32", "node_type": "1", "metadata": {"window": "47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n", "original_text": "arXiv preprint arXiv:1908.08345, 2019.\n\n"}, "hash": "a948c72b32a16f452c4f624a4cc40cd383cdf49d0f8372d1f326188efbc9dfda", "class_name": "RelatedNodeInfo"}}, "text": "Text summarization with pretrained encoders.\n", "start_char_idx": 55874, "end_char_idx": 55919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5cbaf369-7254-4cbf-846d-7be657588a32": {"__data__": {"id_": "5cbaf369-7254-4cbf-846d-7be657588a32", "embedding": null, "metadata": {"window": "47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n", "original_text": "arXiv preprint arXiv:1908.08345, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a18427c8-588a-4b66-98cc-1e201bb71878", "node_type": "1", "metadata": {"window": "In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.\n 47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n", "original_text": "Text summarization with pretrained encoders.\n"}, "hash": "eb7e5389e4f7893d796a54a096bf51eaa41b5efda517271cfb5472ef861d940e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6ead0c8-734f-427f-ac69-7127b48b1a90", "node_type": "1", "metadata": {"window": "References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n", "original_text": "References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n"}, "hash": "7f8f0ea3790c7e4abd95a48fb1b7191a937e2dcadca2a932a26554183a9f5e4a", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1908.08345, 2019.\n\n", "start_char_idx": 55919, "end_char_idx": 55959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6ead0c8-734f-427f-ac69-7127b48b1a90": {"__data__": {"id_": "d6ead0c8-734f-427f-ac69-7127b48b1a90", "embedding": null, "metadata": {"window": "References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n", "original_text": "References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cbaf369-7254-4cbf-846d-7be657588a32", "node_type": "1", "metadata": {"window": "47, 2011.\n\n References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n", "original_text": "arXiv preprint arXiv:1908.08345, 2019.\n\n"}, "hash": "a948c72b32a16f452c4f624a4cc40cd383cdf49d0f8372d1f326188efbc9dfda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24c3e3e8-efc2-4f39-91f6-e829045fb8bd", "node_type": "1", "metadata": {"window": "Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n", "original_text": "Roberta: A robustly optimized bert pretraining approach.\n\n"}, "hash": "78cdc89cc73eac27d59c9798b6555842898c2dfe874448acfc3ff3b6a060f76d", "class_name": "RelatedNodeInfo"}}, "text": "References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n", "start_char_idx": 55959, "end_char_idx": 56106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24c3e3e8-efc2-4f39-91f6-e829045fb8bd": {"__data__": {"id_": "24c3e3e8-efc2-4f39-91f6-e829045fb8bd", "embedding": null, "metadata": {"window": "Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n", "original_text": "Roberta: A robustly optimized bert pretraining approach.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6ead0c8-734f-427f-ac69-7127b48b1a90", "node_type": "1", "metadata": {"window": "References\nYang Liu and Mirella Lapata.\n Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n", "original_text": "References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n"}, "hash": "7f8f0ea3790c7e4abd95a48fb1b7191a937e2dcadca2a932a26554183a9f5e4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c513cb8-2408-48b8-a891-e003715c0e8c", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n", "original_text": "References\narXiv preprint arXiv:1907.11692, 2019.\n\n"}, "hash": "8fe77e842ffd44808a7fef63e02e119b4b1cb1fbcd70af2e5d4d42ead3b3fa40", "class_name": "RelatedNodeInfo"}}, "text": "Roberta: A robustly optimized bert pretraining approach.\n\n", "start_char_idx": 56106, "end_char_idx": 56164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c513cb8-2408-48b8-a891-e003715c0e8c": {"__data__": {"id_": "8c513cb8-2408-48b8-a891-e003715c0e8c", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n", "original_text": "References\narXiv preprint arXiv:1907.11692, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24c3e3e8-efc2-4f39-91f6-e829045fb8bd", "node_type": "1", "metadata": {"window": "Text summarization with pretrained encoders.\n arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n", "original_text": "Roberta: A robustly optimized bert pretraining approach.\n\n"}, "hash": "78cdc89cc73eac27d59c9798b6555842898c2dfe874448acfc3ff3b6a060f76d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2103f2fe-67fd-48c4-b09f-d307bd588ba7", "node_type": "1", "metadata": {"window": "References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n", "original_text": "References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n"}, "hash": "6ab1f47624ef88b5272d8c918996e7f8c5f684a42ece41a51a9630cebdbbd454", "class_name": "RelatedNodeInfo"}}, "text": "References\narXiv preprint arXiv:1907.11692, 2019.\n\n", "start_char_idx": 56164, "end_char_idx": 56215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2103f2fe-67fd-48c4-b09f-d307bd588ba7": {"__data__": {"id_": "2103f2fe-67fd-48c4-b09f-d307bd588ba7", "embedding": null, "metadata": {"window": "References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n", "original_text": "References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c513cb8-2408-48b8-a891-e003715c0e8c", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1908.08345, 2019.\n\n References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n", "original_text": "References\narXiv preprint arXiv:1907.11692, 2019.\n\n"}, "hash": "8fe77e842ffd44808a7fef63e02e119b4b1cb1fbcd70af2e5d4d42ead3b3fa40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b6b8ed5-f11e-4447-af50-c84abe7d6e71", "node_type": "1", "metadata": {"window": "Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n", "original_text": "Ef\ufb01cient estimation of word representations in vector space.\n"}, "hash": "d6d141e4a071c2072a5f20bbea990e1b631e2d43e1d8ebaaad9e3fbe29b1aee8", "class_name": "RelatedNodeInfo"}}, "text": "References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n", "start_char_idx": 56215, "end_char_idx": 56283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b6b8ed5-f11e-4447-af50-c84abe7d6e71": {"__data__": {"id_": "4b6b8ed5-f11e-4447-af50-c84abe7d6e71", "embedding": null, "metadata": {"window": "Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n", "original_text": "Ef\ufb01cient estimation of word representations in vector space.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2103f2fe-67fd-48c4-b09f-d307bd588ba7", "node_type": "1", "metadata": {"window": "References\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n", "original_text": "References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n"}, "hash": "6ab1f47624ef88b5272d8c918996e7f8c5f684a42ece41a51a9630cebdbbd454", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e2164d4-4403-48f2-8cc3-ae9b1f355d0b", "node_type": "1", "metadata": {"window": "References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n", "original_text": "arXiv preprint arXiv:1301.3781, 2013.\n\n"}, "hash": "4d93a3eea5ce542dc438eb31684fce68f5651022e3b4a4c708224d6ba6f0f3c0", "class_name": "RelatedNodeInfo"}}, "text": "Ef\ufb01cient estimation of word representations in vector space.\n", "start_char_idx": 56283, "end_char_idx": 56344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e2164d4-4403-48f2-8cc3-ae9b1f355d0b": {"__data__": {"id_": "1e2164d4-4403-48f2-8cc3-ae9b1f355d0b", "embedding": null, "metadata": {"window": "References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n", "original_text": "arXiv preprint arXiv:1301.3781, 2013.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b6b8ed5-f11e-4447-af50-c84abe7d6e71", "node_type": "1", "metadata": {"window": "Roberta: A robustly optimized bert pretraining approach.\n\n References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n", "original_text": "Ef\ufb01cient estimation of word representations in vector space.\n"}, "hash": "d6d141e4a071c2072a5f20bbea990e1b631e2d43e1d8ebaaad9e3fbe29b1aee8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ac30dfb-59b8-4569-92b6-97fddd82d9c2", "node_type": "1", "metadata": {"window": "References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n", "original_text": "References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n"}, "hash": "270527d1df631da1e9a491664a22c98d7cbf374ad098ac2002d9eff0b78a5010", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1301.3781, 2013.\n\n", "start_char_idx": 56344, "end_char_idx": 56383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ac30dfb-59b8-4569-92b6-97fddd82d9c2": {"__data__": {"id_": "4ac30dfb-59b8-4569-92b6-97fddd82d9c2", "embedding": null, "metadata": {"window": "References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n", "original_text": "References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e2164d4-4403-48f2-8cc3-ae9b1f355d0b", "node_type": "1", "metadata": {"window": "References\narXiv preprint arXiv:1907.11692, 2019.\n\n References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n", "original_text": "arXiv preprint arXiv:1301.3781, 2013.\n\n"}, "hash": "4d93a3eea5ce542dc438eb31684fce68f5651022e3b4a4c708224d6ba6f0f3c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eaf3ecf2-ec1b-4017-b3bb-f6c559cd0dc3", "node_type": "1", "metadata": {"window": "Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n", "original_text": "Don\u2019t give me the details, just the summary!\n"}, "hash": "3eb6e036dbc16a964908fc138df4b60cef67fec85d9c4c07f5bbaf75edcf73e9", "class_name": "RelatedNodeInfo"}}, "text": "References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n", "start_char_idx": 56383, "end_char_idx": 56444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eaf3ecf2-ec1b-4017-b3bb-f6c559cd0dc3": {"__data__": {"id_": "eaf3ecf2-ec1b-4017-b3bb-f6c559cd0dc3", "embedding": null, "metadata": {"window": "Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n", "original_text": "Don\u2019t give me the details, just the summary!\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ac30dfb-59b8-4569-92b6-97fddd82d9c2", "node_type": "1", "metadata": {"window": "References\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n", "original_text": "References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n"}, "hash": "270527d1df631da1e9a491664a22c98d7cbf374ad098ac2002d9eff0b78a5010", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72d700b1-7006-4036-9e49-ed14f623e07b", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n", "original_text": "topicaware convolutional neural networks for extreme summarization.\n"}, "hash": "f706bc92b4f7d47f236a13628d5f510bb6d669ab45f37081dd3ef698a93c140e", "class_name": "RelatedNodeInfo"}}, "text": "Don\u2019t give me the details, just the summary!\n", "start_char_idx": 56444, "end_char_idx": 56489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72d700b1-7006-4036-9e49-ed14f623e07b": {"__data__": {"id_": "72d700b1-7006-4036-9e49-ed14f623e07b", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n", "original_text": "topicaware convolutional neural networks for extreme summarization.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eaf3ecf2-ec1b-4017-b3bb-f6c559cd0dc3", "node_type": "1", "metadata": {"window": "Ef\ufb01cient estimation of word representations in vector space.\n arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n", "original_text": "Don\u2019t give me the details, just the summary!\n"}, "hash": "3eb6e036dbc16a964908fc138df4b60cef67fec85d9c4c07f5bbaf75edcf73e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84e501dd-0d65-4647-a63b-c54d04ba6a23", "node_type": "1", "metadata": {"window": "References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n", "original_text": "arXiv preprint arXiv:1808.08745, 2018.\n\n"}, "hash": "776e24f60236785d1102d80009a2475c723c619b6f73a6fb9d2c624fde691189", "class_name": "RelatedNodeInfo"}}, "text": "topicaware convolutional neural networks for extreme summarization.\n", "start_char_idx": 56489, "end_char_idx": 56557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84e501dd-0d65-4647-a63b-c54d04ba6a23": {"__data__": {"id_": "84e501dd-0d65-4647-a63b-c54d04ba6a23", "embedding": null, "metadata": {"window": "References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n", "original_text": "arXiv preprint arXiv:1808.08745, 2018.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72d700b1-7006-4036-9e49-ed14f623e07b", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1301.3781, 2013.\n\n References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n", "original_text": "topicaware convolutional neural networks for extreme summarization.\n"}, "hash": "f706bc92b4f7d47f236a13628d5f510bb6d669ab45f37081dd3ef698a93c140e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "037c9411-0b54-46b1-a04f-b2182c60c05f", "node_type": "1", "metadata": {"window": "Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n", "original_text": "References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n"}, "hash": "157074d6a90f7aa1da8d12ca82923a28c7e0915afa8d03603d4091269b521beb", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1808.08745, 2018.\n\n", "start_char_idx": 56557, "end_char_idx": 56597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "037c9411-0b54-46b1-a04f-b2182c60c05f": {"__data__": {"id_": "037c9411-0b54-46b1-a04f-b2182c60c05f", "embedding": null, "metadata": {"window": "Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n", "original_text": "References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84e501dd-0d65-4647-a63b-c54d04ba6a23", "node_type": "1", "metadata": {"window": "References\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n", "original_text": "arXiv preprint arXiv:1808.08745, 2018.\n\n"}, "hash": "776e24f60236785d1102d80009a2475c723c619b6f73a6fb9d2c624fde691189", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a509aca-42ce-4374-96f7-dd0cd4c30cf6", "node_type": "1", "metadata": {"window": "topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n", "original_text": "Regularizing neural networks by penalizing con\ufb01dent output distributions.\n"}, "hash": "de2765d91c8aa33f715f15db6c4492a3ee80681020a813061e77590dedfd2abb", "class_name": "RelatedNodeInfo"}}, "text": "References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n", "start_char_idx": 56597, "end_char_idx": 56691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a509aca-42ce-4374-96f7-dd0cd4c30cf6": {"__data__": {"id_": "8a509aca-42ce-4374-96f7-dd0cd4c30cf6", "embedding": null, "metadata": {"window": "topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n", "original_text": "Regularizing neural networks by penalizing con\ufb01dent output distributions.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "037c9411-0b54-46b1-a04f-b2182c60c05f", "node_type": "1", "metadata": {"window": "Don\u2019t give me the details, just the summary!\n topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n", "original_text": "References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n"}, "hash": "157074d6a90f7aa1da8d12ca82923a28c7e0915afa8d03603d4091269b521beb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c4b8b07-0362-4b22-bef5-48a9ad7aa0ce", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n", "original_text": "arXiv preprint arXiv:1701.06548, 2017.\n\n"}, "hash": "f51a2732d5cf5772bfe2abc2cc4a9bdf8fa281eacee3abbcc6d5df1b253f7727", "class_name": "RelatedNodeInfo"}}, "text": "Regularizing neural networks by penalizing con\ufb01dent output distributions.\n", "start_char_idx": 56691, "end_char_idx": 56765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c4b8b07-0362-4b22-bef5-48a9ad7aa0ce": {"__data__": {"id_": "0c4b8b07-0362-4b22-bef5-48a9ad7aa0ce", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n", "original_text": "arXiv preprint arXiv:1701.06548, 2017.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a509aca-42ce-4374-96f7-dd0cd4c30cf6", "node_type": "1", "metadata": {"window": "topicaware convolutional neural networks for extreme summarization.\n arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n", "original_text": "Regularizing neural networks by penalizing con\ufb01dent output distributions.\n"}, "hash": "de2765d91c8aa33f715f15db6c4492a3ee80681020a813061e77590dedfd2abb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8fb9d63-7c1d-4c61-8758-cd4c2d05e129", "node_type": "1", "metadata": {"window": "References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n", "original_text": "References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n"}, "hash": "0c0a6906c6691e632536599276e79139f980cbd0d830d5de688e158cb1e77827", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1701.06548, 2017.\n\n", "start_char_idx": 56765, "end_char_idx": 56805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8fb9d63-7c1d-4c61-8758-cd4c2d05e129": {"__data__": {"id_": "c8fb9d63-7c1d-4c61-8758-cd4c2d05e129", "embedding": null, "metadata": {"window": "References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n", "original_text": "References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c4b8b07-0362-4b22-bef5-48a9ad7aa0ce", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1808.08745, 2018.\n\n References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n", "original_text": "arXiv preprint arXiv:1701.06548, 2017.\n\n"}, "hash": "f51a2732d5cf5772bfe2abc2cc4a9bdf8fa281eacee3abbcc6d5df1b253f7727", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a453ecb5-4b7e-49ba-8f59-161acbf1b7a5", "node_type": "1", "metadata": {"window": "Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n", "original_text": "Deep contextualized word representations.\n"}, "hash": "09d7983c7e5e47ea0085268788e7a3cac76b5eed0577814047923246655d7efd", "class_name": "RelatedNodeInfo"}}, "text": "References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n", "start_char_idx": 56805, "end_char_idx": 56928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a453ecb5-4b7e-49ba-8f59-161acbf1b7a5": {"__data__": {"id_": "a453ecb5-4b7e-49ba-8f59-161acbf1b7a5", "embedding": null, "metadata": {"window": "Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n", "original_text": "Deep contextualized word representations.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8fb9d63-7c1d-4c61-8758-cd4c2d05e129", "node_type": "1", "metadata": {"window": "References\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton.\n Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n", "original_text": "References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n"}, "hash": "0c0a6906c6691e632536599276e79139f980cbd0d830d5de688e158cb1e77827", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28f008a9-c46d-4bd6-a312-6a61302e9d10", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n", "original_text": "arXiv preprint arXiv:1802.05365, 2018.\n\n"}, "hash": "a448594f217798ce753a629a1f1f1db6fb0a63148c367d84ea71daf3f822401c", "class_name": "RelatedNodeInfo"}}, "text": "Deep contextualized word representations.\n", "start_char_idx": 56928, "end_char_idx": 56970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28f008a9-c46d-4bd6-a312-6a61302e9d10": {"__data__": {"id_": "28f008a9-c46d-4bd6-a312-6a61302e9d10", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n", "original_text": "arXiv preprint arXiv:1802.05365, 2018.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a453ecb5-4b7e-49ba-8f59-161acbf1b7a5", "node_type": "1", "metadata": {"window": "Regularizing neural networks by penalizing con\ufb01dent output distributions.\n arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n", "original_text": "Deep contextualized word representations.\n"}, "hash": "09d7983c7e5e47ea0085268788e7a3cac76b5eed0577814047923246655d7efd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d2972ca-596b-4290-a032-1241ee3e961d", "node_type": "1", "metadata": {"window": "References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n", "original_text": "References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n"}, "hash": "c2424b9d70c483932704774f51e068697da182290f385509b5da143981ec08f6", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1802.05365, 2018.\n\n", "start_char_idx": 56970, "end_char_idx": 57010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d2972ca-596b-4290-a032-1241ee3e961d": {"__data__": {"id_": "5d2972ca-596b-4290-a032-1241ee3e961d", "embedding": null, "metadata": {"window": "References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n", "original_text": "References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28f008a9-c46d-4bd6-a312-6a61302e9d10", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1701.06548, 2017.\n\n References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n", "original_text": "arXiv preprint arXiv:1802.05365, 2018.\n\n"}, "hash": "a448594f217798ce753a629a1f1f1db6fb0a63148c367d84ea71daf3f822401c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "255e4044-0811-4fa6-afbb-e8f9766265fc", "node_type": "1", "metadata": {"window": "Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n", "original_text": "Improving language understanding by generative pre-training.\n"}, "hash": "12c9c33ba762fa46cc149c73e1c7f5097135fbe55a2758dadb47327b3f905a25", "class_name": "RelatedNodeInfo"}}, "text": "References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n", "start_char_idx": 57010, "end_char_idx": 57089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "255e4044-0811-4fa6-afbb-e8f9766265fc": {"__data__": {"id_": "255e4044-0811-4fa6-afbb-e8f9766265fc", "embedding": null, "metadata": {"window": "Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n", "original_text": "Improving language understanding by generative pre-training.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d2972ca-596b-4290-a032-1241ee3e961d", "node_type": "1", "metadata": {"window": "References\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n", "original_text": "References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n"}, "hash": "c2424b9d70c483932704774f51e068697da182290f385509b5da143981ec08f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c429d203-51f9-4ed0-8feb-c064dd691a26", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n", "original_text": "URL https://s3-us-west-2.\n"}, "hash": "1c3e68b256356019023146c030eb08e5d1724500af6d896f30b16a43a6c7b9a4", "class_name": "RelatedNodeInfo"}}, "text": "Improving language understanding by generative pre-training.\n", "start_char_idx": 57089, "end_char_idx": 57150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c429d203-51f9-4ed0-8feb-c064dd691a26": {"__data__": {"id_": "c429d203-51f9-4ed0-8feb-c064dd691a26", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n", "original_text": "URL https://s3-us-west-2.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "255e4044-0811-4fa6-afbb-e8f9766265fc", "node_type": "1", "metadata": {"window": "Deep contextualized word representations.\n arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n", "original_text": "Improving language understanding by generative pre-training.\n"}, "hash": "12c9c33ba762fa46cc149c73e1c7f5097135fbe55a2758dadb47327b3f905a25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6256c042-d5a2-4b2e-84ae-e5847c379ea4", "node_type": "1", "metadata": {"window": "References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n", "original_text": "amazonaws.\n"}, "hash": "12d8fd67b77417cc7d7d3192eab7ac40248d70f888263169924343d2af1a113a", "class_name": "RelatedNodeInfo"}}, "text": "URL https://s3-us-west-2.\n", "start_char_idx": 57150, "end_char_idx": 57176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6256c042-d5a2-4b2e-84ae-e5847c379ea4": {"__data__": {"id_": "6256c042-d5a2-4b2e-84ae-e5847c379ea4", "embedding": null, "metadata": {"window": "References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n", "original_text": "amazonaws.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c429d203-51f9-4ed0-8feb-c064dd691a26", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1802.05365, 2018.\n\n References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n", "original_text": "URL https://s3-us-west-2.\n"}, "hash": "1c3e68b256356019023146c030eb08e5d1724500af6d896f30b16a43a6c7b9a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5d8d974-4dae-4d0a-acda-8bd2a85fac3a", "node_type": "1", "metadata": {"window": "Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n", "original_text": "com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n"}, "hash": "084bcbe5e6c96f2f9b7b0f4ffef26de6e9e75a07ce54d7f482fbdbb1ee6e50f6", "class_name": "RelatedNodeInfo"}}, "text": "amazonaws.\n", "start_char_idx": 57176, "end_char_idx": 57187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5d8d974-4dae-4d0a-acda-8bd2a85fac3a": {"__data__": {"id_": "e5d8d974-4dae-4d0a-acda-8bd2a85fac3a", "embedding": null, "metadata": {"window": "Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n", "original_text": "com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6256c042-d5a2-4b2e-84ae-e5847c379ea4", "node_type": "1", "metadata": {"window": "References\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n", "original_text": "amazonaws.\n"}, "hash": "12d8fd67b77417cc7d7d3192eab7ac40248d70f888263169924343d2af1a113a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d10958b9-acf8-4b66-bf0d-119a1bb4c426", "node_type": "1", "metadata": {"window": "URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n", "original_text": "pdf, 2018.\n\n"}, "hash": "30a450f474d6e1ab51a083857bcf7d886e075420ebf2691f029c5f4479cc07cd", "class_name": "RelatedNodeInfo"}}, "text": "com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n", "start_char_idx": 57187, "end_char_idx": 57270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d10958b9-acf8-4b66-bf0d-119a1bb4c426": {"__data__": {"id_": "d10958b9-acf8-4b66-bf0d-119a1bb4c426", "embedding": null, "metadata": {"window": "URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n", "original_text": "pdf, 2018.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5d8d974-4dae-4d0a-acda-8bd2a85fac3a", "node_type": "1", "metadata": {"window": "Improving language understanding by generative pre-training.\n URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n", "original_text": "com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n"}, "hash": "084bcbe5e6c96f2f9b7b0f4ffef26de6e9e75a07ce54d7f482fbdbb1ee6e50f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d65e4e5f-2f8b-4008-9690-e238a8296c7a", "node_type": "1", "metadata": {"window": "amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n", "original_text": "References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n"}, "hash": "3c11fb4c099b06fa704c6857e521688f6af87963474c7af72d19e5648f280353", "class_name": "RelatedNodeInfo"}}, "text": "pdf, 2018.\n\n", "start_char_idx": 57270, "end_char_idx": 57282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d65e4e5f-2f8b-4008-9690-e238a8296c7a": {"__data__": {"id_": "d65e4e5f-2f8b-4008-9690-e238a8296c7a", "embedding": null, "metadata": {"window": "amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n", "original_text": "References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d10958b9-acf8-4b66-bf0d-119a1bb4c426", "node_type": "1", "metadata": {"window": "URL https://s3-us-west-2.\n amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n", "original_text": "pdf, 2018.\n\n"}, "hash": "30a450f474d6e1ab51a083857bcf7d886e075420ebf2691f029c5f4479cc07cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a34442b-61d6-4dae-8b01-92b5efb0094f", "node_type": "1", "metadata": {"window": "com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n", "original_text": "Language models are unsupervised multitask learners.\n"}, "hash": "fed978c3353b3de403a96bde7e62ddaa4b50d4364eafb61a5863b4d9d12b2468", "class_name": "RelatedNodeInfo"}}, "text": "References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n", "start_char_idx": 57282, "end_char_idx": 57378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a34442b-61d6-4dae-8b01-92b5efb0094f": {"__data__": {"id_": "3a34442b-61d6-4dae-8b01-92b5efb0094f", "embedding": null, "metadata": {"window": "com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n", "original_text": "Language models are unsupervised multitask learners.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d65e4e5f-2f8b-4008-9690-e238a8296c7a", "node_type": "1", "metadata": {"window": "amazonaws.\n com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n", "original_text": "References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n"}, "hash": "3c11fb4c099b06fa704c6857e521688f6af87963474c7af72d19e5648f280353", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a12a086f-8050-42a7-8181-0ac6b85729e1", "node_type": "1", "metadata": {"window": "pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n", "original_text": "OpenAI Blog, 1(8), 2019.\n\n"}, "hash": "b77f0f23ec5c280278d6edabac06a82ef6b744bd541533dff29faf5f4fae9183", "class_name": "RelatedNodeInfo"}}, "text": "Language models are unsupervised multitask learners.\n", "start_char_idx": 57378, "end_char_idx": 57431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a12a086f-8050-42a7-8181-0ac6b85729e1": {"__data__": {"id_": "a12a086f-8050-42a7-8181-0ac6b85729e1", "embedding": null, "metadata": {"window": "pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n", "original_text": "OpenAI Blog, 1(8), 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a34442b-61d6-4dae-8b01-92b5efb0094f", "node_type": "1", "metadata": {"window": "com/openaiassets/researchcovers/languageunsupervised/language understanding paper.\n pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n", "original_text": "Language models are unsupervised multitask learners.\n"}, "hash": "fed978c3353b3de403a96bde7e62ddaa4b50d4364eafb61a5863b4d9d12b2468", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0335873e-3aa0-4506-a7b7-4690a2e68bb6", "node_type": "1", "metadata": {"window": "References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n", "original_text": "References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n"}, "hash": "3970b75d13871fa7ed0861acc3ef4fe7316733b18c7a42785dbd5f2e1dadf224", "class_name": "RelatedNodeInfo"}}, "text": "OpenAI Blog, 1(8), 2019.\n\n", "start_char_idx": 57431, "end_char_idx": 57457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0335873e-3aa0-4506-a7b7-4690a2e68bb6": {"__data__": {"id_": "0335873e-3aa0-4506-a7b7-4690a2e68bb6", "embedding": null, "metadata": {"window": "References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n", "original_text": "References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a12a086f-8050-42a7-8181-0ac6b85729e1", "node_type": "1", "metadata": {"window": "pdf, 2018.\n\n References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n", "original_text": "OpenAI Blog, 1(8), 2019.\n\n"}, "hash": "b77f0f23ec5c280278d6edabac06a82ef6b744bd541533dff29faf5f4fae9183", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b5a6a95-caaa-4f8a-945c-3d34a6ac882c", "node_type": "1", "metadata": {"window": "Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n", "original_text": "Squad: 100,000+ questions for machine comprehension of text.\n"}, "hash": "466ad1640b98c3b65a61b567353bc320f993ac6538657cd2e7e39059296e70b2", "class_name": "RelatedNodeInfo"}}, "text": "References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n", "start_char_idx": 57457, "end_char_idx": 57535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b5a6a95-caaa-4f8a-945c-3d34a6ac882c": {"__data__": {"id_": "2b5a6a95-caaa-4f8a-945c-3d34a6ac882c", "embedding": null, "metadata": {"window": "Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n", "original_text": "Squad: 100,000+ questions for machine comprehension of text.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0335873e-3aa0-4506-a7b7-4690a2e68bb6", "node_type": "1", "metadata": {"window": "References\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n", "original_text": "References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n"}, "hash": "3970b75d13871fa7ed0861acc3ef4fe7316733b18c7a42785dbd5f2e1dadf224", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee0d5445-16d6-4917-b09b-7906da577015", "node_type": "1", "metadata": {"window": "OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n", "original_text": "arXiv preprint arXiv:1606.05250, 2016.\n\n"}, "hash": "ea8952a56c0e6e67033f8e55048f112038a5660a0ef0c6981cfad60a497f8db6", "class_name": "RelatedNodeInfo"}}, "text": "Squad: 100,000+ questions for machine comprehension of text.\n", "start_char_idx": 57535, "end_char_idx": 57596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee0d5445-16d6-4917-b09b-7906da577015": {"__data__": {"id_": "ee0d5445-16d6-4917-b09b-7906da577015", "embedding": null, "metadata": {"window": "OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n", "original_text": "arXiv preprint arXiv:1606.05250, 2016.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b5a6a95-caaa-4f8a-945c-3d34a6ac882c", "node_type": "1", "metadata": {"window": "Language models are unsupervised multitask learners.\n OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n", "original_text": "Squad: 100,000+ questions for machine comprehension of text.\n"}, "hash": "466ad1640b98c3b65a61b567353bc320f993ac6538657cd2e7e39059296e70b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ce658c3-b31b-431d-bf98-26cbd5d7f09c", "node_type": "1", "metadata": {"window": "References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n", "original_text": "References\nAbigail See, Peter J Liu, and Christopher D Manning.\n"}, "hash": "45a9783c0f7a91ae458dd8cfc76fa385776d59c7c3214c807cec17beae903083", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1606.05250, 2016.\n\n", "start_char_idx": 57596, "end_char_idx": 57636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ce658c3-b31b-431d-bf98-26cbd5d7f09c": {"__data__": {"id_": "0ce658c3-b31b-431d-bf98-26cbd5d7f09c", "embedding": null, "metadata": {"window": "References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n", "original_text": "References\nAbigail See, Peter J Liu, and Christopher D Manning.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee0d5445-16d6-4917-b09b-7906da577015", "node_type": "1", "metadata": {"window": "OpenAI Blog, 1(8), 2019.\n\n References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n", "original_text": "arXiv preprint arXiv:1606.05250, 2016.\n\n"}, "hash": "ea8952a56c0e6e67033f8e55048f112038a5660a0ef0c6981cfad60a497f8db6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb574db1-05b9-43aa-a2a7-1d8a87e840ec", "node_type": "1", "metadata": {"window": "Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n", "original_text": "Get to the point: Summarization with pointer-generator networks.\n"}, "hash": "f405de633b68a88f72f44e08c503158b1fce2e7ee29472b09a4f7fa1e1589d77", "class_name": "RelatedNodeInfo"}}, "text": "References\nAbigail See, Peter J Liu, and Christopher D Manning.\n", "start_char_idx": 57636, "end_char_idx": 57700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb574db1-05b9-43aa-a2a7-1d8a87e840ec": {"__data__": {"id_": "eb574db1-05b9-43aa-a2a7-1d8a87e840ec", "embedding": null, "metadata": {"window": "Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n", "original_text": "Get to the point: Summarization with pointer-generator networks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ce658c3-b31b-431d-bf98-26cbd5d7f09c", "node_type": "1", "metadata": {"window": "References\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n", "original_text": "References\nAbigail See, Peter J Liu, and Christopher D Manning.\n"}, "hash": "45a9783c0f7a91ae458dd8cfc76fa385776d59c7c3214c807cec17beae903083", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba55dfa6-9270-4263-9dc6-b2037681f58f", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n", "original_text": "arXiv preprint arXiv:1704.04368, 2017.\n\n"}, "hash": "1505c7a449e0dc3f3ecbd9dc34e8cbc641f901285edf3b5a1ad8cf4d0b86b745", "class_name": "RelatedNodeInfo"}}, "text": "Get to the point: Summarization with pointer-generator networks.\n", "start_char_idx": 57700, "end_char_idx": 57765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba55dfa6-9270-4263-9dc6-b2037681f58f": {"__data__": {"id_": "ba55dfa6-9270-4263-9dc6-b2037681f58f", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n", "original_text": "arXiv preprint arXiv:1704.04368, 2017.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb574db1-05b9-43aa-a2a7-1d8a87e840ec", "node_type": "1", "metadata": {"window": "Squad: 100,000+ questions for machine comprehension of text.\n arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n", "original_text": "Get to the point: Summarization with pointer-generator networks.\n"}, "hash": "f405de633b68a88f72f44e08c503158b1fce2e7ee29472b09a4f7fa1e1589d77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "165373b1-92d7-4268-81c5-dcf60334509e", "node_type": "1", "metadata": {"window": "References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n", "original_text": "References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n"}, "hash": "5c7af8a2879a0826605db27b574c8fe0a9aab6193a344489fc26dd609f37135e", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1704.04368, 2017.\n\n", "start_char_idx": 57765, "end_char_idx": 57805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "165373b1-92d7-4268-81c5-dcf60334509e": {"__data__": {"id_": "165373b1-92d7-4268-81c5-dcf60334509e", "embedding": null, "metadata": {"window": "References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n", "original_text": "References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba55dfa6-9270-4263-9dc6-b2037681f58f", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1606.05250, 2016.\n\n References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n", "original_text": "arXiv preprint arXiv:1704.04368, 2017.\n\n"}, "hash": "1505c7a449e0dc3f3ecbd9dc34e8cbc641f901285edf3b5a1ad8cf4d0b86b745", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85cc280c-d80b-4685-b078-18f6265d3f3a", "node_type": "1", "metadata": {"window": "Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n", "original_text": "Edinburgh neural machine translation systems for WMT 16.\n"}, "hash": "e04f84a2d7ca6a4be3250d5bd51bfaff2219a5df7856462fafe7e034c7d0a07d", "class_name": "RelatedNodeInfo"}}, "text": "References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n", "start_char_idx": 57805, "end_char_idx": 57866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85cc280c-d80b-4685-b078-18f6265d3f3a": {"__data__": {"id_": "85cc280c-d80b-4685-b078-18f6265d3f3a", "embedding": null, "metadata": {"window": "Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n", "original_text": "Edinburgh neural machine translation systems for WMT 16.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "165373b1-92d7-4268-81c5-dcf60334509e", "node_type": "1", "metadata": {"window": "References\nAbigail See, Peter J Liu, and Christopher D Manning.\n Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n", "original_text": "References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n"}, "hash": "5c7af8a2879a0826605db27b574c8fe0a9aab6193a344489fc26dd609f37135e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8db87774-86cc-42c3-8c61-f0bb11a4a244", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n", "original_text": "In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n"}, "hash": "8452fde33c0a87cf95753a12baa396ea2cdd9528d4a298a726693ce577cfe44d", "class_name": "RelatedNodeInfo"}}, "text": "Edinburgh neural machine translation systems for WMT 16.\n", "start_char_idx": 57866, "end_char_idx": 57923, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8db87774-86cc-42c3-8c61-f0bb11a4a244": {"__data__": {"id_": "8db87774-86cc-42c3-8c61-f0bb11a4a244", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n", "original_text": "In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85cc280c-d80b-4685-b078-18f6265d3f3a", "node_type": "1", "metadata": {"window": "Get to the point: Summarization with pointer-generator networks.\n arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n", "original_text": "Edinburgh neural machine translation systems for WMT 16.\n"}, "hash": "e04f84a2d7ca6a4be3250d5bd51bfaff2219a5df7856462fafe7e034c7d0a07d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35396054-dce0-4d75-9d39-69257b167109", "node_type": "1", "metadata": {"window": "References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n", "original_text": "References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n"}, "hash": "4e9c875316f172e4d95270c8ca92ee28316eed272cf8fe616b457bbcba3d0ce1", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n", "start_char_idx": 57923, "end_char_idx": 58023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35396054-dce0-4d75-9d39-69257b167109": {"__data__": {"id_": "35396054-dce0-4d75-9d39-69257b167109", "embedding": null, "metadata": {"window": "References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n", "original_text": "References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8db87774-86cc-42c3-8c61-f0bb11a4a244", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1704.04368, 2017.\n\n References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n", "original_text": "In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n"}, "hash": "8452fde33c0a87cf95753a12baa396ea2cdd9528d4a298a726693ce577cfe44d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10b2345f-1cc9-480c-94e6-da748865f8f0", "node_type": "1", "metadata": {"window": "Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n", "original_text": "Recursive deep models for semantic compositionality over a sentiment treebank.\n"}, "hash": "c4c0f6efc290869428d8dd99065960bea9efa33efe2a96dc2d0ed3a0419db074", "class_name": "RelatedNodeInfo"}}, "text": "References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n", "start_char_idx": 58023, "end_char_idx": 58146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10b2345f-1cc9-480c-94e6-da748865f8f0": {"__data__": {"id_": "10b2345f-1cc9-480c-94e6-da748865f8f0", "embedding": null, "metadata": {"window": "Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n", "original_text": "Recursive deep models for semantic compositionality over a sentiment treebank.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35396054-dce0-4d75-9d39-69257b167109", "node_type": "1", "metadata": {"window": "References\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n", "original_text": "References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n"}, "hash": "4e9c875316f172e4d95270c8ca92ee28316eed272cf8fe616b457bbcba3d0ce1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9cf1f8d-9418-4c80-865d-8ed77c43e5ad", "node_type": "1", "metadata": {"window": "In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n", "original_text": "In Proceedings of EMNLP, pp.\n"}, "hash": "2e476bff762d9d1f11e73a6ce709a32bbb194617b1b093de367a15efe78720e0", "class_name": "RelatedNodeInfo"}}, "text": "Recursive deep models for semantic compositionality over a sentiment treebank.\n", "start_char_idx": 58146, "end_char_idx": 58225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9cf1f8d-9418-4c80-865d-8ed77c43e5ad": {"__data__": {"id_": "f9cf1f8d-9418-4c80-865d-8ed77c43e5ad", "embedding": null, "metadata": {"window": "In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n", "original_text": "In Proceedings of EMNLP, pp.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10b2345f-1cc9-480c-94e6-da748865f8f0", "node_type": "1", "metadata": {"window": "Edinburgh neural machine translation systems for WMT 16.\n In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n", "original_text": "Recursive deep models for semantic compositionality over a sentiment treebank.\n"}, "hash": "c4c0f6efc290869428d8dd99065960bea9efa33efe2a96dc2d0ed3a0419db074", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98e918cb-5ed2-4b2a-bca9-c31fe9855542", "node_type": "1", "metadata": {"window": "References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n", "original_text": "1631\u20131642, 2013.\n\n"}, "hash": "4839f20b5de63b54a41ed67a0abd76fb677b26b8ec751e627c8a920ea95e9826", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of EMNLP, pp.\n", "start_char_idx": 58225, "end_char_idx": 58254, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98e918cb-5ed2-4b2a-bca9-c31fe9855542": {"__data__": {"id_": "98e918cb-5ed2-4b2a-bca9-c31fe9855542", "embedding": null, "metadata": {"window": "References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n", "original_text": "1631\u20131642, 2013.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9cf1f8d-9418-4c80-865d-8ed77c43e5ad", "node_type": "1", "metadata": {"window": "In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016.\n\n References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n", "original_text": "In Proceedings of EMNLP, pp.\n"}, "hash": "2e476bff762d9d1f11e73a6ce709a32bbb194617b1b093de367a15efe78720e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17a5f90a-3136-48b7-9e74-7bd070e29622", "node_type": "1", "metadata": {"window": "Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n", "original_text": "References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n"}, "hash": "6eb0eac9c1af992503e30d27a113ca4aede333a03aea4bcd2f7adb8ae854ec70", "class_name": "RelatedNodeInfo"}}, "text": "1631\u20131642, 2013.\n\n", "start_char_idx": 58254, "end_char_idx": 58272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17a5f90a-3136-48b7-9e74-7bd070e29622": {"__data__": {"id_": "17a5f90a-3136-48b7-9e74-7bd070e29622", "embedding": null, "metadata": {"window": "Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n", "original_text": "References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98e918cb-5ed2-4b2a-bca9-c31fe9855542", "node_type": "1", "metadata": {"window": "References\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts.\n Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n", "original_text": "1631\u20131642, 2013.\n\n"}, "hash": "4839f20b5de63b54a41ed67a0abd76fb677b26b8ec751e627c8a920ea95e9826", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25ea7b49-8d3c-45e2-8973-7a180fe8a3c5", "node_type": "1", "metadata": {"window": "In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n", "original_text": "Mass: Masked sequence to sequence pretraining for language generation.\n"}, "hash": "03bd3ce65c1e6e1f23093f155c4bdbc4ca7b4c079c90a12e7105d61a7db0ebf4", "class_name": "RelatedNodeInfo"}}, "text": "References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n", "start_char_idx": 58272, "end_char_idx": 58342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25ea7b49-8d3c-45e2-8973-7a180fe8a3c5": {"__data__": {"id_": "25ea7b49-8d3c-45e2-8973-7a180fe8a3c5", "embedding": null, "metadata": {"window": "In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n", "original_text": "Mass: Masked sequence to sequence pretraining for language generation.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17a5f90a-3136-48b7-9e74-7bd070e29622", "node_type": "1", "metadata": {"window": "Recursive deep models for semantic compositionality over a sentiment treebank.\n In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n", "original_text": "References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n"}, "hash": "6eb0eac9c1af992503e30d27a113ca4aede333a03aea4bcd2f7adb8ae854ec70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8eb67c40-95d4-435a-abb9-6e05532abed3", "node_type": "1", "metadata": {"window": "1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n", "original_text": "In International Conference on Machine Learning, 2019.\n\n"}, "hash": "9d394c7ee012a249d46c6283f9c00a8b03b14703254c5f0c59228421a8c9aa4a", "class_name": "RelatedNodeInfo"}}, "text": "Mass: Masked sequence to sequence pretraining for language generation.\n", "start_char_idx": 58342, "end_char_idx": 58413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8eb67c40-95d4-435a-abb9-6e05532abed3": {"__data__": {"id_": "8eb67c40-95d4-435a-abb9-6e05532abed3", "embedding": null, "metadata": {"window": "1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n", "original_text": "In International Conference on Machine Learning, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25ea7b49-8d3c-45e2-8973-7a180fe8a3c5", "node_type": "1", "metadata": {"window": "In Proceedings of EMNLP, pp.\n 1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n", "original_text": "Mass: Masked sequence to sequence pretraining for language generation.\n"}, "hash": "03bd3ce65c1e6e1f23093f155c4bdbc4ca7b4c079c90a12e7105d61a7db0ebf4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74aec0da-3561-4221-b8b7-06b44613523c", "node_type": "1", "metadata": {"window": "References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n", "original_text": "References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n"}, "hash": "be2cf93b4dbd46c2e641dccd6a7cdc59586f377bffef24a176fd7172f3219e44", "class_name": "RelatedNodeInfo"}}, "text": "In International Conference on Machine Learning, 2019.\n\n", "start_char_idx": 58413, "end_char_idx": 58469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74aec0da-3561-4221-b8b7-06b44613523c": {"__data__": {"id_": "74aec0da-3561-4221-b8b7-06b44613523c", "embedding": null, "metadata": {"window": "References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n", "original_text": "References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8eb67c40-95d4-435a-abb9-6e05532abed3", "node_type": "1", "metadata": {"window": "1631\u20131642, 2013.\n\n References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n", "original_text": "In International Conference on Machine Learning, 2019.\n\n"}, "hash": "9d394c7ee012a249d46c6283f9c00a8b03b14703254c5f0c59228421a8c9aa4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d482e5c2-06bc-49b1-b0a7-420870fbc5c9", "node_type": "1", "metadata": {"window": "Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n", "original_text": "Attention is all you need.\n"}, "hash": "64b5bf1d2809dee1bfa0ee7e33fbb4e6c214b803fc15eae96de7918e197d8ba3", "class_name": "RelatedNodeInfo"}}, "text": "References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n", "start_char_idx": 58469, "end_char_idx": 58605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d482e5c2-06bc-49b1-b0a7-420870fbc5c9": {"__data__": {"id_": "d482e5c2-06bc-49b1-b0a7-420870fbc5c9", "embedding": null, "metadata": {"window": "Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n", "original_text": "Attention is all you need.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74aec0da-3561-4221-b8b7-06b44613523c", "node_type": "1", "metadata": {"window": "References\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu.\n Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n", "original_text": "References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n"}, "hash": "be2cf93b4dbd46c2e641dccd6a7cdc59586f377bffef24a176fd7172f3219e44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b9500a4-104a-4df4-8af8-69dbd910919d", "node_type": "1", "metadata": {"window": "In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n", "original_text": "In Advances in neural information processing systems, pp.\n"}, "hash": "703927ffe38466d012f4dda92a117d15b2315f395007f51c5d290953577bdc61", "class_name": "RelatedNodeInfo"}}, "text": "Attention is all you need.\n", "start_char_idx": 58605, "end_char_idx": 58632, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b9500a4-104a-4df4-8af8-69dbd910919d": {"__data__": {"id_": "6b9500a4-104a-4df4-8af8-69dbd910919d", "embedding": null, "metadata": {"window": "In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n", "original_text": "In Advances in neural information processing systems, pp.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d482e5c2-06bc-49b1-b0a7-420870fbc5c9", "node_type": "1", "metadata": {"window": "Mass: Masked sequence to sequence pretraining for language generation.\n In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n", "original_text": "Attention is all you need.\n"}, "hash": "64b5bf1d2809dee1bfa0ee7e33fbb4e6c214b803fc15eae96de7918e197d8ba3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42172975-271a-4f2c-963c-bf69dc19d372", "node_type": "1", "metadata": {"window": "References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n", "original_text": "5998\u20136008, 2017.\n\n"}, "hash": "7674391cc3ba64a1e1093269b294d9a42d9dea28c880065494913055b3f9d6fa", "class_name": "RelatedNodeInfo"}}, "text": "In Advances in neural information processing systems, pp.\n", "start_char_idx": 54999, "end_char_idx": 55057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42172975-271a-4f2c-963c-bf69dc19d372": {"__data__": {"id_": "42172975-271a-4f2c-963c-bf69dc19d372", "embedding": null, "metadata": {"window": "References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n", "original_text": "5998\u20136008, 2017.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b9500a4-104a-4df4-8af8-69dbd910919d", "node_type": "1", "metadata": {"window": "In International Conference on Machine Learning, 2019.\n\n References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n", "original_text": "In Advances in neural information processing systems, pp.\n"}, "hash": "703927ffe38466d012f4dda92a117d15b2315f395007f51c5d290953577bdc61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65639533-2c06-42b7-942d-a179ca466071", "node_type": "1", "metadata": {"window": "Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n", "original_text": "References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n"}, "hash": "0e363095105e06818068c211ace88c0be8cba6d3d7bd742510d983423c7906e1", "class_name": "RelatedNodeInfo"}}, "text": "5998\u20136008, 2017.\n\n", "start_char_idx": 58690, "end_char_idx": 58708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65639533-2c06-42b7-942d-a179ca466071": {"__data__": {"id_": "65639533-2c06-42b7-942d-a179ca466071", "embedding": null, "metadata": {"window": "Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n", "original_text": "References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42172975-271a-4f2c-963c-bf69dc19d372", "node_type": "1", "metadata": {"window": "References\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n", "original_text": "5998\u20136008, 2017.\n\n"}, "hash": "7674391cc3ba64a1e1093269b294d9a42d9dea28c880065494913055b3f9d6fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf694b2a-ebc0-4104-9ceb-e9135c4e0c9e", "node_type": "1", "metadata": {"window": "In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n", "original_text": "Glue: A multi-task benchmark and analysis platform for natural language understanding.\n"}, "hash": "b5eac12afc093d4475815a6e4ebc9d9d6f65c196fed675d79ed6a3ae50538556", "class_name": "RelatedNodeInfo"}}, "text": "References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n", "start_char_idx": 58708, "end_char_idx": 58807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf694b2a-ebc0-4104-9ceb-e9135c4e0c9e": {"__data__": {"id_": "bf694b2a-ebc0-4104-9ceb-e9135c4e0c9e", "embedding": null, "metadata": {"window": "In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n", "original_text": "Glue: A multi-task benchmark and analysis platform for natural language understanding.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65639533-2c06-42b7-942d-a179ca466071", "node_type": "1", "metadata": {"window": "Attention is all you need.\n In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n", "original_text": "References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n"}, "hash": "0e363095105e06818068c211ace88c0be8cba6d3d7bd742510d983423c7906e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b687cddf-0d26-4b3a-8d22-4599af98a136", "node_type": "1", "metadata": {"window": "5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n", "original_text": "arXiv preprint arXiv:1804.07461, 2018.\n\n"}, "hash": "1f922a8912a98d46a0441340007caf174ee6b93b492df1af0b37d6a3bba5fbd5", "class_name": "RelatedNodeInfo"}}, "text": "Glue: A multi-task benchmark and analysis platform for natural language understanding.\n", "start_char_idx": 58807, "end_char_idx": 58894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b687cddf-0d26-4b3a-8d22-4599af98a136": {"__data__": {"id_": "b687cddf-0d26-4b3a-8d22-4599af98a136", "embedding": null, "metadata": {"window": "5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n", "original_text": "arXiv preprint arXiv:1804.07461, 2018.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf694b2a-ebc0-4104-9ceb-e9135c4e0c9e", "node_type": "1", "metadata": {"window": "In Advances in neural information processing systems, pp.\n 5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n", "original_text": "Glue: A multi-task benchmark and analysis platform for natural language understanding.\n"}, "hash": "b5eac12afc093d4475815a6e4ebc9d9d6f65c196fed675d79ed6a3ae50538556", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa65661f-b8f0-4bd2-9006-cacc49dbe0b9", "node_type": "1", "metadata": {"window": "References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n", "original_text": "References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n"}, "hash": "dc15b5f753b3b43823488acb8a20fdb32848edfeca47b8f652e9af9b327be30f", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1804.07461, 2018.\n\n", "start_char_idx": 58894, "end_char_idx": 58934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa65661f-b8f0-4bd2-9006-cacc49dbe0b9": {"__data__": {"id_": "fa65661f-b8f0-4bd2-9006-cacc49dbe0b9", "embedding": null, "metadata": {"window": "References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n", "original_text": "References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b687cddf-0d26-4b3a-8d22-4599af98a136", "node_type": "1", "metadata": {"window": "5998\u20136008, 2017.\n\n References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n", "original_text": "arXiv preprint arXiv:1804.07461, 2018.\n\n"}, "hash": "1f922a8912a98d46a0441340007caf174ee6b93b492df1af0b37d6a3bba5fbd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a4a58ec-1f38-48a2-b681-288ddbf8f9d5", "node_type": "1", "metadata": {"window": "Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n", "original_text": "Neural network acceptability judgments.\n\n"}, "hash": "2e750a2a5830d584f1c5b7e9aafcf97690db392bfc9cb14348c42b4ce9b15784", "class_name": "RelatedNodeInfo"}}, "text": "References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n", "start_char_idx": 58934, "end_char_idx": 58999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a4a58ec-1f38-48a2-b681-288ddbf8f9d5": {"__data__": {"id_": "1a4a58ec-1f38-48a2-b681-288ddbf8f9d5", "embedding": null, "metadata": {"window": "Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n", "original_text": "Neural network acceptability judgments.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa65661f-b8f0-4bd2-9006-cacc49dbe0b9", "node_type": "1", "metadata": {"window": "References\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n", "original_text": "References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n"}, "hash": "dc15b5f753b3b43823488acb8a20fdb32848edfeca47b8f652e9af9b327be30f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87204183-7605-4b7d-9a74-209b6c42f56a", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n", "original_text": "References\narXiv preprint 1805.12471, 2018.\n\n"}, "hash": "434c029c4fe45cfdb8a593b3856df0804ba73f425d806561f06cda1b05e55fea", "class_name": "RelatedNodeInfo"}}, "text": "Neural network acceptability judgments.\n\n", "start_char_idx": 58999, "end_char_idx": 59040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87204183-7605-4b7d-9a74-209b6c42f56a": {"__data__": {"id_": "87204183-7605-4b7d-9a74-209b6c42f56a", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n", "original_text": "References\narXiv preprint 1805.12471, 2018.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a4a58ec-1f38-48a2-b681-288ddbf8f9d5", "node_type": "1", "metadata": {"window": "Glue: A multi-task benchmark and analysis platform for natural language understanding.\n arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n", "original_text": "Neural network acceptability judgments.\n\n"}, "hash": "2e750a2a5830d584f1c5b7e9aafcf97690db392bfc9cb14348c42b4ce9b15784", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3f9cff1-848d-4a6d-aa5d-45a8c42c6460", "node_type": "1", "metadata": {"window": "References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n", "original_text": "References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n"}, "hash": "07111d5b0d63586161f0beade63a5d6d391b5c9911c33997049667b36c70ce5b", "class_name": "RelatedNodeInfo"}}, "text": "References\narXiv preprint 1805.12471, 2018.\n\n", "start_char_idx": 59040, "end_char_idx": 59085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3f9cff1-848d-4a6d-aa5d-45a8c42c6460": {"__data__": {"id_": "e3f9cff1-848d-4a6d-aa5d-45a8c42c6460", "embedding": null, "metadata": {"window": "References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n", "original_text": "References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87204183-7605-4b7d-9a74-209b6c42f56a", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1804.07461, 2018.\n\n References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n", "original_text": "References\narXiv preprint 1805.12471, 2018.\n\n"}, "hash": "434c029c4fe45cfdb8a593b3856df0804ba73f425d806561f06cda1b05e55fea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88a52315-686a-43c3-a74f-b9148b6d1636", "node_type": "1", "metadata": {"window": "Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n", "original_text": "A broad-coverage challenge corpus for sentence understanding through inference.\n"}, "hash": "e87486e637a6b666d940ece3200a675c4a38c9277f968f02fbcbc6fb4c02c615", "class_name": "RelatedNodeInfo"}}, "text": "References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n", "start_char_idx": 59085, "end_char_idx": 59148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88a52315-686a-43c3-a74f-b9148b6d1636": {"__data__": {"id_": "88a52315-686a-43c3-a74f-b9148b6d1636", "embedding": null, "metadata": {"window": "Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n", "original_text": "A broad-coverage challenge corpus for sentence understanding through inference.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3f9cff1-848d-4a6d-aa5d-45a8c42c6460", "node_type": "1", "metadata": {"window": "References\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman.\n Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n", "original_text": "References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n"}, "hash": "07111d5b0d63586161f0beade63a5d6d391b5c9911c33997049667b36c70ce5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a29b8a2-c3d9-4b87-89d8-0f6dfc7463a6", "node_type": "1", "metadata": {"window": "References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n", "original_text": "arXiv preprint arXiv:1704.05426, 2017.\n\n"}, "hash": "2b8e33760341a0ea68b7800150620919f1c0b556344605aca4c38bd6fdd68e44", "class_name": "RelatedNodeInfo"}}, "text": "A broad-coverage challenge corpus for sentence understanding through inference.\n", "start_char_idx": 59148, "end_char_idx": 59228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a29b8a2-c3d9-4b87-89d8-0f6dfc7463a6": {"__data__": {"id_": "5a29b8a2-c3d9-4b87-89d8-0f6dfc7463a6", "embedding": null, "metadata": {"window": "References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n", "original_text": "arXiv preprint arXiv:1704.05426, 2017.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88a52315-686a-43c3-a74f-b9148b6d1636", "node_type": "1", "metadata": {"window": "Neural network acceptability judgments.\n\n References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n", "original_text": "A broad-coverage challenge corpus for sentence understanding through inference.\n"}, "hash": "e87486e637a6b666d940ece3200a675c4a38c9277f968f02fbcbc6fb4c02c615", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44d34cf5-c09c-47cf-9e57-07e00fcecc58", "node_type": "1", "metadata": {"window": "References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n", "original_text": "References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n"}, "hash": "1152cb35d3a2f146adcc1d8cb7fb9dbd268adad4a9a1eb92cca5b07087390f33", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1704.05426, 2017.\n\n", "start_char_idx": 59228, "end_char_idx": 59268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44d34cf5-c09c-47cf-9e57-07e00fcecc58": {"__data__": {"id_": "44d34cf5-c09c-47cf-9e57-07e00fcecc58", "embedding": null, "metadata": {"window": "References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n", "original_text": "References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a29b8a2-c3d9-4b87-89d8-0f6dfc7463a6", "node_type": "1", "metadata": {"window": "References\narXiv preprint 1805.12471, 2018.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n", "original_text": "arXiv preprint arXiv:1704.05426, 2017.\n\n"}, "hash": "2b8e33760341a0ea68b7800150620919f1c0b556344605aca4c38bd6fdd68e44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cf7edbe-fbc2-4a75-be09-0f45826a650a", "node_type": "1", "metadata": {"window": "A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n", "original_text": "A broad-coverage challenge corpus for sentence understanding through inference.\n"}, "hash": "8f2fa1100308cf5a90834ce354aafe6398129a32449db13b8d069763eee003ec", "class_name": "RelatedNodeInfo"}}, "text": "References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n", "start_char_idx": 59268, "end_char_idx": 59332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cf7edbe-fbc2-4a75-be09-0f45826a650a": {"__data__": {"id_": "1cf7edbe-fbc2-4a75-be09-0f45826a650a", "embedding": null, "metadata": {"window": "A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n", "original_text": "A broad-coverage challenge corpus for sentence understanding through inference.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44d34cf5-c09c-47cf-9e57-07e00fcecc58", "node_type": "1", "metadata": {"window": "References\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n", "original_text": "References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n"}, "hash": "1152cb35d3a2f146adcc1d8cb7fb9dbd268adad4a9a1eb92cca5b07087390f33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4ab4967-60f2-4284-bc8b-3dc0d246bae9", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "In Proceedings of NAACL-HLT, 2018.\n\n"}, "hash": "23091acfbe716fd9a7a14a1cbb29bf3a6031769bd00ece7c89a81a9916d03ca7", "class_name": "RelatedNodeInfo"}}, "text": "A broad-coverage challenge corpus for sentence understanding through inference.\n", "start_char_idx": 59148, "end_char_idx": 59228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4ab4967-60f2-4284-bc8b-3dc0d246bae9": {"__data__": {"id_": "a4ab4967-60f2-4284-bc8b-3dc0d246bae9", "embedding": null, "metadata": {"window": "arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "In Proceedings of NAACL-HLT, 2018.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cf7edbe-fbc2-4a75-be09-0f45826a650a", "node_type": "1", "metadata": {"window": "A broad-coverage challenge corpus for sentence understanding through inference.\n arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n", "original_text": "A broad-coverage challenge corpus for sentence understanding through inference.\n"}, "hash": "8f2fa1100308cf5a90834ce354aafe6398129a32449db13b8d069763eee003ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be473101-fa7b-47ee-bbf7-16f3a5a33b2d", "node_type": "1", "metadata": {"window": "References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n"}, "hash": "b58a369194de5e61e6d3b0dc78ab44493a05ddcb94cd74787d3fc2cc7ac12ed0", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of NAACL-HLT, 2018.\n\n", "start_char_idx": 59412, "end_char_idx": 59448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be473101-fa7b-47ee-bbf7-16f3a5a33b2d": {"__data__": {"id_": "be473101-fa7b-47ee-bbf7-16f3a5a33b2d", "embedding": null, "metadata": {"window": "References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4ab4967-60f2-4284-bc8b-3dc0d246bae9", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1704.05426, 2017.\n\n References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "In Proceedings of NAACL-HLT, 2018.\n\n"}, "hash": "23091acfbe716fd9a7a14a1cbb29bf3a6031769bd00ece7c89a81a9916d03ca7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89e282ab-f157-47ac-a376-83421206924b", "node_type": "1", "metadata": {"window": "A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "Xlnet: Generalized autoregressive pretraining for language understanding.\n"}, "hash": "8f546316752cb4f21ec794d2dca89002218765cbae3b2c78b75b515046b29500", "class_name": "RelatedNodeInfo"}}, "text": "References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n", "start_char_idx": 59448, "end_char_idx": 59551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89e282ab-f157-47ac-a376-83421206924b": {"__data__": {"id_": "89e282ab-f157-47ac-a376-83421206924b", "embedding": null, "metadata": {"window": "A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "Xlnet: Generalized autoregressive pretraining for language understanding.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be473101-fa7b-47ee-bbf7-16f3a5a33b2d", "node_type": "1", "metadata": {"window": "References\nAdina Williams, Nikita Nangia, and Samuel R. Bowman.\n A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n"}, "hash": "b58a369194de5e61e6d3b0dc78ab44493a05ddcb94cd74787d3fc2cc7ac12ed0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b8d4282-4923-40e4-a74d-37303e6ee270", "node_type": "1", "metadata": {"window": "In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "arXiv preprint arXiv:1906.08237, 2019."}, "hash": "fb5f34827e98e69a23cfb439422d28a24d26001fc709c548a9441100c117c5af", "class_name": "RelatedNodeInfo"}}, "text": "Xlnet: Generalized autoregressive pretraining for language understanding.\n", "start_char_idx": 59551, "end_char_idx": 59625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b8d4282-4923-40e4-a74d-37303e6ee270": {"__data__": {"id_": "9b8d4282-4923-40e4-a74d-37303e6ee270", "embedding": null, "metadata": {"window": "In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "arXiv preprint arXiv:1906.08237, 2019."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "6dd94362-1e7c-4578-b709-dfa87d3a3075", "node_type": "4", "metadata": {}, "hash": "1598d2239e31a04903e1d928a5ff062b224d08803bda7ec4347514d855e16aa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89e282ab-f157-47ac-a376-83421206924b", "node_type": "1", "metadata": {"window": "A broad-coverage challenge corpus for sentence understanding through inference.\n In Proceedings of NAACL-HLT, 2018.\n\n References\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n Xlnet: Generalized autoregressive pretraining for language understanding.\n arXiv preprint arXiv:1906.08237, 2019.", "original_text": "Xlnet: Generalized autoregressive pretraining for language understanding.\n"}, "hash": "8f546316752cb4f21ec794d2dca89002218765cbae3b2c78b75b515046b29500", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1906.08237, 2019.", "start_char_idx": 59625, "end_char_idx": 59663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"6dd94362-1e7c-4578-b709-dfa87d3a3075": {"node_ids": ["547f99a0-9005-45f5-8d9c-1416d3ffef92", "a704551f-348d-4902-9fd4-fe0d4dbaffa2", "df2673b4-45d9-4d98-af32-7e6f50c4b1fe", "330fbd79-b681-4383-8558-f1b285544663", "176923e6-80f8-42ae-87e0-e95abd4fc6ba", "1347e842-ed25-47ae-ab26-989559fa8cd6", "5ac97b78-3252-432b-a143-1a49729894c5", "81c5b805-9a95-46c8-9620-f1232187a2b7", "6d22f1c7-5ff2-4376-ae27-1be353771545", "60c8a8df-9867-4032-ab78-d0cfe0720527", "2670f080-137c-4464-9f0a-43ee1441290f", "57555c7a-62b8-4624-89f2-b79f64a40300", "bc6519b1-a119-4d24-bcb8-eaa3ef83c1ec", "81f28ccc-9706-488d-afbb-33eef33899c5", "d5b71df2-552f-4e14-b735-350f72c9bc35", "1639d32f-7f09-46a7-9836-66830e316fbc", "388685a8-36c3-465a-bfae-c8f9009edd45", "2c8c0ba7-54f9-463c-82ee-5ff4b4c6375d", "3f8372cf-fe56-4ca9-91b8-744d80142b12", "7a47188c-fded-44cf-a828-6595064fe1c2", "529c262f-c6d1-4328-9782-e160f69a4651", "cd87d5d7-e94b-47c0-8873-0d28e7eca961", "72db6013-3836-44fe-8c7d-201fc624d34f", "a072151a-ab27-46fa-b8a3-cc20e8345dba", "401f55b0-539d-4506-a21f-3e0b83d1d304", "2a0fe853-350c-458b-b4b0-d344fa75973a", "db6ebf2f-1bd8-485f-b807-d7cfe1013727", "d5077afc-82e5-4125-9ca2-d6202cc52226", "b9bcefab-a990-49dd-b7ad-253014eee8e7", "ef6b795e-15d2-4366-b0cb-05f979ee0072", "742f0c59-f766-4b35-9088-c1b222883235", "12ef0ec8-b8b8-46c5-b5ce-57d24e1db48e", "5586a07d-3dfb-415b-ae4a-a5c7d6cd688b", "bd04c7ec-e7dd-4b77-b28d-f900980fb8af", "3e3bcdc6-9cf7-4470-9422-9aeb5e7592e4", "f65f0633-9e26-48bb-b1ea-dd02dbe3e01b", "5e41bf38-5f72-4c16-9ab9-a88497033515", "aab105bc-aa0c-4c68-b413-4328dd3c6e36", "20d1ca9b-f018-4854-baed-4b4cff004803", "988e70b2-8d3d-46cd-a228-f1ca7e9e6d56", "75278cee-8655-491b-ac50-2f87977ef4eb", "0ff0f7b0-9beb-482c-889d-6a554935fd76", "31b435c3-8b65-44fd-8028-a1def5d48cc0", "4b926bfe-6217-4111-860f-87600a16e9da", "617fc7c9-0a88-4fff-a94c-0c9dd3bd9166", "8e7f7552-472f-43e5-b71f-83b5cd97eeac", "d469d2ee-1416-4b6a-951a-e62b49418299", "9e3fc467-9f60-4880-af4b-96e583e27ce2", "f4befc74-4ea9-4ff6-a214-346c05bc8eba", "0fd81ab0-44f5-4db2-a45d-a36e0eb02a4d", "97888f81-147c-45b9-af5b-fe7d6755510a", "42609e00-26a7-47c3-adff-77188ce87923", "e836b2ca-c5e4-4382-8146-482db695aa28", "4f20d9cb-b092-4c18-b149-cf207e22a228", "371ce4fb-cf20-4b1e-9172-0139f172c6a1", "0b806241-bebb-4935-8d40-483024eb9fe7", "646bb0fe-60fb-4a56-9af1-1914f61869ef", "c7d6e4d0-65d0-4f09-9e5c-a67ac14950d8", "b72c7c39-8dc5-42ca-aae6-829ceb4de9ff", "3cc0f7c4-ffaf-4a44-9f7f-16eabeb3e703", "c2f61c67-c169-4b85-8a2f-48bf1045ac2b", "f4ad0b61-16f7-465a-8d09-88311b46b674", "ae82fa1d-2588-4b6b-b73a-56a15eba0c31", "1f16492d-cbf5-4165-b865-8edc18ff3a8e", "e0156189-1e63-4a4f-862f-39eacee0613f", "1462ffbd-ae4d-48b0-8c24-88cf5574a082", "24977c1d-216f-44a3-8be5-df0545bf8434", "fc8e5678-fcc2-4b89-8cc9-4402dd31031c", "14998b1c-e5b4-41c8-857d-6593b7052de8", "dee275f5-7df6-4765-8362-3783e4192b12", "43d84b72-3f06-412e-a3cc-9058f6f7be8e", "b75c5011-6320-4288-8ea2-784d530380a0", "dd12e284-8f3f-4408-9836-e9a1303df12a", "4a739e16-d63b-4cca-bda8-b313c74c28d1", "953ac142-30df-44c8-b09e-df0b245a59f3", "803d2b1e-80ed-4ffd-9e82-34df0eb70e6f", "127063f6-0ba9-4bbe-bcab-ceb1b726e972", "138113a3-3d83-4093-a42f-e432b51ef0b1", "b0a0b59a-76c9-4c90-bb02-4b728c1d0e88", "82dff16a-6bd9-4564-bbe4-87a576b113ac", "6bc86263-faba-4174-8d17-1da1890632f2", "6002afb9-52ac-4a0e-9750-80a7db473c11", "9e1adf3f-6e87-4ffd-9658-ad23b60aa15f", "cd7210ec-f709-40c8-b0df-83a6cabba803", "819c1497-cf2a-4690-8f16-f3dbc76048df", "91c48cf5-9875-43a2-a5b4-079ab11b97f2", "e661afe9-8e7d-45d4-af86-55591675fa82", "b0baca0f-baf3-47db-82d9-5fe2f8f13496", "5f5d2cbc-03df-475a-a591-c3710e4f9767", "81522c83-276b-4b1e-b8f6-12a56ce1ba0a", "dfcd7fb4-1f72-48cf-9375-1203dfd55d58", "99fcad15-274a-42e0-b907-3c1340ec734b", "7d345fca-28dc-4568-9795-fc5cf0bde6a1", "72e54b2a-ee06-46b8-974b-9dc82506bb1a", "11e4b990-d314-4029-9516-735137186c65", "3cf55fd4-8253-4390-9abf-9a6ec6d9d0bf", "d8e3aba4-9507-4439-a0da-0b3d5a13fa2b", "fa3256e0-1567-4535-9b9d-c2461811531a", "fff33e16-b299-48ab-85c2-f3f5b9e38e8d", "06ddb055-781e-48a3-909d-a1aba2d94922", "89eeb99d-27a9-4e88-8e74-10bf15ac7db2", "dfebabe9-75b2-42ef-93aa-9ae89ed66882", "951976aa-bb00-49b7-9f45-6dec9a94e1ea", "9e015ef8-5f98-4aeb-b029-47c8ab64faaf", "a0fea3ad-36af-4392-abd5-4177de72c488", "fefea734-8549-428e-973c-9f462f99f80c", "6d5c4044-ce5f-453b-b0fc-1ad027eb0d55", "294eef68-ffad-47bc-b64a-f2121d3e7212", "298f9310-40cb-4375-b81b-8b2c99cf0347", "c198fa0d-7522-4620-88bd-07bdfa643302", "f43c1320-da0e-4144-889f-e31614c757ac", "aca4d951-e9aa-4877-b8ca-d110beb2e64d", "e912e6a0-f5ee-480e-8f78-6d2abb5bc632", "58ae5b0a-fce6-4459-a97d-f4b642a7984a", "8649d834-25c2-40ad-9ed1-013a18467f87", "4466e77c-3386-43d1-b5c3-d0c5fffdeeea", "9d72e90f-e3ee-48fc-b0d4-6d77f9c97e46", "70f02ad8-f45c-4f06-ad3e-2d63801cd0b5", "6adbb3d9-511d-44e4-8b78-befe1f1d9628", "431ca2e5-480f-466c-976c-0d7f059cd6d8", "61721f0b-9f03-44be-88b9-d5893725ca43", "cb56fc15-cca7-4cdc-883f-9b1ecb39ccf9", "e61c2eaf-977b-44c0-acd1-2b208c4dd282", "fc6a850c-6129-4b6c-9ad0-3df8341f3346", "b62ba0f1-a715-400b-b942-d62cddf74b84", "b34a463f-8734-4f1a-8aa2-0219615c2042", "64cdf3cd-d1e7-41f5-b9ed-3b18129c79ec", "1908889c-6ace-4511-8282-3a56c4141879", "d64a585e-7c6d-4fef-a3d9-ed7bcf0689dc", "fbebd9be-4a75-4d33-8a71-890c2f2e9dcf", "1807f7ac-1e86-4f32-87bc-83ae684d4327", "95a43bf2-b620-4f64-84a0-1d03a03139e2", "f96a65d0-28bf-431b-9366-6861791d3e41", "4e24b4de-6e12-4d63-97aa-753bdc952f6f", "0c108fc0-6b3d-4d7e-b3cf-61067f12742c", "f4518347-48f0-4546-9cc4-0154db6652de", "824e5304-1f2d-4538-b1e4-676573024aca", "997ea78f-7c5b-4259-8087-d5b2a35e4c44", "d7901f16-872b-48c0-8410-870a7b801725", "05c7232d-627c-498b-9be8-81832a1cf585", "222e4b78-4982-407d-aeaf-dca16f5360a7", "9c2bbbf5-d038-4643-8927-d0339a7140d5", "b99ab72d-ee41-41d6-9042-2c6401f9ce9e", "337d05fe-4019-452b-b08e-e8af51508ade", "c9dd879c-a983-4a07-8516-7454d5257988", "e67ebded-242a-4922-9eb7-e4f587ce8aca", "9779bebf-26a6-4c5d-93c8-aeddeb1874a0", "87bfd103-7e85-451e-9561-b60d37567d57", "acc04eca-e503-49ee-879f-291846f10c3d", "2d5e0bb8-e856-483f-b62c-da69dfae1a48", "a15859d4-293d-4076-8667-6eea8653d569", "ea7ce394-8ce7-4d90-adf4-04be8005c1b2", "1ceacb7f-fd65-48cf-b669-21f3d95e36f3", "7590f3b9-f176-4232-bb89-2644ce7e1a79", "414b568b-a768-4621-83bf-bae1d1ae7a59", "b9033fbc-42bd-4157-8a82-e9b2682431fc", "04ecb3b0-044d-442e-9407-83b12b77b1bd", "0088b0ca-7379-4364-b4d6-e199e54268c6", "4820c030-ecb5-4577-99d9-f063bb7ec589", "8cde89eb-7d8c-424f-a57c-78e503197265", "fbfe8567-48e1-4545-bfd5-9bc68cb1f3cf", "fe215daa-5d7f-4529-9ff9-cf57f2de54dc", "20e6f4f8-621e-4f45-a9bd-4920b2f32628", "97649038-9eb7-430a-a618-b292110fc983", "e30baacb-fd04-42ec-ad6f-783fb65214b9", "aff23419-1adc-4fbe-82d0-2f8a7b60b137", "db090263-2d71-420a-82f0-90ab34352e11", "628243f3-2dc8-458c-a2c5-f977392fb0d4", "f00a85e5-a2a2-4b10-90e2-75fc8579c00e", "c880acad-48aa-4c59-acd6-5b168a6fe355", "21734035-2713-41b0-8028-819e718595c1", "9ab17891-63ce-440c-ab57-d516372bed56", "88f0dfc5-8089-48ee-8136-195f86136e48", "c699add8-ae04-4aef-ad01-3c84a296ed71", "11fe265f-2e0a-40af-aea6-727a0e11b372", "6739b1f4-a215-4bc8-b7b6-a283836831f5", "ed8fc279-0b74-4cf7-95d6-370b8c3d4947", "74c63ce1-903d-4ca4-8946-93693d9b20f0", "1ae66b9d-f22b-44c4-97b8-36a786893308", "2aa4fdc6-66ad-42a4-945d-03e69b64ee2b", "47b5be23-9e06-4af9-9f41-35549a10b10d", "8707c936-00f5-4c06-899c-b4f0bf6e37e7", "cc1562e9-94b7-46b4-a5af-bed32a601635", "106181b1-d011-493a-8f44-3c522e0f6001", "0db5f796-3afd-4ea1-af06-858425a9b453", "4c2bb0a0-a437-4fa0-8798-e6c70b96a812", "ac675ba0-5541-4aae-8e78-16be6378afa3", "0e50f682-48b8-4b6c-9d69-43188569c5fa", "a357e2bd-d2ff-41cc-8ee6-b94767dea07b", "a36a69a1-4a79-48ed-abad-5aa841ce7775", "27a7693a-7691-4b80-acd0-a2a75c10b84f", "628baf0b-57d6-4fbe-938a-75efa6958749", "fdec1b10-2761-4c16-be59-a0d20bcc6032", "f592164c-a8ab-435c-a478-3faad276ca8f", "9e1f96bf-f22b-4bfd-b6d9-f0aaaecbdbf0", "b982fdad-9671-43e9-be32-d523ba375968", "094fe451-2ad4-4459-b64b-9d8aaff77ab4", "c9958109-b101-4252-a517-7a95b1ac6eca", "4374f124-646e-4603-a50e-3cbd2f6ab560", "8b2d69eb-edac-4d43-89a9-15e94830542a", "2c34196f-f916-444b-9f42-869e6b7985db", "3d699d0e-c727-4b35-b367-d393b51776a2", "99666033-5b1d-4c46-94e7-1d052aaef210", "b4be2129-2f4c-4e37-a259-a89b638ed331", "1187b050-891d-4705-8bed-3260d00376b1", "8e0122d6-a04e-4b6b-8f4f-e1ce16cc2411", "9142af83-a119-4d22-9625-6ee2f3d695cd", "a2cc09fc-7357-4138-b0b2-04412c26f38c", "1ce084b9-df7f-4e67-a48b-793abbf668eb", "9b87cc37-621d-4ee3-b38f-9bc769c24bf8", "06389bd4-0980-4b2e-987c-13042428129a", "d75bbcc9-36c5-49b5-ac24-3de68abaef96", "7a568791-ba68-48e3-93c0-4f5c90f7a846", "1653500f-7229-4cb9-acab-b0f854653929", "138559ae-0287-4566-8af9-d5733a46bd06", "a7f9eab7-bfd8-49dd-9b20-62a2c0d0dc22", "88b948a0-b65a-406d-be91-70d816eb9b88", "b8dc0d2b-9396-4835-b36d-b895e0832c17", "12950db0-2646-4518-b126-e58a28f3eb2d", "85e6f023-ff97-4f52-b68a-0bb9c2fc10ed", "bf5ef6e3-8946-4ae1-a3ab-128e8775483a", "ddd1adc4-3bfd-4c11-8488-718c5c6633ae", "40ffc149-c26f-44bc-bb7f-87e1859a7ef2", "75db9b41-7ab7-410d-86bc-e431fad9045a", "c829b6a0-3957-4082-83fc-f95cdc2baff5", "f27722b8-21b8-4701-9e21-644db5d20f8e", "f62e14e1-3795-41bc-aa94-4778af734943", "f21046b5-32ba-41af-bf41-3e7ba186b0cb", "ea76a5da-62c4-4c27-a892-b96027a527aa", "42a1e880-abd9-405f-bcfc-6269c4e96cf7", "efae40fa-d98a-473d-b147-f989c42c928b", "accee453-e9f1-47ae-8737-375f66f7e5b1", "d59a0348-2bab-4583-a064-1d5a1edad1cb", "33e37030-09b0-4d8c-a256-6b5372789fad", "6d741be6-e4a0-4691-9f66-a3e2ec830329", "9a68da14-ddad-4ca3-bb15-3213f5d87e25", "bde180a5-a2a6-4265-8e5d-6464768e6d23", "c0ff4ede-5af4-4928-a5ed-86a0856ce3b8", "75784e55-82ce-4f13-b205-aad6ba353694", "e6613249-ce2a-44cc-aaf7-2f3b091dec62", "ec312de2-659f-4b0a-ae74-40b2bb52510f", "8b3a24ce-7745-4efc-884f-29b7b4d06514", "532dd7cb-6964-41f4-8a71-d609a5dc6462", "3bec0056-d0e6-4968-b8a4-888e28d931c3", "2c99b0a0-06a3-4dd8-aa03-e38afa073a24", "d03636a0-375f-4510-a332-2074242815e2", "e263d69b-6fc9-4c4d-802c-8eca6cee3de3", "23039490-0bbc-45c0-972a-d8bba6dbe2ff", "420b0c1e-f5db-4009-b628-70fc56dee777", "d41be0a5-45cb-45f3-932e-92cb40ce8f7b", "00409930-21fb-4c06-9f09-0d202f60a222", "30a68976-0a54-4426-bf39-a2931902602e", "5f288f02-f821-4a60-9448-1b89db8228d5", "fc14d6ec-6f8d-489c-952a-e6469e55cde0", "062c1682-bd6e-4eb1-a2c1-7c0e71563322", "5699fd5b-61dc-458f-8e38-178607fa0099", "e920d973-62a5-4562-808f-3fac72752ad8", "85e3746d-3039-415d-9bae-05db41ef764d", "8c04fabe-a4f2-4383-88db-b725b774eb26", "0ff1413b-5f9a-4e1a-a019-025d7a49d225", "c7c72136-3670-4b55-8ce3-f6b3f0e789a3", "6163482f-820a-418b-9661-b0cb8dc43cb9", "f1686d47-c496-4758-b9a4-871a97c9cb88", "5462c98a-04ba-4403-a0f8-1ebec84e51e6", "847e0e08-a132-4209-8307-ced74cb732de", "72b7c5fa-0ed3-486c-8668-af259444fe20", "c7a37038-0020-4f69-974d-7197b38efe7b", "bf64f5dc-7e41-47f6-928b-1686c0d8fc08", "191e9baf-9ac0-461c-89df-7610bd035032", "c2e8882d-16d3-4aa3-8930-23ddb198e069", "459fb65c-c455-404e-b8e9-2f9e02703923", "bbf4ad60-a8d6-4e9d-a057-97aca6a5200a", "177c2f40-d84c-4c93-bb4c-ae6b137c9183", "34502be7-bfc7-46c1-9604-50e6dd824f6c", "c9c6b1bb-c4a8-44fa-acd8-fbaffa2f66e2", "d6cc1e19-6600-47e5-8784-657a63fb56ba", "83db6689-2c97-42cd-a930-8cd411607d48", "8bf7bfaa-a348-4cfb-a5ce-25cd1640b7c3", "9429b9e7-db88-4d6f-b65e-bb0a58b9c4d9", "53ec3d4f-92a0-4bf1-99b5-857819ffb954", "ec984e97-9f20-4218-8b92-dd7613a4373c", "65ada733-c0b5-438b-aed8-eb4b63c86d84", "2f22c501-dad9-4b0f-80d2-80abbad2a24a", "6873a094-14cf-4617-a09e-61dac8f36718", "92425318-9c80-489a-9f8a-3a0b8cee8060", "a529a13f-9be0-4d92-8f91-f98d197b6c10", "2e35420c-2831-49a2-a160-be38c0744ab4", "e28aded2-d5bc-41ed-a6a7-b06b9394eec2", "0db4fa9b-7ea0-4d85-a118-e477bb74a518", "2060ad82-81e9-4cca-a8d8-b7586a14ea3b", "c5a04e3c-efdd-4f5a-bd6f-68c0925dc560", "0a64d6e8-0861-4c23-86c3-49333b3019af", "4d27e5f2-34eb-4be9-90be-8a0f5b013d59", "d13602c7-e2fc-4d1b-9e09-cf2aa00c10f3", "abf580f6-c398-46fb-8fc9-dc85d5cd5ae7", "57e88ba8-6b58-4f81-9a12-9924cdb71b02", "6e9a4118-c64f-4e63-845b-9be417029609", "fb93cf69-40da-49da-a3a5-ab55fd505349", "03815ba2-b7d6-4a77-ac75-7a0fabd71986", "93a556a6-dfa3-43c2-8393-0c3f2fdfb287", "7022c02a-b7c8-4b74-bc05-6d85cc64d970", "59e202f0-43ad-491d-a6b0-341b8c44835e", "6ecb7077-e3cc-4273-9d29-aa2ded06b35c", "b9de6d03-c5bf-4188-aa4d-f36b689bf778", "f3e7f4c3-ed31-4b5b-9e1f-2718a2feaf57", "9c944e79-f082-4245-890f-1eb7527c33c3", "f758398e-1207-4eab-a692-79cb91e16ddb", "c4a22827-c5ea-4b70-b245-d8d489cf8c4e", "0370139b-f5bb-4595-8844-24ad6c591e3b", "8f37f03d-6041-47ee-9dac-4ee809cc9a79", "49291c0e-8875-40a9-bf5b-6b3669ef40d2", "96442ac1-aab5-4bcd-8103-2439404c425f", "a18b94c9-7960-471e-94da-ec912ba378c6", "8e600726-a887-4f38-980f-8d3994b3cab8", "326849ec-436d-4d08-8669-07fc2943db33", "5c51e14d-d7e3-43a3-a045-9275372bffff", "c210d2bd-b338-4521-a789-b2a0fcb0d4ad", "a18427c8-588a-4b66-98cc-1e201bb71878", "5cbaf369-7254-4cbf-846d-7be657588a32", "d6ead0c8-734f-427f-ac69-7127b48b1a90", "24c3e3e8-efc2-4f39-91f6-e829045fb8bd", "8c513cb8-2408-48b8-a891-e003715c0e8c", "2103f2fe-67fd-48c4-b09f-d307bd588ba7", "4b6b8ed5-f11e-4447-af50-c84abe7d6e71", "1e2164d4-4403-48f2-8cc3-ae9b1f355d0b", "4ac30dfb-59b8-4569-92b6-97fddd82d9c2", "eaf3ecf2-ec1b-4017-b3bb-f6c559cd0dc3", "72d700b1-7006-4036-9e49-ed14f623e07b", "84e501dd-0d65-4647-a63b-c54d04ba6a23", "037c9411-0b54-46b1-a04f-b2182c60c05f", "8a509aca-42ce-4374-96f7-dd0cd4c30cf6", "0c4b8b07-0362-4b22-bef5-48a9ad7aa0ce", "c8fb9d63-7c1d-4c61-8758-cd4c2d05e129", "a453ecb5-4b7e-49ba-8f59-161acbf1b7a5", "28f008a9-c46d-4bd6-a312-6a61302e9d10", "5d2972ca-596b-4290-a032-1241ee3e961d", "255e4044-0811-4fa6-afbb-e8f9766265fc", "c429d203-51f9-4ed0-8feb-c064dd691a26", "6256c042-d5a2-4b2e-84ae-e5847c379ea4", "e5d8d974-4dae-4d0a-acda-8bd2a85fac3a", "d10958b9-acf8-4b66-bf0d-119a1bb4c426", "d65e4e5f-2f8b-4008-9690-e238a8296c7a", "3a34442b-61d6-4dae-8b01-92b5efb0094f", "a12a086f-8050-42a7-8181-0ac6b85729e1", "0335873e-3aa0-4506-a7b7-4690a2e68bb6", "2b5a6a95-caaa-4f8a-945c-3d34a6ac882c", "ee0d5445-16d6-4917-b09b-7906da577015", "0ce658c3-b31b-431d-bf98-26cbd5d7f09c", "eb574db1-05b9-43aa-a2a7-1d8a87e840ec", "ba55dfa6-9270-4263-9dc6-b2037681f58f", "165373b1-92d7-4268-81c5-dcf60334509e", "85cc280c-d80b-4685-b078-18f6265d3f3a", "8db87774-86cc-42c3-8c61-f0bb11a4a244", "35396054-dce0-4d75-9d39-69257b167109", "10b2345f-1cc9-480c-94e6-da748865f8f0", "f9cf1f8d-9418-4c80-865d-8ed77c43e5ad", "98e918cb-5ed2-4b2a-bca9-c31fe9855542", "17a5f90a-3136-48b7-9e74-7bd070e29622", "25ea7b49-8d3c-45e2-8973-7a180fe8a3c5", "8eb67c40-95d4-435a-abb9-6e05532abed3", "74aec0da-3561-4221-b8b7-06b44613523c", "d482e5c2-06bc-49b1-b0a7-420870fbc5c9", "6b9500a4-104a-4df4-8af8-69dbd910919d", "42172975-271a-4f2c-963c-bf69dc19d372", "65639533-2c06-42b7-942d-a179ca466071", "bf694b2a-ebc0-4104-9ceb-e9135c4e0c9e", "b687cddf-0d26-4b3a-8d22-4599af98a136", "fa65661f-b8f0-4bd2-9006-cacc49dbe0b9", "1a4a58ec-1f38-48a2-b681-288ddbf8f9d5", "87204183-7605-4b7d-9a74-209b6c42f56a", "e3f9cff1-848d-4a6d-aa5d-45a8c42c6460", "88a52315-686a-43c3-a74f-b9148b6d1636", "5a29b8a2-c3d9-4b87-89d8-0f6dfc7463a6", "44d34cf5-c09c-47cf-9e57-07e00fcecc58", "1cf7edbe-fbc2-4a75-be09-0f45826a650a", "a4ab4967-60f2-4284-bc8b-3dc0d246bae9", "be473101-fa7b-47ee-bbf7-16f3a5a33b2d", "89e282ab-f157-47ac-a376-83421206924b", "9b8d4282-4923-40e4-a74d-37303e6ee270"], "metadata": {"window": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n We evaluate a number of noising approaches, \ufb01nding the best performance by both randomly shuf\ufb02ing the order of the original sentences and using a novel in-\ufb01lling scheme, where spans of text are replaced with a single mask token.\n", "original_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n"}}}}