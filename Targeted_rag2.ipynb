{"cells":[{"cell_type":"markdown","id":"03ccd61b-ca55-4ff0-8481-ac07523b5029","metadata":{},"source":["# Iterating on LLM Apps with TruLens\n","\n","Our simple RAG often struggles with retrieving not enough information from the insurance manual to properly answer the question. The information needed may be just outside the chunk that is identified and retrieved by our app. Reducing the size of the chunk and adding \"sentence windows\" to our retrieval is an advanced RAG technique that can help with retrieving more targeted, complete context. Here we can try this technique, and test its success with TruLens."]},{"cell_type":"code","execution_count":1,"id":"76434568-9f5d-40a7-8048-f71ccaabd1da","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":57,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["Package protobuf is installed but has a version conflict:\n","\t(protobuf 3.20.0 (c:\\users\\28263\\anaconda3\\lib\\site-packages), Requirement.parse('protobuf>=4.23.2'))\n","\n","This package is optional for trulens_eval so this may not be a problem but if\n","you need to use the related optional features and find there are errors, you\n","will need to resolve the conflict:\n","\n","    ```bash\n","    pip install 'protobuf>=4.23.2'\n","    ```\n","\n","If you are running trulens_eval in a notebook, you may need to restart the\n","kernel after resolving the conflict. If your distribution is in a bad place\n","beyond this package, you may need to reinstall trulens_eval so that all of the\n","dependencies get installed and hopefully corrected:\n","    \n","    ```bash\n","    pip uninstall -y trulens_eval\n","    pip install trulens_eval\n","    ```\n","\n","Package watchdog is installed but has a version conflict:\n","\t(watchdog 2.1.6 (c:\\users\\28263\\anaconda3\\lib\\site-packages), Requirement.parse('watchdog>=3.0.0'))\n","\n","This package is optional for trulens_eval so this may not be a problem but if\n","you need to use the related optional features and find there are errors, you\n","will need to resolve the conflict:\n","\n","    ```bash\n","    pip install 'watchdog>=3.0.0'\n","    ```\n","\n","If you are running trulens_eval in a notebook, you may need to restart the\n","kernel after resolving the conflict. If your distribution is in a bad place\n","beyond this package, you may need to reinstall trulens_eval so that all of the\n","dependencies get installed and hopefully corrected:\n","    \n","    ```bash\n","    pip uninstall -y trulens_eval\n","    pip install trulens_eval\n","    ```\n","\n","Package scikit-learn is installed but has a version conflict:\n","\t(scikit-learn 1.2.2 (c:\\users\\28263\\anaconda3\\lib\\site-packages), Requirement.parse('scikit-learn>=1.3.1'))\n","\n","This package is optional for trulens_eval so this may not be a problem but if\n","you need to use the related optional features and find there are errors, you\n","will need to resolve the conflict:\n","\n","    ```bash\n","    pip install 'scikit-learn>=1.3.1'\n","    ```\n","\n","If you are running trulens_eval in a notebook, you may need to restart the\n","kernel after resolving the conflict. If your distribution is in a bad place\n","beyond this package, you may need to reinstall trulens_eval so that all of the\n","dependencies get installed and hopefully corrected:\n","    \n","    ```bash\n","    pip uninstall -y trulens_eval\n","    pip install trulens_eval\n","    ```\n","\n","Package nbconvert is installed but has a version conflict:\n","\t(nbconvert 6.5.4 (c:\\users\\28263\\anaconda3\\lib\\site-packages), Requirement.parse('nbconvert>=7.14.2'))\n","\n","This package is optional for trulens_eval so this may not be a problem but if\n","you need to use the related optional features and find there are errors, you\n","will need to resolve the conflict:\n","\n","    ```bash\n","    pip install 'nbconvert>=7.14.2'\n","    ```\n","\n","If you are running trulens_eval in a notebook, you may need to restart the\n","kernel after resolving the conflict. If your distribution is in a bad place\n","beyond this package, you may need to reinstall trulens_eval so that all of the\n","dependencies get installed and hopefully corrected:\n","    \n","    ```bash\n","    pip uninstall -y trulens_eval\n","    pip install trulens_eval\n","    ```\n","\n","Package docarray is installed but has a version conflict:\n","\t(docarray 0.38.0 (c:\\users\\28263\\anaconda3\\lib\\site-packages), Requirement.parse('docarray>=0.39.1'))\n","\n","This package is optional for trulens_eval so this may not be a problem but if\n","you need to use the related optional features and find there are errors, you\n","will need to resolve the conflict:\n","\n","    ```bash\n","    pip install 'docarray>=0.39.1'\n","    ```\n","\n","If you are running trulens_eval in a notebook, you may need to restart the\n","kernel after resolving the conflict. If your distribution is in a bad place\n","beyond this package, you may need to reinstall trulens_eval so that all of the\n","dependencies get installed and hopefully corrected:\n","    \n","    ```bash\n","    pip uninstall -y trulens_eval\n","    pip install trulens_eval\n","    ```\n","\n","Package ipywidgets is installed but has a version conflict:\n","\t(ipywidgets 7.6.5 (c:\\users\\28263\\anaconda3\\lib\\site-packages), Requirement.parse('ipywidgets>=8.0.6'))\n","\n","This package is optional for trulens_eval so this may not be a problem but if\n","you need to use the related optional features and find there are errors, you\n","will need to resolve the conflict:\n","\n","    ```bash\n","    pip install 'ipywidgets>=8.0.6'\n","    ```\n","\n","If you are running trulens_eval in a notebook, you may need to restart the\n","kernel after resolving the conflict. If your distribution is in a bad place\n","beyond this package, you may need to reinstall trulens_eval so that all of the\n","dependencies get installed and hopefully corrected:\n","    \n","    ```bash\n","    pip uninstall -y trulens_eval\n","    pip install trulens_eval\n","    ```\n","\n","Package datasets is installed but has a version conflict:\n","\t(datasets 2.10.1 (c:\\users\\28263\\anaconda3\\lib\\site-packages), Requirement.parse('datasets>=2.12.0'))\n","\n","This package is optional for trulens_eval so this may not be a problem but if\n","you need to use the related optional features and find there are errors, you\n","will need to resolve the conflict:\n","\n","    ```bash\n","    pip install 'datasets>=2.12.0'\n","    ```\n","\n","If you are running trulens_eval in a notebook, you may need to restart the\n","kernel after resolving the conflict. If your distribution is in a bad place\n","beyond this package, you may need to reinstall trulens_eval so that all of the\n","dependencies get installed and hopefully corrected:\n","    \n","    ```bash\n","    pip uninstall -y trulens_eval\n","    pip install trulens_eval\n","    ```\n","\n","c:\\Users\\28263\\anaconda3\\lib\\site-packages\\trulens_eval\\utils\\imports.py:489: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n","  return self.imp(name, globals, locals, fromlist, level)\n"]},{"name":"stdout","output_type":"stream","text":["ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n","ðŸ”’ Secret keys will not be included in the database.\n"]}],"source":["import openai\n","from trulens_eval import Tru\n","tru = Tru(database_redact_keys=True)"]},{"cell_type":"markdown","id":"779f2438-60d4-4475-a2ca-206bc0fc641b","metadata":{},"source":["## Load data and test set"]},{"cell_type":"code","execution_count":4,"id":"e9a06972-cc46-4541-8923-a3b3538a5a29","metadata":{"executionCancelledAt":null,"executionTime":20279,"lastExecutedAt":1701189512633,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_hub.smart_pdf_loader import SmartPDFLoader\n\nllmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\npdf_loader = SmartPDFLoader(llmsherpa_api_url=llmsherpa_api_url)\n\ndocuments = pdf_loader.load_data(\"https://www.iii.org/sites/default/files/docs/pdf/Insurance_Handbook_20103.pdf\")\n\n# Load some questions for evaluation\nhonest_evals = [\n    \"What are the typical coverage options for homeowners insurance?\",\n    \"What are the requirements for long term care insurance to start?\",\n    \"Can annuity benefits be passed to beneficiaries?\",\n    \"Are credit scores used to set insurance premiums? If so, how?\",\n    \"Who provides flood insurance?\",\n    \"Can you get flood insurance outside high-risk areas?\",\n    \"How much in losses does fraud account for in property & casualty insurance?\",\n    \"Do pay-as-you-drive insurance policies have an impact on greenhouse gas emissions? How much?\",\n    \"What was the most costly earthquake in US history for insurers?\",\n    \"Does it matter who is at fault to be compensated when injured on the job?\"\n]"},"outputs":[],"source":["from llama_index.readers.smart_pdf_loader import SmartPDFLoader\n","\n","llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n","pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\"  # also allowed is a file path e.g. /home/downloads/xyz.pdf\n","pdf_loader = SmartPDFLoader(llmsherpa_api_url=llmsherpa_api_url)\n","documents = pdf_loader.load_data(pdf_url)\n","# Load some questions for evaluation\n","honest_evals = [\n","    \"What are the typical coverage options for homeowners insurance?\",\n","    \"What are the requirements for long term care insurance to start?\",\n","    \"Can annuity benefits be passed to beneficiaries?\",\n","    \"Are credit scores used to set insurance premiums? If so, how?\",\n","    \"Who provides flood insurance?\",\n","    \"Can you get flood insurance outside high-risk areas?\",\n","    \"How much in losses does fraud account for in property & casualty insurance?\",\n","    \"Do pay-as-you-drive insurance policies have an impact on greenhouse gas emissions? How much?\",\n","    \"What was the most costly earthquake in US history for insurers?\",\n","    \"Does it matter who is at fault to be compensated when injured on the job?\"\n","]"]},{"cell_type":"markdown","id":"9da4f007-422c-45ca-a05c-f1e82288797f","metadata":{},"source":["## Set up Evaluation"]},{"cell_type":"code","execution_count":6,"id":"779812f4-daeb-45d0-a504-eefeb5bd7166","metadata":{"executionCancelledAt":null,"executionTime":874,"lastExecutedAt":1701189513508,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\nimport numpy as np\nfrom trulens_eval import Tru, Feedback, TruLlama, OpenAI as fOpenAI\n\nfrom trulens_eval.feedback import Groundedness\n\nopenai = fOpenAI()\n\nqa_relevance = (\n    Feedback(openai.relevance_with_cot_reasons, name=\"Answer Relevance\")\n    .on_input_output()\n)\n\nqs_relevance = (\n    Feedback(openai.relevance_with_cot_reasons, name = \"Context Relevance\")\n    .on_input()\n    .on(TruLlama.select_source_nodes().node.text)\n    .aggregate(np.mean)\n)\n\n# embedding distance\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom trulens_eval.feedback import Embeddings\n\nmodel_name = 'text-embedding-ada-002'\n\nembed_model = OpenAIEmbeddings(\n    model=model_name,\n    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n)\n\nembed = Embeddings(embed_model=embed_model)\nf_embed_dist = (\n    Feedback(embed.cosine_distance)\n    .on_input()\n    .on(TruLlama.select_source_nodes().node.text)\n)\n\nfrom trulens_eval.feedback import Groundedness\n\ngrounded = Groundedness(groundedness_provider=openai)\n\nf_groundedness = (\n    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n        .on(TruLlama.select_source_nodes().node.text.collect())\n        .on_output()\n        .aggregate(grounded.grounded_statements_aggregator)\n)\n\nhonest_feedbacks = [qa_relevance, qs_relevance, f_embed_dist, f_groundedness]","outputsMetadata":{"0":{"height":177,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n","âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n","âœ… In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n","âœ… In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n","âœ… In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text.collect() .\n","âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"]}],"source":["import os\n","import numpy as np\n","from trulens_eval import Tru, Feedback, TruLlama, OpenAI as fOpenAI\n","\n","from trulens_eval.feedback import Groundedness\n","\n","openai = fOpenAI()\n","\n","qa_relevance = (\n","    Feedback(openai.relevance_with_cot_reasons, name=\"Answer Relevance\")\n","    .on_input_output()\n",")\n","\n","qs_relevance = (\n","    Feedback(openai.relevance_with_cot_reasons, name = \"Context Relevance\")\n","    .on_input()\n","    .on(TruLlama.select_source_nodes().node.text)\n","    .aggregate(np.mean)\n",")\n","\n","# embedding distance\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from trulens_eval.feedback import Embeddings\n","\n","\n","#TODO fix embeddings\n","# model_name = 'text-embedding-ada-002'\n","\n","# embed_model = OpenAIEmbeddings(\n","#     model=model_name,\n","#     openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n","# )\n","\n","# embed = Embeddings(embed_model=embed_model)\n","# f_embed_dist = (\n","#     Feedback(embed.cosine_distance)\n","#     .on_input()\n","#     .on(TruLlama.select_source_nodes().node.text)\n","# )\n","#honest_feedbacks = [qa_relevance, qs_relevance, f_embed_dist, f_groundedness]\n","\n","\n","from trulens_eval.feedback import Groundedness\n","\n","grounded = Groundedness(groundedness_provider=openai)\n","\n","f_groundedness = (\n","    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n","        .on(TruLlama.select_source_nodes().node.text.collect())\n","        .on_output()\n","        .aggregate(grounded.grounded_statements_aggregator)\n",")\n","\n","\n","honest_feedbacks = [qa_relevance, qs_relevance, f_groundedness]"]},{"cell_type":"markdown","id":"7fd5d9b8-b5eb-4252-972c-b892fdcf1eb4","metadata":{},"source":[" simple RAG often struggles with retrieving not enough information from the insurance manual to properly answer the question. The information needed may be just outside the chunk that is identified and retrieved by our app. Try sentence window retrieval to retrieve a wider chunk."]},{"cell_type":"code","execution_count":10,"id":"ac1ec265-82a3-4bce-a90d-841444dd3701","metadata":{"executionCancelledAt":null,"executionTime":18481,"lastExecutedAt":1701189793026,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_index.node_parser import SentenceWindowNodeParser\nfrom llama_index.indices.postprocessor import MetadataReplacementPostProcessor\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\nfrom llama_index import load_index_from_storage\nfrom llama_index import Document\nfrom llama_index import ServiceContext, VectorStoreIndex, StorageContext\nfrom llama_index.llms import OpenAI\nimport os\n\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n\n# knowledge store\ndocument = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n\n# set system prompt\nfrom llama_index import Prompt\nsystem_prompt = Prompt(\"We have provided context information below that you may use. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Please answer the question: {query_str}\\n\")\n\ndef build_sentence_window_index(\n    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n):\n    # create the sentence window node parser w/ default settings\n    node_parser = SentenceWindowNodeParser.from_defaults(\n        window_size=3,\n        window_metadata_key=\"window\",\n        original_text_metadata_key=\"original_text\",\n    )\n    sentence_context = ServiceContext.from_defaults(\n        llm=llm, # fill llm\n        embed_model=embed_model, # embed model\n        node_parser=node_parser, # node parser\n    )\n    if not os.path.exists(save_dir):\n        sentence_index = VectorStoreIndex.from_documents(\n            [document], service_context=sentence_context\n        )\n        sentence_index.storage_context.persist(persist_dir=save_dir)\n    else:\n        sentence_index = load_index_from_storage(\n            StorageContext.from_defaults(persist_dir=save_dir),\n            service_context=sentence_context,\n        )\n\n    return sentence_index\n\nsentence_index = build_sentence_window_index(\n    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n)\n\ndef get_sentence_window_query_engine(\n    sentence_index,\n    system_prompt,\n    similarity_top_k=6, # top k\n    rerank_top_n=2,\n):\n    # define postprocessors\n    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n    rerank = SentenceTransformerRerank(\n        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n    )\n\n    sentence_window_engine = sentence_index.as_query_engine(\n        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank], text_qa_template = system_prompt\n    )\n    return sentence_window_engine\n\nsentence_window_engine = get_sentence_window_query_engine(sentence_index, system_prompt=system_prompt)\n\ntru_recorder_rag_sentencewindow = TruLlama(\n        sentence_window_engine,\n        app_id='2) Sentence Window RAG - Honest Eval',\n        feedbacks=honest_feedbacks\n    )"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\28263\\AppData\\Local\\Temp\\ipykernel_19424\\1861917252.py:38: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n","  sentence_context = ServiceContext.from_defaults(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7d30e758ce745abb20a6fbe1f61649d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\28263\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\28263\\AppData\\Local\\llama_index\\models\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f43e5072cb64487ca0bf95d5610b683a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7dd665f858a84cefac4098d387b49d40","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"968cc17867014f2a9c39130e6691713b","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0518e5792e574b10a85b5c6c7737e3d6","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6648a66a24084d4da3042baffbd11cb5","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08c0dba9c4a347369d590b52ad73c255","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/799 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\28263\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\28263\\.cache\\huggingface\\hub\\models--BAAI--bge-reranker-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cadb2cc9ef274566a31b91900fa64c83","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21363a22b70f430d81a235cf9b2829e7","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8df0b8b4c93435daeb24971c5b807fa","version_major":2,"version_minor":0},"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d14a309efeb4130ae3bcb1e45be78b1","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf1b0d6b8cb0476a87c5094188e06a35","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from llama_index.core.node_parser import SimpleFileNodeParser\n","from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n","from llama_index.core.postprocessor import SentenceTransformerRerank\n","from llama_index.core.indices.loading import load_index_from_storage\n","from llama_index.core.schema import Document\n","from llama_index.core.node_parser  import SentenceWindowNodeParser\n","from llama_index.core.indices.service_context import ServiceContext\n","from llama_index.core.indices.vector_store.base import VectorStoreIndex\n","from llama_index.core.storage.storage_context import StorageContext\n","from llama_index.llms.openai import OpenAI\n","from llama_index.core import PromptTemplate \n","\n","import os\n","\n","# initialize llm\n","llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n","\n","# knowledge store\n","document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n","\n","# set system prompt\n","\n","system_prompt = PromptTemplate(\"We have provided context information below that you may use. \\n\"\n","    \"---------------------\\n\"\n","    \"{context_str}\"\n","    \"\\n---------------------\\n\"\n","    \"Please answer the question: {query_str}\\n\")\n","\n","def build_sentence_window_index(\n","    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n","):\n","    # create the sentence window node parser w/ default settings\n","    node_parser = SentenceWindowNodeParser.from_defaults(\n","        window_size=3, \n","        window_metadata_key=\"window\",\n","        original_text_metadata_key=\"original_text\",\n","    )\n","    sentence_context = ServiceContext.from_defaults(\n","        llm=llm, # fill llm\n","        embed_model=embed_model, # embed model\n","        node_parser=node_parser, # node parser\n","    )\n","    if not os.path.exists(save_dir):\n","        sentence_index = VectorStoreIndex.from_documents(\n","            [document], service_context=sentence_context\n","        )\n","        sentence_index.storage_context.persist(persist_dir=save_dir)\n","    else:\n","        sentence_index = load_index_from_storage(\n","            StorageContext.from_defaults(persist_dir=save_dir),\n","            service_context=sentence_context,\n","        )\n","\n","    return sentence_index\n","\n","sentence_index = build_sentence_window_index(\n","    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n",")\n","\n","def get_sentence_window_query_engine(\n","    sentence_index,\n","    system_prompt,\n","    similarity_top_k=6, # top k\n","    rerank_top_n=2,\n","):\n","    # define postprocessors\n","    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n","    rerank = SentenceTransformerRerank(\n","        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n","    )\n","\n","    sentence_window_engine = sentence_index.as_query_engine(\n","        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank], text_qa_template = system_prompt\n","    )\n","    return sentence_window_engine\n","\n","sentence_window_engine = get_sentence_window_query_engine(sentence_index, system_prompt=system_prompt)\n","\n","tru_recorder_rag_sentencewindow = TruLlama(\n","        sentence_window_engine,\n","        app_id='2) Sentence Window RAG - Honest Eval',\n","        feedbacks=honest_feedbacks\n","    )"]},{"cell_type":"code","execution_count":11,"id":"e35774a4-a42d-4a73-a1da-ba46291b7382","metadata":{"executionCancelledAt":null,"executionTime":54611,"lastExecutedAt":1701189865007,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run evaluation on 10 sample questions\nwith tru_recorder_rag_sentencewindow as recording:\n    for question in honest_evals:\n        response = sentence_window_engine.query(question)","outputsMetadata":{"0":{"height":417,"type":"stream"}}},"outputs":[],"source":["# Run evaluation on 10 sample questions\n","with tru_recorder_rag_sentencewindow as recording:\n","    for question in honest_evals:\n","        response = sentence_window_engine.query(question)"]},{"cell_type":"code","execution_count":12,"id":"ab588e13-414a-456a-96b9-9321e5e16c13","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":138,"type":"dataFrame"}}},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Groundedness</th>\n","      <th>Answer Relevance</th>\n","      <th>Context Relevance</th>\n","      <th>latency</th>\n","      <th>total_cost</th>\n","    </tr>\n","    <tr>\n","      <th>app_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2) Sentence Window RAG - Honest Eval</th>\n","      <td>0.133333</td>\n","      <td>0.77</td>\n","      <td>0.0</td>\n","      <td>7.1</td>\n","      <td>0.000732</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      Groundedness  Answer Relevance  \\\n","app_id                                                                 \n","2) Sentence Window RAG - Honest Eval      0.133333              0.77   \n","\n","                                      Context Relevance  latency  total_cost  \n","app_id                                                                        \n","2) Sentence Window RAG - Honest Eval                0.0      7.1    0.000732  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["tru.get_leaderboard(app_ids=[\"1) Basic RAG - Honest Eval\", \"2) Sentence Window RAG - Honest Eval\"])"]},{"cell_type":"markdown","id":"b7a0e72d-1fc1-4b43-a614-a1f771c232d5","metadata":{},"source":[]}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}
